{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os   # 운영체제와 상호작용하기 위한 모듈\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경 변수 로드\n",
    "load_dotenv()\n",
    "base_dir = os.getenv('BASE_DIR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63843, 15, 235)\n"
     ]
    }
   ],
   "source": [
    "data = np.concatenate([\n",
    "    # np.load(os.path.join(base_dir, 'seq_data/seq_npy_1729150467_20_10.npy')),\n",
    "    # np.load(os.path.join(base_dir, 'seq_data/seq_npy_flip_1729150467_20_10.npy')),\n",
    "    # np.load(os.path.join(base_dir, 'seq_data/seq_npy_shift_1729150468_20_10.npy')),\n",
    "    # np.load(os.path.join(base_dir, 'seq_data/seq_npy_flip_shift_1729150469_20_10.npy')),\n",
    "\n",
    "    # # 잘 인식하지 못하는 단어 데이터 추가\n",
    "    # np.load(os.path.join(base_dir, 'seq_data/seq_npy_1729349658_20_10.npy')),\n",
    "    # np.load(os.path.join(base_dir, 'seq_data/seq_npy_flip_1729349658_20_10.npy')),\n",
    "\n",
    "    np.load(os.path.join(base_dir, 'seq_data_with_test/seq_npy_with_test_1731388067_15_5.npy')),\n",
    "    np.load(os.path.join(base_dir, 'seq_data_with_test/seq_npy_with_test_flip_1731388069_15_5.npy')),\n",
    "    np.load(os.path.join(base_dir, 'seq_data_with_test/seq_npy_with_test_shift_1731388071_15_5.npy')),\n",
    "    np.load(os.path.join(base_dir, 'seq_data_with_test/seq_npy_with_test_flip_shift_1731388074_15_5.npy')),\n",
    "], axis=0)\n",
    "\n",
    "print(data.shape)\n",
    "# (프레임 수, 시퀀스 길이, 한 프레임당 데이터 개수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\mshof\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "(63843, 15, 234) (63843,) (63843, 40)\n",
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
      " 36. 37. 38. 39.]\n"
     ]
    }
   ],
   "source": [
    "from setting import actions\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# 데이터 분리 및 전처리\n",
    "x_data = data[:, :, :-1]    # 시퀀스의 마지막 요소 제외한 모든 값 가져와 할당\n",
    "labels = data[:, 0, -1]     # 마지막 요소는 레이블 값\n",
    "\n",
    "# 원-핫 인코딩으로 변환\n",
    "y_data = to_categorical(labels, num_classes=len(actions))\n",
    "\n",
    "print(x_data.shape, labels.shape, y_data.shape)     # y_data 형태 -> [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...]\n",
    "print(np.unique(labels))    # 레이블 값 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51074, 15, 234) (51074, 40)\n",
      "(12769, 15, 234) (12769, 40)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터셋 분할 (학습 데이터와 검증 데이터만 사용)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2, random_state=23, stratify=labels)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\mshof\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\rnn\\lstm.py:148: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\mshof\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 256)               502784    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 40)                5160      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 540840 (2.06 MB)\n",
      "Trainable params: 540840 (2.06 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# 모델 정의\n",
    "model = Sequential([\n",
    "    LSTM(256, activation='tanh', input_shape=x_data.shape[1:3]),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(len(actions), activation='softmax')\n",
    "])\n",
    "\n",
    "# 모델 컴파일 (최적화 알고리즘, 레이블 클래스 2개 이상일 때 사용하는 손실 함수, 모델평가지표)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델 요약\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "WARNING:tensorflow:From c:\\Users\\mshof\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\mshof\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 1.5462 - accuracy: 0.5267\n",
      "Epoch 1: val_accuracy improved from -inf to 0.71862, saving model to models\\model.keras\n",
      "1597/1597 [==============================] - 50s 29ms/step - loss: 1.5462 - accuracy: 0.5267 - val_loss: 0.8492 - val_accuracy: 0.7186 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.6639 - accuracy: 0.7850\n",
      "Epoch 2: val_accuracy improved from 0.71862 to 0.83664, saving model to models\\model.keras\n",
      "1597/1597 [==============================] - 43s 27ms/step - loss: 0.6639 - accuracy: 0.7850 - val_loss: 0.4874 - val_accuracy: 0.8366 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.4232 - accuracy: 0.8609\n",
      "Epoch 3: val_accuracy improved from 0.83664 to 0.88456, saving model to models\\model.keras\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 0.4231 - accuracy: 0.8610 - val_loss: 0.3474 - val_accuracy: 0.8846 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.3205 - accuracy: 0.8946\n",
      "Epoch 4: val_accuracy improved from 0.88456 to 0.93163, saving model to models\\model.keras\n",
      "1597/1597 [==============================] - 41s 26ms/step - loss: 0.3204 - accuracy: 0.8946 - val_loss: 0.2065 - val_accuracy: 0.9316 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.2508 - accuracy: 0.9164\n",
      "Epoch 5: val_accuracy did not improve from 0.93163\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 0.2508 - accuracy: 0.9164 - val_loss: 0.4115 - val_accuracy: 0.8633 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.2127 - accuracy: 0.9280\n",
      "Epoch 6: val_accuracy improved from 0.93163 to 0.93837, saving model to models\\model.keras\n",
      "1597/1597 [==============================] - 43s 27ms/step - loss: 0.2127 - accuracy: 0.9280 - val_loss: 0.1974 - val_accuracy: 0.9384 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.1880 - accuracy: 0.9377\n",
      "Epoch 7: val_accuracy improved from 0.93837 to 0.95379, saving model to models\\model.keras\n",
      "1597/1597 [==============================] - 44s 28ms/step - loss: 0.1880 - accuracy: 0.9377 - val_loss: 0.1346 - val_accuracy: 0.9538 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.1580 - accuracy: 0.9484\n",
      "Epoch 8: val_accuracy did not improve from 0.95379\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 0.1580 - accuracy: 0.9484 - val_loss: 0.1870 - val_accuracy: 0.9368 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.1506 - accuracy: 0.9508\n",
      "Epoch 9: val_accuracy improved from 0.95379 to 0.97228, saving model to models\\model.keras\n",
      "1597/1597 [==============================] - 42s 26ms/step - loss: 0.1506 - accuracy: 0.9508 - val_loss: 0.0792 - val_accuracy: 0.9723 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.1265 - accuracy: 0.9589\n",
      "Epoch 10: val_accuracy did not improve from 0.97228\n",
      "1597/1597 [==============================] - 44s 28ms/step - loss: 0.1265 - accuracy: 0.9589 - val_loss: 0.0948 - val_accuracy: 0.9691 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.1164 - accuracy: 0.9612\n",
      "Epoch 11: val_accuracy improved from 0.97228 to 0.97455, saving model to models\\model.keras\n",
      "1597/1597 [==============================] - 43s 27ms/step - loss: 0.1164 - accuracy: 0.9612 - val_loss: 0.0820 - val_accuracy: 0.9745 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.1174 - accuracy: 0.9625\n",
      "Epoch 12: val_accuracy did not improve from 0.97455\n",
      "1597/1597 [==============================] - 43s 27ms/step - loss: 0.1174 - accuracy: 0.9625 - val_loss: 0.1200 - val_accuracy: 0.9619 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0958 - accuracy: 0.9679\n",
      "Epoch 13: val_accuracy did not improve from 0.97455\n",
      "1597/1597 [==============================] - 45s 28ms/step - loss: 0.0959 - accuracy: 0.9679 - val_loss: 0.0868 - val_accuracy: 0.9707 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0879 - accuracy: 0.9714\n",
      "Epoch 14: val_accuracy did not improve from 0.97455\n",
      "1597/1597 [==============================] - 39s 24ms/step - loss: 0.0879 - accuracy: 0.9714 - val_loss: 0.0798 - val_accuracy: 0.9739 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.1026 - accuracy: 0.9677\n",
      "Epoch 15: val_accuracy did not improve from 0.97455\n",
      "1597/1597 [==============================] - 45s 28ms/step - loss: 0.1027 - accuracy: 0.9677 - val_loss: 0.1076 - val_accuracy: 0.9672 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0849 - accuracy: 0.9722\n",
      "Epoch 16: val_accuracy improved from 0.97455 to 0.98324, saving model to models\\model.keras\n",
      "1597/1597 [==============================] - 40s 25ms/step - loss: 0.0849 - accuracy: 0.9722 - val_loss: 0.0507 - val_accuracy: 0.9832 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0773 - accuracy: 0.9758\n",
      "Epoch 17: val_accuracy did not improve from 0.98324\n",
      "1597/1597 [==============================] - 43s 27ms/step - loss: 0.0773 - accuracy: 0.9758 - val_loss: 0.0931 - val_accuracy: 0.9727 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0734 - accuracy: 0.9758\n",
      "Epoch 18: val_accuracy improved from 0.98324 to 0.98348, saving model to models\\model.keras\n",
      "1597/1597 [==============================] - 41s 26ms/step - loss: 0.0734 - accuracy: 0.9758 - val_loss: 0.0518 - val_accuracy: 0.9835 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0767 - accuracy: 0.9754\n",
      "Epoch 19: val_accuracy did not improve from 0.98348\n",
      "1597/1597 [==============================] - 45s 28ms/step - loss: 0.0767 - accuracy: 0.9754 - val_loss: 0.0492 - val_accuracy: 0.9832 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0598 - accuracy: 0.9810\n",
      "Epoch 20: val_accuracy did not improve from 0.98348\n",
      "1597/1597 [==============================] - 45s 28ms/step - loss: 0.0600 - accuracy: 0.9810 - val_loss: 0.0532 - val_accuracy: 0.9831 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0777 - accuracy: 0.9763\n",
      "Epoch 21: val_accuracy improved from 0.98348 to 0.98520, saving model to models\\model.keras\n",
      "1597/1597 [==============================] - 39s 24ms/step - loss: 0.0777 - accuracy: 0.9763 - val_loss: 0.0397 - val_accuracy: 0.9852 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "1594/1597 [============================>.] - ETA: 0s - loss: 0.0654 - accuracy: 0.9793\n",
      "Epoch 22: val_accuracy improved from 0.98520 to 0.98536, saving model to models\\model.keras\n",
      "1597/1597 [==============================] - 45s 28ms/step - loss: 0.0654 - accuracy: 0.9793 - val_loss: 0.0435 - val_accuracy: 0.9854 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0699 - accuracy: 0.9775\n",
      "Epoch 23: val_accuracy improved from 0.98536 to 0.98739, saving model to models\\model.keras\n",
      "1597/1597 [==============================] - 40s 25ms/step - loss: 0.0698 - accuracy: 0.9775 - val_loss: 0.0420 - val_accuracy: 0.9874 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0556 - accuracy: 0.9831\n",
      "Epoch 24: val_accuracy did not improve from 0.98739\n",
      "1597/1597 [==============================] - 44s 27ms/step - loss: 0.0556 - accuracy: 0.9831 - val_loss: 0.0404 - val_accuracy: 0.9872 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0591 - accuracy: 0.9814\n",
      "Epoch 25: val_accuracy improved from 0.98739 to 0.98825, saving model to models\\model.keras\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 0.0591 - accuracy: 0.9814 - val_loss: 0.0343 - val_accuracy: 0.9883 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0651 - accuracy: 0.9795\n",
      "Epoch 26: val_accuracy did not improve from 0.98825\n",
      "1597/1597 [==============================] - 42s 26ms/step - loss: 0.0651 - accuracy: 0.9795 - val_loss: 0.1076 - val_accuracy: 0.9682 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0574 - accuracy: 0.9825\n",
      "Epoch 27: val_accuracy did not improve from 0.98825\n",
      "1597/1597 [==============================] - 44s 27ms/step - loss: 0.0574 - accuracy: 0.9825 - val_loss: 0.0706 - val_accuracy: 0.9774 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0637 - accuracy: 0.9794\n",
      "Epoch 28: val_accuracy did not improve from 0.98825\n",
      "1597/1597 [==============================] - 39s 24ms/step - loss: 0.0637 - accuracy: 0.9794 - val_loss: 0.0727 - val_accuracy: 0.9785 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0649 - accuracy: 0.9809\n",
      "Epoch 29: val_accuracy improved from 0.98825 to 0.99084, saving model to models\\model.keras\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 0.0649 - accuracy: 0.9809 - val_loss: 0.0248 - val_accuracy: 0.9908 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0517 - accuracy: 0.9838\n",
      "Epoch 30: val_accuracy did not improve from 0.99084\n",
      "1597/1597 [==============================] - 42s 26ms/step - loss: 0.0517 - accuracy: 0.9838 - val_loss: 0.0374 - val_accuracy: 0.9886 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0490 - accuracy: 0.9851\n",
      "Epoch 31: val_accuracy did not improve from 0.99084\n",
      "1597/1597 [==============================] - 42s 27ms/step - loss: 0.0490 - accuracy: 0.9851 - val_loss: 0.0321 - val_accuracy: 0.9903 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0485 - accuracy: 0.9849\n",
      "Epoch 32: val_accuracy improved from 0.99084 to 0.99366, saving model to models\\model.keras\n",
      "1597/1597 [==============================] - 41s 26ms/step - loss: 0.0485 - accuracy: 0.9849 - val_loss: 0.0203 - val_accuracy: 0.9937 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0545 - accuracy: 0.9834\n",
      "Epoch 33: val_accuracy did not improve from 0.99366\n",
      "1597/1597 [==============================] - 43s 27ms/step - loss: 0.0545 - accuracy: 0.9834 - val_loss: 0.0431 - val_accuracy: 0.9866 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0422 - accuracy: 0.9868\n",
      "Epoch 34: val_accuracy did not improve from 0.99366\n",
      "1597/1597 [==============================] - 44s 27ms/step - loss: 0.0422 - accuracy: 0.9868 - val_loss: 0.0349 - val_accuracy: 0.9904 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0575 - accuracy: 0.9825\n",
      "Epoch 35: val_accuracy did not improve from 0.99366\n",
      "1597/1597 [==============================] - 39s 24ms/step - loss: 0.0575 - accuracy: 0.9825 - val_loss: 0.0361 - val_accuracy: 0.9883 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0506 - accuracy: 0.9856\n",
      "Epoch 36: val_accuracy did not improve from 0.99366\n",
      "1597/1597 [==============================] - 45s 28ms/step - loss: 0.0506 - accuracy: 0.9856 - val_loss: 0.1287 - val_accuracy: 0.9641 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0467 - accuracy: 0.9859\n",
      "Epoch 37: val_accuracy did not improve from 0.99366\n",
      "1597/1597 [==============================] - 40s 25ms/step - loss: 0.0467 - accuracy: 0.9859 - val_loss: 0.0293 - val_accuracy: 0.9913 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0526 - accuracy: 0.9840\n",
      "Epoch 38: val_accuracy did not improve from 0.99366\n",
      "1597/1597 [==============================] - 42s 26ms/step - loss: 0.0526 - accuracy: 0.9840 - val_loss: 0.0300 - val_accuracy: 0.9907 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0449 - accuracy: 0.9870\n",
      "Epoch 39: val_accuracy did not improve from 0.99366\n",
      "1597/1597 [==============================] - 45s 28ms/step - loss: 0.0449 - accuracy: 0.9870 - val_loss: 0.0283 - val_accuracy: 0.9916 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0594 - accuracy: 0.9833\n",
      "Epoch 40: val_accuracy improved from 0.99366 to 0.99428, saving model to models\\model.keras\n",
      "1597/1597 [==============================] - 39s 24ms/step - loss: 0.0594 - accuracy: 0.9833 - val_loss: 0.0194 - val_accuracy: 0.9943 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0359 - accuracy: 0.9894\n",
      "Epoch 41: val_accuracy did not improve from 0.99428\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 0.0360 - accuracy: 0.9893 - val_loss: 0.0898 - val_accuracy: 0.9748 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0455 - accuracy: 0.9865\n",
      "Epoch 42: val_accuracy did not improve from 0.99428\n",
      "1597/1597 [==============================] - 39s 25ms/step - loss: 0.0455 - accuracy: 0.9865 - val_loss: 0.1150 - val_accuracy: 0.9680 - lr: 0.0010\n",
      "Epoch 43/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0600 - accuracy: 0.9828\n",
      "Epoch 43: val_accuracy did not improve from 0.99428\n",
      "1597/1597 [==============================] - 44s 27ms/step - loss: 0.0599 - accuracy: 0.9828 - val_loss: 0.0295 - val_accuracy: 0.9910 - lr: 0.0010\n",
      "Epoch 44/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0395 - accuracy: 0.9878\n",
      "Epoch 44: val_accuracy did not improve from 0.99428\n",
      "1597/1597 [==============================] - 40s 25ms/step - loss: 0.0395 - accuracy: 0.9878 - val_loss: 0.0389 - val_accuracy: 0.9895 - lr: 0.0010\n",
      "Epoch 45/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0466 - accuracy: 0.9865\n",
      "Epoch 45: val_accuracy did not improve from 0.99428\n",
      "1597/1597 [==============================] - 42s 26ms/step - loss: 0.0466 - accuracy: 0.9865 - val_loss: 0.0173 - val_accuracy: 0.9943 - lr: 0.0010\n",
      "Epoch 46/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0513 - accuracy: 0.9845\n",
      "Epoch 46: val_accuracy did not improve from 0.99428\n",
      "1597/1597 [==============================] - 44s 27ms/step - loss: 0.0513 - accuracy: 0.9845 - val_loss: 0.0417 - val_accuracy: 0.9888 - lr: 0.0010\n",
      "Epoch 47/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0394 - accuracy: 0.9878\n",
      "Epoch 47: val_accuracy did not improve from 0.99428\n",
      "1597/1597 [==============================] - 40s 25ms/step - loss: 0.0395 - accuracy: 0.9878 - val_loss: 0.0479 - val_accuracy: 0.9868 - lr: 0.0010\n",
      "Epoch 48/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0485 - accuracy: 0.9855\n",
      "Epoch 48: val_accuracy did not improve from 0.99428\n",
      "1597/1597 [==============================] - 43s 27ms/step - loss: 0.0485 - accuracy: 0.9855 - val_loss: 0.0204 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 49/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 0.9886\n",
      "Epoch 49: val_accuracy did not improve from 0.99428\n",
      "1597/1597 [==============================] - 39s 24ms/step - loss: 0.0379 - accuracy: 0.9886 - val_loss: 0.0323 - val_accuracy: 0.9919 - lr: 0.0010\n",
      "Epoch 50/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0521 - accuracy: 0.9850\n",
      "Epoch 50: val_accuracy did not improve from 0.99428\n",
      "1597/1597 [==============================] - 44s 28ms/step - loss: 0.0521 - accuracy: 0.9850 - val_loss: 0.0451 - val_accuracy: 0.9861 - lr: 0.0010\n",
      "Epoch 51/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0415 - accuracy: 0.9873\n",
      "Epoch 51: val_accuracy did not improve from 0.99428\n",
      "1597/1597 [==============================] - 39s 25ms/step - loss: 0.0415 - accuracy: 0.9873 - val_loss: 0.0239 - val_accuracy: 0.9920 - lr: 0.0010\n",
      "Epoch 52/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.9879\n",
      "Epoch 52: val_accuracy did not improve from 0.99428\n",
      "1597/1597 [==============================] - 42s 26ms/step - loss: 0.0403 - accuracy: 0.9879 - val_loss: 0.0566 - val_accuracy: 0.9817 - lr: 0.0010\n",
      "Epoch 53/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0452 - accuracy: 0.9868\n",
      "Epoch 53: val_accuracy improved from 0.99428 to 0.99452, saving model to models\\model.keras\n",
      "1597/1597 [==============================] - 41s 25ms/step - loss: 0.0452 - accuracy: 0.9868 - val_loss: 0.0202 - val_accuracy: 0.9945 - lr: 0.0010\n",
      "Epoch 54/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0412 - accuracy: 0.9879\n",
      "Epoch 54: val_accuracy did not improve from 0.99452\n",
      "1597/1597 [==============================] - 41s 26ms/step - loss: 0.0412 - accuracy: 0.9879 - val_loss: 0.0311 - val_accuracy: 0.9911 - lr: 0.0010\n",
      "Epoch 55/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0452 - accuracy: 0.9866\n",
      "Epoch 55: val_accuracy did not improve from 0.99452\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 0.0452 - accuracy: 0.9866 - val_loss: 0.0209 - val_accuracy: 0.9933 - lr: 0.0010\n",
      "Epoch 56/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0404 - accuracy: 0.9879\n",
      "Epoch 56: val_accuracy improved from 0.99452 to 0.99687, saving model to models\\model.keras\n",
      "1597/1597 [==============================] - 39s 24ms/step - loss: 0.0404 - accuracy: 0.9879 - val_loss: 0.0114 - val_accuracy: 0.9969 - lr: 0.0010\n",
      "Epoch 57/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0477 - accuracy: 0.9859\n",
      "Epoch 57: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 44s 27ms/step - loss: 0.0477 - accuracy: 0.9859 - val_loss: 0.0326 - val_accuracy: 0.9902 - lr: 0.0010\n",
      "Epoch 58/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0370 - accuracy: 0.9888\n",
      "Epoch 58: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 41s 25ms/step - loss: 0.0370 - accuracy: 0.9888 - val_loss: 0.0181 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 59/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 0.9889\n",
      "Epoch 59: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 41s 26ms/step - loss: 0.0382 - accuracy: 0.9889 - val_loss: 0.0427 - val_accuracy: 0.9872 - lr: 0.0010\n",
      "Epoch 60/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0494 - accuracy: 0.9855\n",
      "Epoch 60: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 44s 27ms/step - loss: 0.0494 - accuracy: 0.9855 - val_loss: 0.0330 - val_accuracy: 0.9890 - lr: 0.0010\n",
      "Epoch 61/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0340 - accuracy: 0.9899\n",
      "Epoch 61: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 39s 24ms/step - loss: 0.0340 - accuracy: 0.9899 - val_loss: 0.0169 - val_accuracy: 0.9944 - lr: 0.0010\n",
      "Epoch 62/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0408 - accuracy: 0.9881\n",
      "Epoch 62: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 0.0408 - accuracy: 0.9881 - val_loss: 0.0288 - val_accuracy: 0.9926 - lr: 0.0010\n",
      "Epoch 63/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0457 - accuracy: 0.9871\n",
      "Epoch 63: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 40s 25ms/step - loss: 0.0457 - accuracy: 0.9871 - val_loss: 0.0794 - val_accuracy: 0.9774 - lr: 0.0010\n",
      "Epoch 64/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0422 - accuracy: 0.9876\n",
      "Epoch 64: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 43s 27ms/step - loss: 0.0422 - accuracy: 0.9876 - val_loss: 0.0334 - val_accuracy: 0.9904 - lr: 0.0010\n",
      "Epoch 65/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0420 - accuracy: 0.9882\n",
      "Epoch 65: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 41s 26ms/step - loss: 0.0420 - accuracy: 0.9882 - val_loss: 0.0492 - val_accuracy: 0.9864 - lr: 0.0010\n",
      "Epoch 66/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0487 - accuracy: 0.9859\n",
      "Epoch 66: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 43s 27ms/step - loss: 0.0488 - accuracy: 0.9858 - val_loss: 0.0240 - val_accuracy: 0.9923 - lr: 0.0010\n",
      "Epoch 67/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0370 - accuracy: 0.9893\n",
      "Epoch 67: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 45s 28ms/step - loss: 0.0370 - accuracy: 0.9893 - val_loss: 0.0317 - val_accuracy: 0.9901 - lr: 0.0010\n",
      "Epoch 68/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0352 - accuracy: 0.9893\n",
      "Epoch 68: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 40s 25ms/step - loss: 0.0352 - accuracy: 0.9893 - val_loss: 0.0304 - val_accuracy: 0.9914 - lr: 0.0010\n",
      "Epoch 69/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0476 - accuracy: 0.9862\n",
      "Epoch 69: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 45s 28ms/step - loss: 0.0476 - accuracy: 0.9862 - val_loss: 0.0270 - val_accuracy: 0.9917 - lr: 0.0010\n",
      "Epoch 70/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0364 - accuracy: 0.9896\n",
      "Epoch 70: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 40s 25ms/step - loss: 0.0364 - accuracy: 0.9896 - val_loss: 0.0319 - val_accuracy: 0.9915 - lr: 0.0010\n",
      "Epoch 71/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0358 - accuracy: 0.9895\n",
      "Epoch 71: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 42s 27ms/step - loss: 0.0358 - accuracy: 0.9895 - val_loss: 0.0239 - val_accuracy: 0.9931 - lr: 0.0010\n",
      "Epoch 72/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0379 - accuracy: 0.9885\n",
      "Epoch 72: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 43s 27ms/step - loss: 0.0379 - accuracy: 0.9885 - val_loss: 0.0447 - val_accuracy: 0.9865 - lr: 0.0010\n",
      "Epoch 73/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0464 - accuracy: 0.9866\n",
      "Epoch 73: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 40s 25ms/step - loss: 0.0464 - accuracy: 0.9866 - val_loss: 0.0281 - val_accuracy: 0.9910 - lr: 0.0010\n",
      "Epoch 74/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0384 - accuracy: 0.9884\n",
      "Epoch 74: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 43s 27ms/step - loss: 0.0384 - accuracy: 0.9884 - val_loss: 0.0643 - val_accuracy: 0.9826 - lr: 0.0010\n",
      "Epoch 75/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0421 - accuracy: 0.9874\n",
      "Epoch 75: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 39s 25ms/step - loss: 0.0421 - accuracy: 0.9874 - val_loss: 0.0237 - val_accuracy: 0.9922 - lr: 0.0010\n",
      "Epoch 76/500\n",
      "1594/1597 [============================>.] - ETA: 0s - loss: 0.0336 - accuracy: 0.9898\n",
      "Epoch 76: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 43s 27ms/step - loss: 0.0335 - accuracy: 0.9899 - val_loss: 0.0123 - val_accuracy: 0.9959 - lr: 0.0010\n",
      "Epoch 77/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0416 - accuracy: 0.9880\n",
      "Epoch 77: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 39s 25ms/step - loss: 0.0416 - accuracy: 0.9880 - val_loss: 0.0149 - val_accuracy: 0.9952 - lr: 0.0010\n",
      "Epoch 78/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0396 - accuracy: 0.9886\n",
      "Epoch 78: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 41s 26ms/step - loss: 0.0396 - accuracy: 0.9886 - val_loss: 0.0152 - val_accuracy: 0.9953 - lr: 0.0010\n",
      "Epoch 79/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0411 - accuracy: 0.9879\n",
      "Epoch 79: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 42s 26ms/step - loss: 0.0412 - accuracy: 0.9879 - val_loss: 0.0216 - val_accuracy: 0.9929 - lr: 0.0010\n",
      "Epoch 80/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0376 - accuracy: 0.9887\n",
      "Epoch 80: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 40s 25ms/step - loss: 0.0375 - accuracy: 0.9887 - val_loss: 0.0335 - val_accuracy: 0.9912 - lr: 0.0010\n",
      "Epoch 81/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 0.9902\n",
      "Epoch 81: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 50s 32ms/step - loss: 0.0321 - accuracy: 0.9902 - val_loss: 0.0379 - val_accuracy: 0.9883 - lr: 0.0010\n",
      "Epoch 82/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0369 - accuracy: 0.9891\n",
      "Epoch 82: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 54s 34ms/step - loss: 0.0369 - accuracy: 0.9891 - val_loss: 0.0310 - val_accuracy: 0.9912 - lr: 0.0010\n",
      "Epoch 83/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0446 - accuracy: 0.9875\n",
      "Epoch 83: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 0.0446 - accuracy: 0.9875 - val_loss: 0.0397 - val_accuracy: 0.9883 - lr: 0.0010\n",
      "Epoch 84/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0437 - accuracy: 0.9876\n",
      "Epoch 84: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 51s 32ms/step - loss: 0.0437 - accuracy: 0.9876 - val_loss: 0.0209 - val_accuracy: 0.9933 - lr: 0.0010\n",
      "Epoch 85/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9885\n",
      "Epoch 85: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 44s 27ms/step - loss: 0.0385 - accuracy: 0.9885 - val_loss: 0.0452 - val_accuracy: 0.9877 - lr: 0.0010\n",
      "Epoch 86/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9882\n",
      "Epoch 86: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 39s 24ms/step - loss: 0.0411 - accuracy: 0.9882 - val_loss: 0.0207 - val_accuracy: 0.9941 - lr: 0.0010\n",
      "Epoch 87/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0436 - accuracy: 0.9875\n",
      "Epoch 87: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 50s 31ms/step - loss: 0.0436 - accuracy: 0.9875 - val_loss: 0.0150 - val_accuracy: 0.9961 - lr: 0.0010\n",
      "Epoch 88/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0387 - accuracy: 0.9889\n",
      "Epoch 88: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 52s 33ms/step - loss: 0.0387 - accuracy: 0.9889 - val_loss: 0.0388 - val_accuracy: 0.9889 - lr: 0.0010\n",
      "Epoch 89/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0351 - accuracy: 0.9901\n",
      "Epoch 89: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 53s 33ms/step - loss: 0.0351 - accuracy: 0.9901 - val_loss: 0.0206 - val_accuracy: 0.9941 - lr: 0.0010\n",
      "Epoch 90/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0444 - accuracy: 0.9874\n",
      "Epoch 90: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 55s 34ms/step - loss: 0.0444 - accuracy: 0.9874 - val_loss: 0.0307 - val_accuracy: 0.9912 - lr: 0.0010\n",
      "Epoch 91/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0388 - accuracy: 0.9888\n",
      "Epoch 91: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 59s 37ms/step - loss: 0.0388 - accuracy: 0.9888 - val_loss: 0.0352 - val_accuracy: 0.9886 - lr: 0.0010\n",
      "Epoch 92/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0393 - accuracy: 0.9884\n",
      "Epoch 92: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 55s 34ms/step - loss: 0.0393 - accuracy: 0.9884 - val_loss: 0.0199 - val_accuracy: 0.9948 - lr: 0.0010\n",
      "Epoch 93/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0333 - accuracy: 0.9905\n",
      "Epoch 93: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 41s 26ms/step - loss: 0.0333 - accuracy: 0.9905 - val_loss: 0.0560 - val_accuracy: 0.9846 - lr: 0.0010\n",
      "Epoch 94/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0458 - accuracy: 0.9872\n",
      "Epoch 94: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 44s 28ms/step - loss: 0.0458 - accuracy: 0.9872 - val_loss: 0.0582 - val_accuracy: 0.9839 - lr: 0.0010\n",
      "Epoch 95/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0411 - accuracy: 0.9886\n",
      "Epoch 95: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 41s 26ms/step - loss: 0.0412 - accuracy: 0.9886 - val_loss: 0.0499 - val_accuracy: 0.9868 - lr: 0.0010\n",
      "Epoch 96/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0460 - accuracy: 0.9870\n",
      "Epoch 96: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 45s 28ms/step - loss: 0.0462 - accuracy: 0.9869 - val_loss: 0.0320 - val_accuracy: 0.9915 - lr: 0.0010\n",
      "Epoch 97/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0367 - accuracy: 0.9896\n",
      "Epoch 97: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 45s 28ms/step - loss: 0.0367 - accuracy: 0.9896 - val_loss: 0.0313 - val_accuracy: 0.9915 - lr: 0.0010\n",
      "Epoch 98/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9888\n",
      "Epoch 98: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 42s 27ms/step - loss: 0.0387 - accuracy: 0.9888 - val_loss: 0.0509 - val_accuracy: 0.9852 - lr: 0.0010\n",
      "Epoch 99/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0514 - accuracy: 0.9863\n",
      "Epoch 99: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 41s 26ms/step - loss: 0.0514 - accuracy: 0.9863 - val_loss: 0.0197 - val_accuracy: 0.9936 - lr: 0.0010\n",
      "Epoch 100/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9902\n",
      "Epoch 100: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 42s 27ms/step - loss: 0.0335 - accuracy: 0.9902 - val_loss: 0.0716 - val_accuracy: 0.9837 - lr: 0.0010\n",
      "Epoch 101/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0468 - accuracy: 0.9871\n",
      "Epoch 101: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 43s 27ms/step - loss: 0.0468 - accuracy: 0.9871 - val_loss: 0.0307 - val_accuracy: 0.9907 - lr: 0.0010\n",
      "Epoch 102/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0427 - accuracy: 0.9876\n",
      "Epoch 102: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 39s 24ms/step - loss: 0.0427 - accuracy: 0.9876 - val_loss: 0.0345 - val_accuracy: 0.9904 - lr: 0.0010\n",
      "Epoch 103/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0433 - accuracy: 0.9869\n",
      "Epoch 103: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 44s 27ms/step - loss: 0.0433 - accuracy: 0.9869 - val_loss: 0.0147 - val_accuracy: 0.9961 - lr: 0.0010\n",
      "Epoch 104/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0411 - accuracy: 0.9885\n",
      "Epoch 104: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 40s 25ms/step - loss: 0.0412 - accuracy: 0.9885 - val_loss: 0.0385 - val_accuracy: 0.9866 - lr: 0.0010\n",
      "Epoch 105/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0387 - accuracy: 0.9888\n",
      "Epoch 105: val_accuracy did not improve from 0.99687\n",
      "1597/1597 [==============================] - 43s 27ms/step - loss: 0.0387 - accuracy: 0.9888 - val_loss: 0.0330 - val_accuracy: 0.9901 - lr: 0.0010\n",
      "Epoch 106/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0398 - accuracy: 0.9884\n",
      "Epoch 106: val_accuracy did not improve from 0.99687\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "1597/1597 [==============================] - 42s 27ms/step - loss: 0.0398 - accuracy: 0.9884 - val_loss: 0.0265 - val_accuracy: 0.9921 - lr: 0.0010\n",
      "Epoch 107/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0140 - accuracy: 0.9955\n",
      "Epoch 107: val_accuracy improved from 0.99687 to 0.99820, saving model to models\\model.keras\n",
      "1597/1597 [==============================] - 39s 25ms/step - loss: 0.0140 - accuracy: 0.9955 - val_loss: 0.0066 - val_accuracy: 0.9982 - lr: 5.0000e-04\n",
      "Epoch 108/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0117 - accuracy: 0.9964\n",
      "Epoch 108: val_accuracy did not improve from 0.99820\n",
      "1597/1597 [==============================] - 44s 27ms/step - loss: 0.0117 - accuracy: 0.9964 - val_loss: 0.0103 - val_accuracy: 0.9975 - lr: 5.0000e-04\n",
      "Epoch 109/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0089 - accuracy: 0.9972\n",
      "Epoch 109: val_accuracy did not improve from 0.99820\n",
      "1597/1597 [==============================] - 43s 27ms/step - loss: 0.0089 - accuracy: 0.9972 - val_loss: 0.0121 - val_accuracy: 0.9969 - lr: 5.0000e-04\n",
      "Epoch 110/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0107 - accuracy: 0.9966\n",
      "Epoch 110: val_accuracy did not improve from 0.99820\n",
      "1597/1597 [==============================] - 42s 26ms/step - loss: 0.0107 - accuracy: 0.9966 - val_loss: 0.0055 - val_accuracy: 0.9979 - lr: 5.0000e-04\n",
      "Epoch 111/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9967\n",
      "Epoch 111: val_accuracy did not improve from 0.99820\n",
      "1597/1597 [==============================] - 41s 26ms/step - loss: 0.0121 - accuracy: 0.9967 - val_loss: 0.0086 - val_accuracy: 0.9976 - lr: 5.0000e-04\n",
      "Epoch 112/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 0.9977\n",
      "Epoch 112: val_accuracy did not improve from 0.99820\n",
      "1597/1597 [==============================] - 41s 26ms/step - loss: 0.0073 - accuracy: 0.9977 - val_loss: 0.0184 - val_accuracy: 0.9944 - lr: 5.0000e-04\n",
      "Epoch 113/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0140 - accuracy: 0.9959\n",
      "Epoch 113: val_accuracy did not improve from 0.99820\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 0.0140 - accuracy: 0.9959 - val_loss: 0.0074 - val_accuracy: 0.9982 - lr: 5.0000e-04\n",
      "Epoch 114/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0115 - accuracy: 0.9966\n",
      "Epoch 114: val_accuracy did not improve from 0.99820\n",
      "1597/1597 [==============================] - 39s 25ms/step - loss: 0.0115 - accuracy: 0.9966 - val_loss: 0.0100 - val_accuracy: 0.9973 - lr: 5.0000e-04\n",
      "Epoch 115/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0105 - accuracy: 0.9967\n",
      "Epoch 115: val_accuracy did not improve from 0.99820\n",
      "1597/1597 [==============================] - 43s 27ms/step - loss: 0.0105 - accuracy: 0.9967 - val_loss: 0.0244 - val_accuracy: 0.9955 - lr: 5.0000e-04\n",
      "Epoch 116/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0109 - accuracy: 0.9969\n",
      "Epoch 116: val_accuracy did not improve from 0.99820\n",
      "1597/1597 [==============================] - 39s 24ms/step - loss: 0.0109 - accuracy: 0.9969 - val_loss: 0.0133 - val_accuracy: 0.9971 - lr: 5.0000e-04\n",
      "Epoch 117/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0145 - accuracy: 0.9962\n",
      "Epoch 117: val_accuracy did not improve from 0.99820\n",
      "1597/1597 [==============================] - 44s 28ms/step - loss: 0.0145 - accuracy: 0.9962 - val_loss: 0.0154 - val_accuracy: 0.9960 - lr: 5.0000e-04\n",
      "Epoch 118/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0092 - accuracy: 0.9972\n",
      "Epoch 118: val_accuracy improved from 0.99820 to 0.99843, saving model to models\\model.keras\n",
      "1597/1597 [==============================] - 41s 26ms/step - loss: 0.0092 - accuracy: 0.9972 - val_loss: 0.0096 - val_accuracy: 0.9984 - lr: 5.0000e-04\n",
      "Epoch 119/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0106 - accuracy: 0.9972\n",
      "Epoch 119: val_accuracy did not improve from 0.99843\n",
      "1597/1597 [==============================] - 42s 26ms/step - loss: 0.0106 - accuracy: 0.9972 - val_loss: 0.0112 - val_accuracy: 0.9974 - lr: 5.0000e-04\n",
      "Epoch 120/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0117 - accuracy: 0.9966\n",
      "Epoch 120: val_accuracy did not improve from 0.99843\n",
      "1597/1597 [==============================] - 42s 26ms/step - loss: 0.0117 - accuracy: 0.9966 - val_loss: 0.0092 - val_accuracy: 0.9977 - lr: 5.0000e-04\n",
      "Epoch 121/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 0.9974\n",
      "Epoch 121: val_accuracy did not improve from 0.99843\n",
      "1597/1597 [==============================] - 39s 25ms/step - loss: 0.0078 - accuracy: 0.9974 - val_loss: 0.0100 - val_accuracy: 0.9973 - lr: 5.0000e-04\n",
      "Epoch 122/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9979\n",
      "Epoch 122: val_accuracy did not improve from 0.99843\n",
      "1597/1597 [==============================] - 43s 27ms/step - loss: 0.0072 - accuracy: 0.9979 - val_loss: 0.0090 - val_accuracy: 0.9983 - lr: 5.0000e-04\n",
      "Epoch 123/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0120 - accuracy: 0.9967\n",
      "Epoch 123: val_accuracy did not improve from 0.99843\n",
      "1597/1597 [==============================] - 43s 27ms/step - loss: 0.0120 - accuracy: 0.9967 - val_loss: 0.0086 - val_accuracy: 0.9984 - lr: 5.0000e-04\n",
      "Epoch 124/500\n",
      "1594/1597 [============================>.] - ETA: 0s - loss: 0.0121 - accuracy: 0.9965\n",
      "Epoch 124: val_accuracy did not improve from 0.99843\n",
      "1597/1597 [==============================] - 40s 25ms/step - loss: 0.0121 - accuracy: 0.9965 - val_loss: 0.0054 - val_accuracy: 0.9984 - lr: 5.0000e-04\n",
      "Epoch 125/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0097 - accuracy: 0.9971\n",
      "Epoch 125: val_accuracy improved from 0.99843 to 0.99867, saving model to models\\model.keras\n",
      "1597/1597 [==============================] - 44s 28ms/step - loss: 0.0097 - accuracy: 0.9971 - val_loss: 0.0055 - val_accuracy: 0.9987 - lr: 5.0000e-04\n",
      "Epoch 126/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0092 - accuracy: 0.9970\n",
      "Epoch 126: val_accuracy did not improve from 0.99867\n",
      "1597/1597 [==============================] - 40s 25ms/step - loss: 0.0092 - accuracy: 0.9970 - val_loss: 0.0075 - val_accuracy: 0.9981 - lr: 5.0000e-04\n",
      "Epoch 127/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0104 - accuracy: 0.9972\n",
      "Epoch 127: val_accuracy did not improve from 0.99867\n",
      "1597/1597 [==============================] - 43s 27ms/step - loss: 0.0104 - accuracy: 0.9972 - val_loss: 0.0091 - val_accuracy: 0.9980 - lr: 5.0000e-04\n",
      "Epoch 128/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.9977\n",
      "Epoch 128: val_accuracy did not improve from 0.99867\n",
      "1597/1597 [==============================] - 40s 25ms/step - loss: 0.0090 - accuracy: 0.9977 - val_loss: 0.0069 - val_accuracy: 0.9979 - lr: 5.0000e-04\n",
      "Epoch 129/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9986\n",
      "Epoch 129: val_accuracy did not improve from 0.99867\n",
      "1597/1597 [==============================] - 42s 27ms/step - loss: 0.0048 - accuracy: 0.9986 - val_loss: 0.0142 - val_accuracy: 0.9973 - lr: 5.0000e-04\n",
      "Epoch 130/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0153 - accuracy: 0.9960\n",
      "Epoch 130: val_accuracy did not improve from 0.99867\n",
      "1597/1597 [==============================] - 43s 27ms/step - loss: 0.0153 - accuracy: 0.9960 - val_loss: 0.0117 - val_accuracy: 0.9970 - lr: 5.0000e-04\n",
      "Epoch 131/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0059 - accuracy: 0.9984\n",
      "Epoch 131: val_accuracy improved from 0.99867 to 0.99914, saving model to models\\model.keras\n",
      "1597/1597 [==============================] - 40s 25ms/step - loss: 0.0059 - accuracy: 0.9984 - val_loss: 0.0085 - val_accuracy: 0.9991 - lr: 5.0000e-04\n",
      "Epoch 132/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0135 - accuracy: 0.9962\n",
      "Epoch 132: val_accuracy did not improve from 0.99914\n",
      "1597/1597 [==============================] - 44s 28ms/step - loss: 0.0135 - accuracy: 0.9962 - val_loss: 0.0113 - val_accuracy: 0.9972 - lr: 5.0000e-04\n",
      "Epoch 133/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0102 - accuracy: 0.9972\n",
      "Epoch 133: val_accuracy did not improve from 0.99914\n",
      "1597/1597 [==============================] - 39s 24ms/step - loss: 0.0102 - accuracy: 0.9972 - val_loss: 0.0071 - val_accuracy: 0.9983 - lr: 5.0000e-04\n",
      "Epoch 134/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.9979\n",
      "Epoch 134: val_accuracy did not improve from 0.99914\n",
      "1597/1597 [==============================] - 43s 27ms/step - loss: 0.0079 - accuracy: 0.9979 - val_loss: 0.0108 - val_accuracy: 0.9979 - lr: 5.0000e-04\n",
      "Epoch 135/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0134 - accuracy: 0.9963\n",
      "Epoch 135: val_accuracy did not improve from 0.99914\n",
      "1597/1597 [==============================] - 40s 25ms/step - loss: 0.0137 - accuracy: 0.9963 - val_loss: 0.0174 - val_accuracy: 0.9971 - lr: 5.0000e-04\n",
      "Epoch 136/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0096 - accuracy: 0.9975\n",
      "Epoch 136: val_accuracy did not improve from 0.99914\n",
      "1597/1597 [==============================] - 43s 27ms/step - loss: 0.0097 - accuracy: 0.9974 - val_loss: 0.0101 - val_accuracy: 0.9982 - lr: 5.0000e-04\n",
      "Epoch 137/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.9977\n",
      "Epoch 137: val_accuracy did not improve from 0.99914\n",
      "1597/1597 [==============================] - 40s 25ms/step - loss: 0.0087 - accuracy: 0.9977 - val_loss: 0.0108 - val_accuracy: 0.9976 - lr: 5.0000e-04\n",
      "Epoch 138/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0098 - accuracy: 0.9974\n",
      "Epoch 138: val_accuracy did not improve from 0.99914\n",
      "1597/1597 [==============================] - 42s 26ms/step - loss: 0.0098 - accuracy: 0.9974 - val_loss: 0.0084 - val_accuracy: 0.9984 - lr: 5.0000e-04\n",
      "Epoch 139/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.9973\n",
      "Epoch 139: val_accuracy did not improve from 0.99914\n",
      "1597/1597 [==============================] - 43s 27ms/step - loss: 0.0095 - accuracy: 0.9973 - val_loss: 0.0042 - val_accuracy: 0.9985 - lr: 5.0000e-04\n",
      "Epoch 140/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0102 - accuracy: 0.9973\n",
      "Epoch 140: val_accuracy did not improve from 0.99914\n",
      "1597/1597 [==============================] - 40s 25ms/step - loss: 0.0102 - accuracy: 0.9973 - val_loss: 0.0070 - val_accuracy: 0.9980 - lr: 5.0000e-04\n",
      "Epoch 141/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9984\n",
      "Epoch 141: val_accuracy did not improve from 0.99914\n",
      "1597/1597 [==============================] - 44s 28ms/step - loss: 0.0048 - accuracy: 0.9984 - val_loss: 0.0090 - val_accuracy: 0.9977 - lr: 5.0000e-04\n",
      "Epoch 142/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0123 - accuracy: 0.9965\n",
      "Epoch 142: val_accuracy did not improve from 0.99914\n",
      "1597/1597 [==============================] - 39s 24ms/step - loss: 0.0123 - accuracy: 0.9965 - val_loss: 0.0066 - val_accuracy: 0.9980 - lr: 5.0000e-04\n",
      "Epoch 143/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0100 - accuracy: 0.9970\n",
      "Epoch 143: val_accuracy did not improve from 0.99914\n",
      "1597/1597 [==============================] - 43s 27ms/step - loss: 0.0100 - accuracy: 0.9970 - val_loss: 0.0126 - val_accuracy: 0.9971 - lr: 5.0000e-04\n",
      "Epoch 144/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0094 - accuracy: 0.9975\n",
      "Epoch 144: val_accuracy did not improve from 0.99914\n",
      "1597/1597 [==============================] - 42s 26ms/step - loss: 0.0094 - accuracy: 0.9975 - val_loss: 0.0052 - val_accuracy: 0.9990 - lr: 5.0000e-04\n",
      "Epoch 145/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0112 - accuracy: 0.9968\n",
      "Epoch 145: val_accuracy did not improve from 0.99914\n",
      "1597/1597 [==============================] - 41s 26ms/step - loss: 0.0112 - accuracy: 0.9968 - val_loss: 0.0224 - val_accuracy: 0.9953 - lr: 5.0000e-04\n",
      "Epoch 146/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0085 - accuracy: 0.9974\n",
      "Epoch 146: val_accuracy did not improve from 0.99914\n",
      "1597/1597 [==============================] - 43s 27ms/step - loss: 0.0085 - accuracy: 0.9974 - val_loss: 0.0125 - val_accuracy: 0.9979 - lr: 5.0000e-04\n",
      "Epoch 147/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0065 - accuracy: 0.9982\n",
      "Epoch 147: val_accuracy did not improve from 0.99914\n",
      "1597/1597 [==============================] - 39s 25ms/step - loss: 0.0065 - accuracy: 0.9982 - val_loss: 0.0224 - val_accuracy: 0.9965 - lr: 5.0000e-04\n",
      "Epoch 148/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0094 - accuracy: 0.9973\n",
      "Epoch 148: val_accuracy did not improve from 0.99914\n",
      "1597/1597 [==============================] - 42s 27ms/step - loss: 0.0094 - accuracy: 0.9973 - val_loss: 0.0120 - val_accuracy: 0.9978 - lr: 5.0000e-04\n",
      "Epoch 149/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9963\n",
      "Epoch 149: val_accuracy did not improve from 0.99914\n",
      "1597/1597 [==============================] - 42s 27ms/step - loss: 0.0147 - accuracy: 0.9963 - val_loss: 0.0156 - val_accuracy: 0.9966 - lr: 5.0000e-04\n",
      "Epoch 150/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0069 - accuracy: 0.9979\n",
      "Epoch 150: val_accuracy did not improve from 0.99914\n",
      "1597/1597 [==============================] - 45s 28ms/step - loss: 0.0069 - accuracy: 0.9979 - val_loss: 0.0099 - val_accuracy: 0.9978 - lr: 5.0000e-04\n",
      "Epoch 151/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0143 - accuracy: 0.9961\n",
      "Epoch 151: val_accuracy did not improve from 0.99914\n",
      "1597/1597 [==============================] - 44s 28ms/step - loss: 0.0143 - accuracy: 0.9961 - val_loss: 0.0067 - val_accuracy: 0.9985 - lr: 5.0000e-04\n",
      "Epoch 152/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0116 - accuracy: 0.9968\n",
      "Epoch 152: val_accuracy did not improve from 0.99914\n",
      "1597/1597 [==============================] - 41s 26ms/step - loss: 0.0116 - accuracy: 0.9968 - val_loss: 0.0146 - val_accuracy: 0.9962 - lr: 5.0000e-04\n",
      "Epoch 153/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.9978\n",
      "Epoch 153: val_accuracy did not improve from 0.99914\n",
      "1597/1597 [==============================] - 42s 26ms/step - loss: 0.0080 - accuracy: 0.9978 - val_loss: 0.0079 - val_accuracy: 0.9984 - lr: 5.0000e-04\n",
      "Epoch 154/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0110 - accuracy: 0.9969\n",
      "Epoch 154: val_accuracy did not improve from 0.99914\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 0.0110 - accuracy: 0.9969 - val_loss: 0.0066 - val_accuracy: 0.9980 - lr: 5.0000e-04\n",
      "Epoch 155/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0091 - accuracy: 0.9974\n",
      "Epoch 155: val_accuracy did not improve from 0.99914\n",
      "1597/1597 [==============================] - 40s 25ms/step - loss: 0.0091 - accuracy: 0.9974 - val_loss: 0.0074 - val_accuracy: 0.9984 - lr: 5.0000e-04\n",
      "Epoch 156/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.9977\n",
      "Epoch 156: val_accuracy did not improve from 0.99914\n",
      "1597/1597 [==============================] - 43s 27ms/step - loss: 0.0083 - accuracy: 0.9977 - val_loss: 0.0099 - val_accuracy: 0.9977 - lr: 5.0000e-04\n",
      "Epoch 157/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0131 - accuracy: 0.9967\n",
      "Epoch 157: val_accuracy did not improve from 0.99914\n",
      "1597/1597 [==============================] - 42s 26ms/step - loss: 0.0131 - accuracy: 0.9967 - val_loss: 0.0054 - val_accuracy: 0.9991 - lr: 5.0000e-04\n",
      "Epoch 158/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0099 - accuracy: 0.9974\n",
      "Epoch 158: val_accuracy did not improve from 0.99914\n",
      "1597/1597 [==============================] - 40s 25ms/step - loss: 0.0099 - accuracy: 0.9974 - val_loss: 0.0089 - val_accuracy: 0.9972 - lr: 5.0000e-04\n",
      "Epoch 159/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9964\n",
      "Epoch 159: val_accuracy did not improve from 0.99914\n",
      "1597/1597 [==============================] - 42s 26ms/step - loss: 0.0120 - accuracy: 0.9964 - val_loss: 0.0113 - val_accuracy: 0.9977 - lr: 5.0000e-04\n",
      "Epoch 160/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0094 - accuracy: 0.9974\n",
      "Epoch 160: val_accuracy did not improve from 0.99914\n",
      "1597/1597 [==============================] - 43s 27ms/step - loss: 0.0094 - accuracy: 0.9974 - val_loss: 0.0124 - val_accuracy: 0.9969 - lr: 5.0000e-04\n",
      "Epoch 161/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0056 - accuracy: 0.9983\n",
      "Epoch 161: val_accuracy improved from 0.99914 to 0.99922, saving model to models\\model.keras\n",
      "1597/1597 [==============================] - 40s 25ms/step - loss: 0.0056 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 0.9992 - lr: 5.0000e-04\n",
      "Epoch 162/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0129 - accuracy: 0.9965\n",
      "Epoch 162: val_accuracy did not improve from 0.99922\n",
      "1597/1597 [==============================] - 44s 27ms/step - loss: 0.0129 - accuracy: 0.9965 - val_loss: 0.0044 - val_accuracy: 0.9984 - lr: 5.0000e-04\n",
      "Epoch 163/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0092 - accuracy: 0.9974\n",
      "Epoch 163: val_accuracy did not improve from 0.99922\n",
      "1597/1597 [==============================] - 40s 25ms/step - loss: 0.0092 - accuracy: 0.9974 - val_loss: 0.0075 - val_accuracy: 0.9978 - lr: 5.0000e-04\n",
      "Epoch 164/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0114 - accuracy: 0.9967\n",
      "Epoch 164: val_accuracy did not improve from 0.99922\n",
      "1597/1597 [==============================] - 42s 26ms/step - loss: 0.0114 - accuracy: 0.9967 - val_loss: 0.0064 - val_accuracy: 0.9984 - lr: 5.0000e-04\n",
      "Epoch 165/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0075 - accuracy: 0.9981\n",
      "Epoch 165: val_accuracy did not improve from 0.99922\n",
      "1597/1597 [==============================] - 62s 39ms/step - loss: 0.0075 - accuracy: 0.9981 - val_loss: 0.0064 - val_accuracy: 0.9984 - lr: 5.0000e-04\n",
      "Epoch 166/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0129 - accuracy: 0.9964\n",
      "Epoch 166: val_accuracy did not improve from 0.99922\n",
      "1597/1597 [==============================] - 72s 45ms/step - loss: 0.0129 - accuracy: 0.9964 - val_loss: 0.0063 - val_accuracy: 0.9983 - lr: 5.0000e-04\n",
      "Epoch 167/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0096 - accuracy: 0.9975\n",
      "Epoch 167: val_accuracy did not improve from 0.99922\n",
      "1597/1597 [==============================] - 52s 32ms/step - loss: 0.0096 - accuracy: 0.9975 - val_loss: 0.0227 - val_accuracy: 0.9959 - lr: 5.0000e-04\n",
      "Epoch 168/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0096 - accuracy: 0.9974\n",
      "Epoch 168: val_accuracy did not improve from 0.99922\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 0.0096 - accuracy: 0.9974 - val_loss: 0.0244 - val_accuracy: 0.9941 - lr: 5.0000e-04\n",
      "Epoch 169/500\n",
      "1594/1597 [============================>.] - ETA: 0s - loss: 0.0093 - accuracy: 0.9973\n",
      "Epoch 169: val_accuracy did not improve from 0.99922\n",
      "1597/1597 [==============================] - 49s 30ms/step - loss: 0.0093 - accuracy: 0.9973 - val_loss: 0.0159 - val_accuracy: 0.9968 - lr: 5.0000e-04\n",
      "Epoch 170/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9967\n",
      "Epoch 170: val_accuracy did not improve from 0.99922\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 0.0129 - accuracy: 0.9967 - val_loss: 0.0067 - val_accuracy: 0.9987 - lr: 5.0000e-04\n",
      "Epoch 171/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0119 - accuracy: 0.9967\n",
      "Epoch 171: val_accuracy did not improve from 0.99922\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 0.0119 - accuracy: 0.9967 - val_loss: 0.0173 - val_accuracy: 0.9968 - lr: 5.0000e-04\n",
      "Epoch 172/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0058 - accuracy: 0.9983\n",
      "Epoch 172: val_accuracy did not improve from 0.99922\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 0.0058 - accuracy: 0.9983 - val_loss: 0.0119 - val_accuracy: 0.9980 - lr: 5.0000e-04\n",
      "Epoch 173/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0147 - accuracy: 0.9964\n",
      "Epoch 173: val_accuracy did not improve from 0.99922\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 0.0146 - accuracy: 0.9964 - val_loss: 0.0185 - val_accuracy: 0.9961 - lr: 5.0000e-04\n",
      "Epoch 174/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0116 - accuracy: 0.9966\n",
      "Epoch 174: val_accuracy did not improve from 0.99922\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 0.0116 - accuracy: 0.9966 - val_loss: 0.0058 - val_accuracy: 0.9985 - lr: 5.0000e-04\n",
      "Epoch 175/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0106 - accuracy: 0.9972\n",
      "Epoch 175: val_accuracy did not improve from 0.99922\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 0.0106 - accuracy: 0.9972 - val_loss: 0.0081 - val_accuracy: 0.9979 - lr: 5.0000e-04\n",
      "Epoch 176/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0084 - accuracy: 0.9976\n",
      "Epoch 176: val_accuracy did not improve from 0.99922\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 0.0084 - accuracy: 0.9976 - val_loss: 0.0094 - val_accuracy: 0.9978 - lr: 5.0000e-04\n",
      "Epoch 177/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0118 - accuracy: 0.9966\n",
      "Epoch 177: val_accuracy did not improve from 0.99922\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 0.0118 - accuracy: 0.9966 - val_loss: 0.0087 - val_accuracy: 0.9977 - lr: 5.0000e-04\n",
      "Epoch 178/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9967\n",
      "Epoch 178: val_accuracy did not improve from 0.99922\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 0.0118 - accuracy: 0.9967 - val_loss: 0.0139 - val_accuracy: 0.9974 - lr: 5.0000e-04\n",
      "Epoch 179/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 0.9973\n",
      "Epoch 179: val_accuracy did not improve from 0.99922\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 0.0096 - accuracy: 0.9973 - val_loss: 0.0130 - val_accuracy: 0.9982 - lr: 5.0000e-04\n",
      "Epoch 180/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0099 - accuracy: 0.9974\n",
      "Epoch 180: val_accuracy did not improve from 0.99922\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 0.0099 - accuracy: 0.9974 - val_loss: 0.0104 - val_accuracy: 0.9971 - lr: 5.0000e-04\n",
      "Epoch 181/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0116 - accuracy: 0.9969\n",
      "Epoch 181: val_accuracy did not improve from 0.99922\n",
      "\n",
      "Epoch 181: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 0.0116 - accuracy: 0.9969 - val_loss: 0.0086 - val_accuracy: 0.9983 - lr: 5.0000e-04\n",
      "Epoch 182/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0050 - accuracy: 0.9986\n",
      "Epoch 182: val_accuracy did not improve from 0.99922\n",
      "1597/1597 [==============================] - 45s 28ms/step - loss: 0.0050 - accuracy: 0.9986 - val_loss: 0.0083 - val_accuracy: 0.9990 - lr: 2.5000e-04\n",
      "Epoch 183/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9993\n",
      "Epoch 183: val_accuracy improved from 0.99922 to 0.99945, saving model to models\\model.keras\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 0.0017 - accuracy: 0.9993 - val_loss: 0.0051 - val_accuracy: 0.9995 - lr: 2.5000e-04\n",
      "Epoch 184/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9994\n",
      "Epoch 184: val_accuracy did not improve from 0.99945\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 0.0017 - accuracy: 0.9994 - val_loss: 0.0056 - val_accuracy: 0.9988 - lr: 2.5000e-04\n",
      "Epoch 185/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 0.9991\n",
      "Epoch 185: val_accuracy did not improve from 0.99945\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 0.0025 - accuracy: 0.9991 - val_loss: 0.0049 - val_accuracy: 0.9988 - lr: 2.5000e-04\n",
      "Epoch 186/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 0.9990\n",
      "Epoch 186: val_accuracy did not improve from 0.99945\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 0.0029 - accuracy: 0.9990 - val_loss: 0.0044 - val_accuracy: 0.9986 - lr: 2.5000e-04\n",
      "Epoch 187/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 0.9994\n",
      "Epoch 187: val_accuracy did not improve from 0.99945\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.0058 - val_accuracy: 0.9988 - lr: 2.5000e-04\n",
      "Epoch 188/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 0.9994\n",
      "Epoch 188: val_accuracy did not improve from 0.99945\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.0058 - val_accuracy: 0.9987 - lr: 2.5000e-04\n",
      "Epoch 189/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9995\n",
      "Epoch 189: val_accuracy did not improve from 0.99945\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.0033 - val_accuracy: 0.9991 - lr: 2.5000e-04\n",
      "Epoch 190/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 0.9994\n",
      "Epoch 190: val_accuracy did not improve from 0.99945\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0051 - val_accuracy: 0.9989 - lr: 2.5000e-04\n",
      "Epoch 191/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 191: val_accuracy did not improve from 0.99945\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.0036 - val_accuracy: 0.9993 - lr: 2.5000e-04\n",
      "Epoch 192/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 192: val_accuracy did not improve from 0.99945\n",
      "1597/1597 [==============================] - 49s 30ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.0141 - val_accuracy: 0.9987 - lr: 2.5000e-04\n",
      "Epoch 193/500\n",
      "1594/1597 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9994\n",
      "Epoch 193: val_accuracy improved from 0.99945 to 0.99953, saving model to models\\model.keras\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 0.0016 - accuracy: 0.9994 - val_loss: 0.0052 - val_accuracy: 0.9995 - lr: 2.5000e-04\n",
      "Epoch 194/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9994\n",
      "Epoch 194: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.0114 - val_accuracy: 0.9983 - lr: 2.5000e-04\n",
      "Epoch 195/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 7.6671e-04 - accuracy: 0.9998\n",
      "Epoch 195: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 7.6668e-04 - accuracy: 0.9998 - val_loss: 0.0055 - val_accuracy: 0.9991 - lr: 2.5000e-04\n",
      "Epoch 196/500\n",
      "1594/1597 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9995\n",
      "Epoch 196: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.0041 - val_accuracy: 0.9990 - lr: 2.5000e-04\n",
      "Epoch 197/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9990\n",
      "Epoch 197: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.0040 - val_accuracy: 0.9992 - lr: 2.5000e-04\n",
      "Epoch 198/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9996\n",
      "Epoch 198: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 0.0030 - val_accuracy: 0.9995 - lr: 2.5000e-04\n",
      "Epoch 199/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9993\n",
      "Epoch 199: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.0040 - val_accuracy: 0.9991 - lr: 2.5000e-04\n",
      "Epoch 200/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9995\n",
      "Epoch 200: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.0041 - val_accuracy: 0.9990 - lr: 2.5000e-04\n",
      "Epoch 201/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 0.9991\n",
      "Epoch 201: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.0053 - val_accuracy: 0.9992 - lr: 2.5000e-04\n",
      "Epoch 202/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9995\n",
      "Epoch 202: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 0.0026 - val_accuracy: 0.9991 - lr: 2.5000e-04\n",
      "Epoch 203/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9993\n",
      "Epoch 203: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 49s 30ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.0035 - val_accuracy: 0.9992 - lr: 2.5000e-04\n",
      "Epoch 204/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 204: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 45s 28ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.0031 - val_accuracy: 0.9995 - lr: 2.5000e-04\n",
      "Epoch 205/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 0.9991\n",
      "Epoch 205: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.0029 - val_accuracy: 0.9994 - lr: 2.5000e-04\n",
      "Epoch 206/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9994\n",
      "Epoch 206: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.0033 - val_accuracy: 0.9992 - lr: 2.5000e-04\n",
      "Epoch 207/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 0.9995\n",
      "Epoch 207: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.0036 - val_accuracy: 0.9994 - lr: 2.5000e-04\n",
      "Epoch 208/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 0.9993\n",
      "Epoch 208: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0163 - val_accuracy: 0.9982 - lr: 2.5000e-04\n",
      "Epoch 209/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0030 - accuracy: 0.9993\n",
      "Epoch 209: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0026 - val_accuracy: 0.9995 - lr: 2.5000e-04\n",
      "Epoch 210/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 0.9992\n",
      "Epoch 210: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 49s 30ms/step - loss: 0.0029 - accuracy: 0.9992 - val_loss: 0.0040 - val_accuracy: 0.9989 - lr: 2.5000e-04\n",
      "Epoch 211/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 0.9996\n",
      "Epoch 211: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 0.0051 - val_accuracy: 0.9991 - lr: 2.5000e-04\n",
      "Epoch 212/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9994\n",
      "Epoch 212: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.0094 - val_accuracy: 0.9987 - lr: 2.5000e-04\n",
      "Epoch 213/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9995\n",
      "Epoch 213: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 56s 35ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.0088 - val_accuracy: 0.9987 - lr: 2.5000e-04\n",
      "Epoch 214/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9995\n",
      "Epoch 214: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 50s 31ms/step - loss: 0.0012 - accuracy: 0.9995 - val_loss: 0.0089 - val_accuracy: 0.9987 - lr: 2.5000e-04\n",
      "Epoch 215/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 0.9996\n",
      "Epoch 215: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 0.0052 - val_accuracy: 0.9991 - lr: 2.5000e-04\n",
      "Epoch 216/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9995\n",
      "Epoch 216: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 50s 31ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.0091 - val_accuracy: 0.9990 - lr: 2.5000e-04\n",
      "Epoch 217/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 0.9992\n",
      "Epoch 217: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.0128 - val_accuracy: 0.9986 - lr: 2.5000e-04\n",
      "Epoch 218/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9995\n",
      "Epoch 218: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.0054 - val_accuracy: 0.9995 - lr: 2.5000e-04\n",
      "Epoch 219/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9993\n",
      "Epoch 219: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0112 - val_accuracy: 0.9984 - lr: 2.5000e-04\n",
      "Epoch 220/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0028 - accuracy: 0.9993\n",
      "Epoch 220: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.0034 - val_accuracy: 0.9994 - lr: 2.5000e-04\n",
      "Epoch 221/500\n",
      "1594/1597 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9994\n",
      "Epoch 221: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.0093 - val_accuracy: 0.9991 - lr: 2.5000e-04\n",
      "Epoch 222/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9990\n",
      "Epoch 222: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 0.0038 - accuracy: 0.9990 - val_loss: 0.0067 - val_accuracy: 0.9989 - lr: 2.5000e-04\n",
      "Epoch 223/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 8.9146e-04 - accuracy: 0.9997\n",
      "Epoch 223: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 8.9143e-04 - accuracy: 0.9997 - val_loss: 0.0070 - val_accuracy: 0.9995 - lr: 2.5000e-04\n",
      "Epoch 224/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9995\n",
      "Epoch 224: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 0.0016 - accuracy: 0.9995 - val_loss: 0.0080 - val_accuracy: 0.9993 - lr: 2.5000e-04\n",
      "Epoch 225/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9995\n",
      "Epoch 225: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0077 - val_accuracy: 0.9992 - lr: 2.5000e-04\n",
      "Epoch 226/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9993\n",
      "Epoch 226: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 0.0020 - accuracy: 0.9993 - val_loss: 0.0069 - val_accuracy: 0.9991 - lr: 2.5000e-04\n",
      "Epoch 227/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0030 - accuracy: 0.9994\n",
      "Epoch 227: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0097 - val_accuracy: 0.9990 - lr: 2.5000e-04\n",
      "Epoch 228/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 0.9993\n",
      "Epoch 228: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0068 - val_accuracy: 0.9986 - lr: 2.5000e-04\n",
      "Epoch 229/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 229: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.0031 - val_accuracy: 0.9995 - lr: 2.5000e-04\n",
      "Epoch 230/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 0.9994\n",
      "Epoch 230: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 50s 31ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0082 - val_accuracy: 0.9984 - lr: 2.5000e-04\n",
      "Epoch 231/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 231: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.0024 - val_accuracy: 0.9995 - lr: 2.5000e-04\n",
      "Epoch 232/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 0.9994\n",
      "Epoch 232: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0068 - val_accuracy: 0.9993 - lr: 2.5000e-04\n",
      "Epoch 233/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 0.9995\n",
      "Epoch 233: val_accuracy did not improve from 0.99953\n",
      "\n",
      "Epoch 233: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.0045 - val_accuracy: 0.9989 - lr: 2.5000e-04\n",
      "Epoch 234/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 234: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.0046 - val_accuracy: 0.9989 - lr: 1.2500e-04\n",
      "Epoch 235/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 8.6699e-04 - accuracy: 0.9997\n",
      "Epoch 235: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 8.6696e-04 - accuracy: 0.9997 - val_loss: 0.0056 - val_accuracy: 0.9993 - lr: 1.2500e-04\n",
      "Epoch 236/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 5.6401e-04 - accuracy: 0.9997\n",
      "Epoch 236: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 5.6399e-04 - accuracy: 0.9997 - val_loss: 0.0071 - val_accuracy: 0.9992 - lr: 1.2500e-04\n",
      "Epoch 237/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 5.3778e-04 - accuracy: 0.9998\n",
      "Epoch 237: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 5.3743e-04 - accuracy: 0.9998 - val_loss: 0.0054 - val_accuracy: 0.9995 - lr: 1.2500e-04\n",
      "Epoch 238/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 6.7768e-04 - accuracy: 0.9998\n",
      "Epoch 238: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 6.7766e-04 - accuracy: 0.9998 - val_loss: 0.0062 - val_accuracy: 0.9995 - lr: 1.2500e-04\n",
      "Epoch 239/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 2.8260e-04 - accuracy: 0.9999\n",
      "Epoch 239: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 2.8259e-04 - accuracy: 0.9999 - val_loss: 0.0052 - val_accuracy: 0.9994 - lr: 1.2500e-04\n",
      "Epoch 240/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 6.4991e-04 - accuracy: 0.9998\n",
      "Epoch 240: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 60s 38ms/step - loss: 6.4989e-04 - accuracy: 0.9998 - val_loss: 0.0050 - val_accuracy: 0.9991 - lr: 1.2500e-04\n",
      "Epoch 241/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 2.1112e-04 - accuracy: 0.9999\n",
      "Epoch 241: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 2.1111e-04 - accuracy: 0.9999 - val_loss: 0.0040 - val_accuracy: 0.9995 - lr: 1.2500e-04\n",
      "Epoch 242/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 3.0570e-04 - accuracy: 1.0000\n",
      "Epoch 242: val_accuracy did not improve from 0.99953\n",
      "1597/1597 [==============================] - 50s 32ms/step - loss: 3.0570e-04 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 0.9995 - lr: 1.2500e-04\n",
      "Epoch 243/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 1.7190e-04 - accuracy: 1.0000\n",
      "Epoch 243: val_accuracy improved from 0.99953 to 0.99961, saving model to models\\model.keras\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 1.7189e-04 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 0.9996 - lr: 1.2500e-04\n",
      "Epoch 244/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 7.8814e-04 - accuracy: 0.9998\n",
      "Epoch 244: val_accuracy did not improve from 0.99961\n",
      "1597/1597 [==============================] - 45s 28ms/step - loss: 7.8814e-04 - accuracy: 0.9998 - val_loss: 0.0035 - val_accuracy: 0.9994 - lr: 1.2500e-04\n",
      "Epoch 245/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 5.1282e-04 - accuracy: 0.9998\n",
      "Epoch 245: val_accuracy did not improve from 0.99961\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 5.1248e-04 - accuracy: 0.9998 - val_loss: 0.0031 - val_accuracy: 0.9996 - lr: 1.2500e-04\n",
      "Epoch 246/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 6.6275e-04 - accuracy: 0.9999\n",
      "Epoch 246: val_accuracy did not improve from 0.99961\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 6.6231e-04 - accuracy: 0.9999 - val_loss: 0.0047 - val_accuracy: 0.9995 - lr: 1.2500e-04\n",
      "Epoch 247/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 2.1870e-04 - accuracy: 0.9999\n",
      "Epoch 247: val_accuracy did not improve from 0.99961\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 2.1870e-04 - accuracy: 0.9999 - val_loss: 0.0063 - val_accuracy: 0.9994 - lr: 1.2500e-04\n",
      "Epoch 248/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 1.7199e-04 - accuracy: 0.9999\n",
      "Epoch 248: val_accuracy did not improve from 0.99961\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 1.7198e-04 - accuracy: 0.9999 - val_loss: 0.0062 - val_accuracy: 0.9995 - lr: 1.2500e-04\n",
      "Epoch 249/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 6.0470e-04 - accuracy: 0.9998\n",
      "Epoch 249: val_accuracy improved from 0.99961 to 0.99977, saving model to models\\model.keras\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 6.0468e-04 - accuracy: 0.9998 - val_loss: 0.0036 - val_accuracy: 0.9998 - lr: 1.2500e-04\n",
      "Epoch 250/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 1.6770e-04 - accuracy: 0.9999\n",
      "Epoch 250: val_accuracy improved from 0.99977 to 0.99984, saving model to models\\model.keras\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 1.6770e-04 - accuracy: 0.9999 - val_loss: 0.0022 - val_accuracy: 0.9998 - lr: 1.2500e-04\n",
      "Epoch 251/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 6.8303e-05 - accuracy: 1.0000\n",
      "Epoch 251: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 45s 28ms/step - loss: 6.8300e-05 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 0.9998 - lr: 1.2500e-04\n",
      "Epoch 252/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 2.1862e-04 - accuracy: 0.9999\n",
      "Epoch 252: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 2.1862e-04 - accuracy: 0.9999 - val_loss: 0.0045 - val_accuracy: 0.9994 - lr: 1.2500e-04\n",
      "Epoch 253/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 7.4187e-04 - accuracy: 0.9998\n",
      "Epoch 253: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 7.4184e-04 - accuracy: 0.9998 - val_loss: 0.0057 - val_accuracy: 0.9997 - lr: 1.2500e-04\n",
      "Epoch 254/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 3.5503e-04 - accuracy: 0.9999\n",
      "Epoch 254: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 3.5502e-04 - accuracy: 0.9999 - val_loss: 0.0051 - val_accuracy: 0.9998 - lr: 1.2500e-04\n",
      "Epoch 255/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 2.1851e-04 - accuracy: 0.9999\n",
      "Epoch 255: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 49s 30ms/step - loss: 2.1850e-04 - accuracy: 0.9999 - val_loss: 0.0055 - val_accuracy: 0.9997 - lr: 1.2500e-04\n",
      "Epoch 256/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 1.8365e-04 - accuracy: 0.9999\n",
      "Epoch 256: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 45s 28ms/step - loss: 1.8365e-04 - accuracy: 0.9999 - val_loss: 0.0049 - val_accuracy: 0.9995 - lr: 1.2500e-04\n",
      "Epoch 257/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 7.9255e-04 - accuracy: 0.9998\n",
      "Epoch 257: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 7.9202e-04 - accuracy: 0.9998 - val_loss: 0.0061 - val_accuracy: 0.9996 - lr: 1.2500e-04\n",
      "Epoch 258/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 6.6390e-04 - accuracy: 0.9999\n",
      "Epoch 258: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 6.6346e-04 - accuracy: 0.9999 - val_loss: 0.0039 - val_accuracy: 0.9997 - lr: 1.2500e-04\n",
      "Epoch 259/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 3.5306e-04 - accuracy: 0.9999\n",
      "Epoch 259: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 3.5304e-04 - accuracy: 0.9999 - val_loss: 0.0047 - val_accuracy: 0.9998 - lr: 1.2500e-04\n",
      "Epoch 260/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 1.2238e-04 - accuracy: 1.0000\n",
      "Epoch 260: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 45s 28ms/step - loss: 1.2238e-04 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 0.9998 - lr: 1.2500e-04\n",
      "Epoch 261/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 1.2931e-04 - accuracy: 1.0000\n",
      "Epoch 261: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 49s 30ms/step - loss: 1.2931e-04 - accuracy: 1.0000 - val_loss: 8.6126e-04 - val_accuracy: 0.9997 - lr: 1.2500e-04\n",
      "Epoch 262/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 2.1799e-04 - accuracy: 0.9999\n",
      "Epoch 262: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 2.1799e-04 - accuracy: 0.9999 - val_loss: 0.0028 - val_accuracy: 0.9997 - lr: 1.2500e-04\n",
      "Epoch 263/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 5.5407e-04 - accuracy: 0.9998\n",
      "Epoch 263: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 5.5405e-04 - accuracy: 0.9998 - val_loss: 0.0046 - val_accuracy: 0.9996 - lr: 1.2500e-04\n",
      "Epoch 264/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 2.1107e-04 - accuracy: 0.9999\n",
      "Epoch 264: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 2.1093e-04 - accuracy: 0.9999 - val_loss: 0.0066 - val_accuracy: 0.9994 - lr: 1.2500e-04\n",
      "Epoch 265/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 3.2031e-04 - accuracy: 0.9999\n",
      "Epoch 265: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 45s 28ms/step - loss: 3.2030e-04 - accuracy: 0.9999 - val_loss: 0.0044 - val_accuracy: 0.9995 - lr: 1.2500e-04\n",
      "Epoch 266/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 1.4344e-04 - accuracy: 0.9999\n",
      "Epoch 266: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 50s 31ms/step - loss: 1.4343e-04 - accuracy: 0.9999 - val_loss: 0.0020 - val_accuracy: 0.9997 - lr: 1.2500e-04\n",
      "Epoch 267/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 4.6885e-04 - accuracy: 0.9999\n",
      "Epoch 267: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 4.6883e-04 - accuracy: 0.9999 - val_loss: 0.0079 - val_accuracy: 0.9996 - lr: 1.2500e-04\n",
      "Epoch 268/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 3.8575e-04 - accuracy: 0.9999\n",
      "Epoch 268: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 45s 28ms/step - loss: 3.8549e-04 - accuracy: 0.9999 - val_loss: 0.0056 - val_accuracy: 0.9996 - lr: 1.2500e-04\n",
      "Epoch 269/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 4.3312e-04 - accuracy: 0.9998\n",
      "Epoch 269: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 4.3312e-04 - accuracy: 0.9998 - val_loss: 0.0053 - val_accuracy: 0.9995 - lr: 1.2500e-04\n",
      "Epoch 270/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 3.7805e-04 - accuracy: 0.9999\n",
      "Epoch 270: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 3.7804e-04 - accuracy: 0.9999 - val_loss: 0.0041 - val_accuracy: 0.9998 - lr: 1.2500e-04\n",
      "Epoch 271/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 2.3454e-04 - accuracy: 0.9999\n",
      "Epoch 271: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 45s 28ms/step - loss: 2.3438e-04 - accuracy: 0.9999 - val_loss: 0.0038 - val_accuracy: 0.9998 - lr: 1.2500e-04\n",
      "Epoch 272/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 3.3995e-04 - accuracy: 0.9999\n",
      "Epoch 272: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 3.3973e-04 - accuracy: 0.9999 - val_loss: 0.0034 - val_accuracy: 0.9995 - lr: 1.2500e-04\n",
      "Epoch 273/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 4.2865e-04 - accuracy: 0.9999\n",
      "Epoch 273: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 4.2863e-04 - accuracy: 0.9999 - val_loss: 0.0033 - val_accuracy: 0.9995 - lr: 1.2500e-04\n",
      "Epoch 274/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 3.1783e-04 - accuracy: 0.9999\n",
      "Epoch 274: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 52s 33ms/step - loss: 3.1762e-04 - accuracy: 0.9999 - val_loss: 0.0043 - val_accuracy: 0.9998 - lr: 1.2500e-04\n",
      "Epoch 275/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 6.7085e-04 - accuracy: 0.9998\n",
      "Epoch 275: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 6.7083e-04 - accuracy: 0.9998 - val_loss: 0.0027 - val_accuracy: 0.9998 - lr: 1.2500e-04\n",
      "Epoch 276/500\n",
      "1594/1597 [============================>.] - ETA: 0s - loss: 4.8206e-04 - accuracy: 0.9999\n",
      "Epoch 276: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 50s 31ms/step - loss: 4.8144e-04 - accuracy: 0.9999 - val_loss: 0.0036 - val_accuracy: 0.9995 - lr: 1.2500e-04\n",
      "Epoch 277/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 4.5777e-04 - accuracy: 0.9998\n",
      "Epoch 277: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 4.5775e-04 - accuracy: 0.9998 - val_loss: 0.0037 - val_accuracy: 0.9996 - lr: 1.2500e-04\n",
      "Epoch 278/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 4.6718e-04 - accuracy: 0.9998\n",
      "Epoch 278: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 50s 31ms/step - loss: 4.6716e-04 - accuracy: 0.9998 - val_loss: 0.0033 - val_accuracy: 0.9995 - lr: 1.2500e-04\n",
      "Epoch 279/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 7.5946e-05 - accuracy: 1.0000\n",
      "Epoch 279: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 45s 28ms/step - loss: 7.5895e-05 - accuracy: 1.0000 - val_loss: 0.0071 - val_accuracy: 0.9996 - lr: 1.2500e-04\n",
      "Epoch 280/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 5.2370e-05 - accuracy: 1.0000\n",
      "Epoch 280: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 49s 30ms/step - loss: 5.2370e-05 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 0.9995 - lr: 1.2500e-04\n",
      "Epoch 281/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 3.6561e-04 - accuracy: 0.9999\n",
      "Epoch 281: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 45s 28ms/step - loss: 3.6559e-04 - accuracy: 0.9999 - val_loss: 0.0013 - val_accuracy: 0.9998 - lr: 1.2500e-04\n",
      "Epoch 282/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 3.5246e-04 - accuracy: 0.9999\n",
      "Epoch 282: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 3.5244e-04 - accuracy: 0.9999 - val_loss: 0.0043 - val_accuracy: 0.9996 - lr: 1.2500e-04\n",
      "Epoch 283/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 4.3857e-04 - accuracy: 0.9999\n",
      "Epoch 283: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 45s 28ms/step - loss: 4.3855e-04 - accuracy: 0.9999 - val_loss: 0.0049 - val_accuracy: 0.9998 - lr: 1.2500e-04\n",
      "Epoch 284/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 5.5623e-05 - accuracy: 1.0000\n",
      "Epoch 284: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 5.5623e-05 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 0.9996 - lr: 1.2500e-04\n",
      "Epoch 285/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 1.0757e-04 - accuracy: 1.0000\n",
      "Epoch 285: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 1.0749e-04 - accuracy: 1.0000 - val_loss: 0.0068 - val_accuracy: 0.9995 - lr: 1.2500e-04\n",
      "Epoch 286/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 1.9334e-04 - accuracy: 1.0000\n",
      "Epoch 286: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 1.9321e-04 - accuracy: 1.0000 - val_loss: 0.0038 - val_accuracy: 0.9994 - lr: 1.2500e-04\n",
      "Epoch 287/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 2.2175e-04 - accuracy: 0.9999\n",
      "Epoch 287: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 2.2175e-04 - accuracy: 0.9999 - val_loss: 0.0061 - val_accuracy: 0.9992 - lr: 1.2500e-04\n",
      "Epoch 288/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 2.1114e-04 - accuracy: 0.9999\n",
      "Epoch 288: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 2.1100e-04 - accuracy: 0.9999 - val_loss: 0.0061 - val_accuracy: 0.9995 - lr: 1.2500e-04\n",
      "Epoch 289/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 6.9910e-05 - accuracy: 1.0000\n",
      "Epoch 289: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 6.9908e-05 - accuracy: 1.0000 - val_loss: 0.0080 - val_accuracy: 0.9992 - lr: 1.2500e-04\n",
      "Epoch 290/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 1.9497e-04 - accuracy: 0.9999\n",
      "Epoch 290: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 1.9496e-04 - accuracy: 0.9999 - val_loss: 0.0097 - val_accuracy: 0.9990 - lr: 1.2500e-04\n",
      "Epoch 291/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 3.7090e-04 - accuracy: 0.9999\n",
      "Epoch 291: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 3.7090e-04 - accuracy: 0.9999 - val_loss: 0.0076 - val_accuracy: 0.9991 - lr: 1.2500e-04\n",
      "Epoch 292/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 3.5615e-04 - accuracy: 0.9998\n",
      "Epoch 292: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 3.5614e-04 - accuracy: 0.9998 - val_loss: 0.0051 - val_accuracy: 0.9995 - lr: 1.2500e-04\n",
      "Epoch 293/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 3.3094e-04 - accuracy: 0.9999\n",
      "Epoch 293: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 3.3072e-04 - accuracy: 0.9999 - val_loss: 0.0043 - val_accuracy: 0.9996 - lr: 1.2500e-04\n",
      "Epoch 294/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 5.1128e-04 - accuracy: 0.9999\n",
      "Epoch 294: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 5.1094e-04 - accuracy: 0.9999 - val_loss: 0.0050 - val_accuracy: 0.9996 - lr: 1.2500e-04\n",
      "Epoch 295/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 2.4131e-04 - accuracy: 1.0000\n",
      "Epoch 295: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 2.4115e-04 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 0.9996 - lr: 1.2500e-04\n",
      "Epoch 296/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 1.6931e-04 - accuracy: 1.0000\n",
      "Epoch 296: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 1.6930e-04 - accuracy: 1.0000 - val_loss: 0.0044 - val_accuracy: 0.9997 - lr: 1.2500e-04\n",
      "Epoch 297/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 1.6275e-04 - accuracy: 0.9999\n",
      "Epoch 297: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 1.6275e-04 - accuracy: 0.9999 - val_loss: 0.0060 - val_accuracy: 0.9995 - lr: 1.2500e-04\n",
      "Epoch 298/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 6.4593e-04 - accuracy: 0.9998\n",
      "Epoch 298: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 6.4590e-04 - accuracy: 0.9998 - val_loss: 0.0079 - val_accuracy: 0.9993 - lr: 1.2500e-04\n",
      "Epoch 299/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 4.8387e-04 - accuracy: 0.9999\n",
      "Epoch 299: val_accuracy did not improve from 0.99984\n",
      "\n",
      "Epoch 299: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 4.8387e-04 - accuracy: 0.9999 - val_loss: 0.0065 - val_accuracy: 0.9993 - lr: 1.2500e-04\n",
      "Epoch 300/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 7.6074e-05 - accuracy: 1.0000\n",
      "Epoch 300: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 7.6071e-05 - accuracy: 1.0000 - val_loss: 0.0073 - val_accuracy: 0.9993 - lr: 6.2500e-05\n",
      "Epoch 301/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 5.4603e-04 - accuracy: 0.9998\n",
      "Epoch 301: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 5.4567e-04 - accuracy: 0.9998 - val_loss: 0.0064 - val_accuracy: 0.9993 - lr: 6.2500e-05\n",
      "Epoch 302/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 1.5975e-04 - accuracy: 1.0000\n",
      "Epoch 302: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 1.5964e-04 - accuracy: 1.0000 - val_loss: 0.0055 - val_accuracy: 0.9994 - lr: 6.2500e-05\n",
      "Epoch 303/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 4.2086e-05 - accuracy: 1.0000\n",
      "Epoch 303: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 4.2058e-05 - accuracy: 1.0000 - val_loss: 0.0065 - val_accuracy: 0.9994 - lr: 6.2500e-05\n",
      "Epoch 304/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 2.3659e-04 - accuracy: 0.9999\n",
      "Epoch 304: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 2.3659e-04 - accuracy: 0.9999 - val_loss: 0.0080 - val_accuracy: 0.9993 - lr: 6.2500e-05\n",
      "Epoch 305/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 1.9017e-04 - accuracy: 0.9999\n",
      "Epoch 305: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 1.9016e-04 - accuracy: 0.9999 - val_loss: 0.0079 - val_accuracy: 0.9994 - lr: 6.2500e-05\n",
      "Epoch 306/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 8.7785e-05 - accuracy: 1.0000\n",
      "Epoch 306: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 8.7782e-05 - accuracy: 1.0000 - val_loss: 0.0058 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 307/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 2.8229e-05 - accuracy: 1.0000\n",
      "Epoch 307: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 2.8228e-05 - accuracy: 1.0000 - val_loss: 0.0056 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 308/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 4.6797e-05 - accuracy: 1.0000\n",
      "Epoch 308: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 4.6795e-05 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 0.9993 - lr: 6.2500e-05\n",
      "Epoch 309/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 5.1975e-05 - accuracy: 1.0000\n",
      "Epoch 309: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 5.1975e-05 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 0.9993 - lr: 6.2500e-05\n",
      "Epoch 310/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 1.0169e-04 - accuracy: 1.0000\n",
      "Epoch 310: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 1.0169e-04 - accuracy: 1.0000 - val_loss: 0.0061 - val_accuracy: 0.9994 - lr: 6.2500e-05\n",
      "Epoch 311/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 5.9758e-05 - accuracy: 1.0000\n",
      "Epoch 311: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 5.9718e-05 - accuracy: 1.0000 - val_loss: 0.0064 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 312/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 1.5351e-04 - accuracy: 0.9999\n",
      "Epoch 312: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 1.5351e-04 - accuracy: 0.9999 - val_loss: 0.0045 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 313/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 1.0815e-04 - accuracy: 0.9999\n",
      "Epoch 313: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 1.0815e-04 - accuracy: 0.9999 - val_loss: 0.0041 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 314/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 6.2449e-05 - accuracy: 1.0000\n",
      "Epoch 314: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 6.2446e-05 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 315/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 3.4895e-05 - accuracy: 1.0000\n",
      "Epoch 315: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 51s 32ms/step - loss: 3.4894e-05 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 316/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 1.4203e-04 - accuracy: 1.0000\n",
      "Epoch 316: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 49s 30ms/step - loss: 1.4194e-04 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 0.9994 - lr: 6.2500e-05\n",
      "Epoch 317/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 7.6825e-05 - accuracy: 1.0000\n",
      "Epoch 317: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 7.6825e-05 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 0.9993 - lr: 6.2500e-05\n",
      "Epoch 318/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 3.3341e-05 - accuracy: 1.0000\n",
      "Epoch 318: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 51s 32ms/step - loss: 3.3339e-05 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 0.9994 - lr: 6.2500e-05\n",
      "Epoch 319/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 1.8868e-04 - accuracy: 1.0000\n",
      "Epoch 319: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 45s 28ms/step - loss: 1.8856e-04 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 320/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 2.6495e-05 - accuracy: 1.0000\n",
      "Epoch 320: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 2.6495e-05 - accuracy: 1.0000 - val_loss: 0.0054 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 321/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 8.0964e-05 - accuracy: 1.0000\n",
      "Epoch 321: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 8.0910e-05 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 322/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 1.8721e-05 - accuracy: 1.0000\n",
      "Epoch 322: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 1.8721e-05 - accuracy: 1.0000 - val_loss: 0.0038 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 323/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 6.3452e-05 - accuracy: 1.0000\n",
      "Epoch 323: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 6.3410e-05 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 324/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 5.5021e-05 - accuracy: 1.0000\n",
      "Epoch 324: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 5.5021e-05 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 325/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 2.9374e-06 - accuracy: 1.0000\n",
      "Epoch 325: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 2.9355e-06 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 326/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 5.6801e-06 - accuracy: 1.0000\n",
      "Epoch 326: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 5.6801e-06 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 327/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 2.1971e-05 - accuracy: 1.0000\n",
      "Epoch 327: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 2.1971e-05 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 328/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 1.1020e-05 - accuracy: 1.0000\n",
      "Epoch 328: val_accuracy did not improve from 0.99984\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 1.1020e-05 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 0.9996 - lr: 6.2500e-05\n",
      "Epoch 329/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 1.1673e-04 - accuracy: 1.0000\n",
      "Epoch 329: val_accuracy improved from 0.99984 to 0.99992, saving model to models\\model.keras\n",
      "1597/1597 [==============================] - 49s 30ms/step - loss: 1.1666e-04 - accuracy: 1.0000 - val_loss: 2.3673e-04 - val_accuracy: 0.9999 - lr: 6.2500e-05\n",
      "Epoch 330/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 4.6217e-05 - accuracy: 1.0000\n",
      "Epoch 330: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 4.6217e-05 - accuracy: 1.0000 - val_loss: 9.7961e-04 - val_accuracy: 0.9997 - lr: 6.2500e-05\n",
      "Epoch 331/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 4.7375e-06 - accuracy: 1.0000\n",
      "Epoch 331: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 4.7373e-06 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 332/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 1.0357e-05 - accuracy: 1.0000\n",
      "Epoch 332: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 1.0350e-05 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 0.9997 - lr: 6.2500e-05\n",
      "Epoch 333/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 3.3680e-05 - accuracy: 1.0000\n",
      "Epoch 333: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 3.3658e-05 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 0.9997 - lr: 6.2500e-05\n",
      "Epoch 334/500\n",
      "1594/1597 [============================>.] - ETA: 0s - loss: 1.4301e-04 - accuracy: 1.0000\n",
      "Epoch 334: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 56s 35ms/step - loss: 1.4282e-04 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 0.9997 - lr: 6.2500e-05\n",
      "Epoch 335/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 1.7094e-05 - accuracy: 1.0000\n",
      "Epoch 335: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 1.7083e-05 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 0.9997 - lr: 6.2500e-05\n",
      "Epoch 336/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 4.3642e-05 - accuracy: 1.0000\n",
      "Epoch 336: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 4.3640e-05 - accuracy: 1.0000 - val_loss: 0.0044 - val_accuracy: 0.9997 - lr: 6.2500e-05\n",
      "Epoch 337/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 8.9958e-05 - accuracy: 1.0000\n",
      "Epoch 337: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 8.9898e-05 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 0.9997 - lr: 6.2500e-05\n",
      "Epoch 338/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 2.2688e-04 - accuracy: 1.0000\n",
      "Epoch 338: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 2.2673e-04 - accuracy: 1.0000 - val_loss: 0.0055 - val_accuracy: 0.9996 - lr: 6.2500e-05\n",
      "Epoch 339/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 6.4355e-05 - accuracy: 1.0000\n",
      "Epoch 339: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 6.4352e-05 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 0.9997 - lr: 6.2500e-05\n",
      "Epoch 340/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 3.3518e-05 - accuracy: 1.0000\n",
      "Epoch 340: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 3.3496e-05 - accuracy: 1.0000 - val_loss: 0.0054 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 341/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 9.9111e-06 - accuracy: 1.0000\n",
      "Epoch 341: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 9.9107e-06 - accuracy: 1.0000 - val_loss: 0.0064 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 342/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 1.0729e-05 - accuracy: 1.0000\n",
      "Epoch 342: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 1.0729e-05 - accuracy: 1.0000 - val_loss: 0.0063 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 343/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 1.0962e-04 - accuracy: 1.0000\n",
      "Epoch 343: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 1.0954e-04 - accuracy: 1.0000 - val_loss: 0.0080 - val_accuracy: 0.9994 - lr: 6.2500e-05\n",
      "Epoch 344/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 8.1210e-06 - accuracy: 1.0000\n",
      "Epoch 344: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 8.1210e-06 - accuracy: 1.0000 - val_loss: 0.0068 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 345/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 6.3232e-05 - accuracy: 1.0000\n",
      "Epoch 345: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 6.3190e-05 - accuracy: 1.0000 - val_loss: 0.0068 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 346/500\n",
      "1594/1597 [============================>.] - ETA: 0s - loss: 9.2534e-05 - accuracy: 1.0000\n",
      "Epoch 346: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 9.2415e-05 - accuracy: 1.0000 - val_loss: 0.0066 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 347/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 2.2873e-05 - accuracy: 1.0000\n",
      "Epoch 347: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 2.2857e-05 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 348/500\n",
      "1594/1597 [============================>.] - ETA: 0s - loss: 2.5881e-05 - accuracy: 1.0000\n",
      "Epoch 348: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 2.5848e-05 - accuracy: 1.0000 - val_loss: 0.0068 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 349/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 8.8081e-06 - accuracy: 1.0000\n",
      "Epoch 349: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 8.8078e-06 - accuracy: 1.0000 - val_loss: 0.0062 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 350/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 7.4422e-06 - accuracy: 1.0000\n",
      "Epoch 350: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 45s 28ms/step - loss: 7.4419e-06 - accuracy: 1.0000 - val_loss: 0.0061 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 351/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 1.1144e-04 - accuracy: 1.0000\n",
      "Epoch 351: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 1.1144e-04 - accuracy: 1.0000 - val_loss: 0.0079 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 352/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 9.8395e-05 - accuracy: 1.0000\n",
      "Epoch 352: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 9.8391e-05 - accuracy: 1.0000 - val_loss: 0.0057 - val_accuracy: 0.9996 - lr: 6.2500e-05\n",
      "Epoch 353/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 7.2185e-05 - accuracy: 1.0000\n",
      "Epoch 353: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 7.2139e-05 - accuracy: 1.0000 - val_loss: 0.0049 - val_accuracy: 0.9996 - lr: 6.2500e-05\n",
      "Epoch 354/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 2.6477e-05 - accuracy: 1.0000\n",
      "Epoch 354: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 2.6477e-05 - accuracy: 1.0000 - val_loss: 0.0060 - val_accuracy: 0.9996 - lr: 6.2500e-05\n",
      "Epoch 355/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 6.9863e-06 - accuracy: 1.0000\n",
      "Epoch 355: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 6.9817e-06 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 0.9996 - lr: 6.2500e-05\n",
      "Epoch 356/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 8.0675e-06 - accuracy: 1.0000\n",
      "Epoch 356: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 8.0621e-06 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 0.9998 - lr: 6.2500e-05\n",
      "Epoch 357/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 4.3391e-05 - accuracy: 1.0000\n",
      "Epoch 357: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 4.3389e-05 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 0.9994 - lr: 6.2500e-05\n",
      "Epoch 358/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 5.9850e-05 - accuracy: 1.0000\n",
      "Epoch 358: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 5.9810e-05 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 359/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 2.0129e-05 - accuracy: 1.0000\n",
      "Epoch 359: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 2.0129e-05 - accuracy: 1.0000 - val_loss: 0.0055 - val_accuracy: 0.9996 - lr: 6.2500e-05\n",
      "Epoch 360/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 8.8170e-06 - accuracy: 1.0000\n",
      "Epoch 360: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 8.8167e-06 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 0.9996 - lr: 6.2500e-05\n",
      "Epoch 361/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 1.6493e-05 - accuracy: 1.0000\n",
      "Epoch 361: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 1.6492e-05 - accuracy: 1.0000 - val_loss: 0.0044 - val_accuracy: 0.9998 - lr: 6.2500e-05\n",
      "Epoch 362/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 2.6666e-05 - accuracy: 1.0000\n",
      "Epoch 362: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 2.6648e-05 - accuracy: 1.0000 - val_loss: 0.0050 - val_accuracy: 0.9997 - lr: 6.2500e-05\n",
      "Epoch 363/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 3.2264e-05 - accuracy: 1.0000\n",
      "Epoch 363: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 3.2263e-05 - accuracy: 1.0000 - val_loss: 0.0050 - val_accuracy: 0.9996 - lr: 6.2500e-05\n",
      "Epoch 364/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 9.8986e-06 - accuracy: 1.0000\n",
      "Epoch 364: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 49s 30ms/step - loss: 9.8982e-06 - accuracy: 1.0000 - val_loss: 0.0049 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 365/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 6.5720e-05 - accuracy: 1.0000\n",
      "Epoch 365: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 6.5720e-05 - accuracy: 1.0000 - val_loss: 0.0066 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 366/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 1.0727e-04 - accuracy: 1.0000\n",
      "Epoch 366: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 1.0727e-04 - accuracy: 1.0000 - val_loss: 0.0048 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 367/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 9.3906e-06 - accuracy: 1.0000\n",
      "Epoch 367: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 9.3902e-06 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 0.9996 - lr: 6.2500e-05\n",
      "Epoch 368/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 4.2823e-06 - accuracy: 1.0000\n",
      "Epoch 368: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 4.2795e-06 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 369/500\n",
      "1594/1597 [============================>.] - ETA: 0s - loss: 1.1791e-05 - accuracy: 1.0000\n",
      "Epoch 369: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 1.1776e-05 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 0.9997 - lr: 6.2500e-05\n",
      "Epoch 370/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 3.6152e-05 - accuracy: 1.0000\n",
      "Epoch 370: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 3.6151e-05 - accuracy: 1.0000 - val_loss: 0.0068 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 371/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 4.5719e-05 - accuracy: 1.0000\n",
      "Epoch 371: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 4.5688e-05 - accuracy: 1.0000 - val_loss: 0.0062 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 372/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 1.2380e-04 - accuracy: 1.0000\n",
      "Epoch 372: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 1.2372e-04 - accuracy: 1.0000 - val_loss: 0.0064 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 373/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 7.1545e-06 - accuracy: 1.0000\n",
      "Epoch 373: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 7.1497e-06 - accuracy: 1.0000 - val_loss: 0.0061 - val_accuracy: 0.9996 - lr: 6.2500e-05\n",
      "Epoch 374/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 2.3861e-06 - accuracy: 1.0000\n",
      "Epoch 374: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 2.3860e-06 - accuracy: 1.0000 - val_loss: 0.0055 - val_accuracy: 0.9996 - lr: 6.2500e-05\n",
      "Epoch 375/500\n",
      "1594/1597 [============================>.] - ETA: 0s - loss: 1.1985e-05 - accuracy: 1.0000\n",
      "Epoch 375: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 1.1969e-05 - accuracy: 1.0000 - val_loss: 0.0056 - val_accuracy: 0.9996 - lr: 6.2500e-05\n",
      "Epoch 376/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 7.9215e-06 - accuracy: 1.0000\n",
      "Epoch 376: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 7.9162e-06 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 0.9996 - lr: 6.2500e-05\n",
      "Epoch 377/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 4.1997e-06 - accuracy: 1.0000\n",
      "Epoch 377: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 4.1995e-06 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 0.9996 - lr: 6.2500e-05\n",
      "Epoch 378/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 1.0169e-04 - accuracy: 1.0000\n",
      "Epoch 378: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 1.0162e-04 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 379/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 2.3130e-05 - accuracy: 1.0000\n",
      "Epoch 379: val_accuracy did not improve from 0.99992\n",
      "\n",
      "Epoch 379: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "1597/1597 [==============================] - 50s 31ms/step - loss: 2.3130e-05 - accuracy: 1.0000 - val_loss: 0.0050 - val_accuracy: 0.9995 - lr: 6.2500e-05\n",
      "Epoch 380/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 8.8785e-06 - accuracy: 1.0000\n",
      "Epoch 380: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 8.8781e-06 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 0.9995 - lr: 3.1250e-05\n",
      "Epoch 381/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 1.0079e-05 - accuracy: 1.0000\n",
      "Epoch 381: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 1.0079e-05 - accuracy: 1.0000 - val_loss: 0.0038 - val_accuracy: 0.9995 - lr: 3.1250e-05\n",
      "Epoch 382/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 3.6879e-05 - accuracy: 1.0000\n",
      "Epoch 382: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 3.6878e-05 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 0.9996 - lr: 3.1250e-05\n",
      "Epoch 383/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 7.2938e-05 - accuracy: 1.0000\n",
      "Epoch 383: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 7.2938e-05 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 0.9996 - lr: 3.1250e-05\n",
      "Epoch 384/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 4.4870e-06 - accuracy: 1.0000\n",
      "Epoch 384: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 4.4870e-06 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 0.9996 - lr: 3.1250e-05\n",
      "Epoch 385/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 2.0849e-06 - accuracy: 1.0000\n",
      "Epoch 385: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 2.0849e-06 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 0.9996 - lr: 3.1250e-05\n",
      "Epoch 386/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 2.4187e-06 - accuracy: 1.0000\n",
      "Epoch 386: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 2.4187e-06 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 0.9996 - lr: 3.1250e-05\n",
      "Epoch 387/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 7.2681e-06 - accuracy: 1.0000\n",
      "Epoch 387: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 7.2633e-06 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 0.9996 - lr: 3.1250e-05\n",
      "Epoch 388/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 6.0451e-05 - accuracy: 1.0000\n",
      "Epoch 388: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 6.0411e-05 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 0.9996 - lr: 3.1250e-05\n",
      "Epoch 389/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 4.4728e-06 - accuracy: 1.0000\n",
      "Epoch 389: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 4.4728e-06 - accuracy: 1.0000 - val_loss: 0.0048 - val_accuracy: 0.9996 - lr: 3.1250e-05\n",
      "Epoch 390/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 2.0327e-06 - accuracy: 1.0000\n",
      "Epoch 390: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 2.0323e-06 - accuracy: 1.0000 - val_loss: 0.0048 - val_accuracy: 0.9996 - lr: 3.1250e-05\n",
      "Epoch 391/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 2.4799e-05 - accuracy: 1.0000\n",
      "Epoch 391: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 2.4798e-05 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 0.9995 - lr: 3.1250e-05\n",
      "Epoch 392/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 1.4080e-04 - accuracy: 1.0000\n",
      "Epoch 392: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 50s 31ms/step - loss: 1.4071e-04 - accuracy: 1.0000 - val_loss: 0.0048 - val_accuracy: 0.9996 - lr: 3.1250e-05\n",
      "Epoch 393/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 2.4405e-06 - accuracy: 1.0000\n",
      "Epoch 393: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 2.4404e-06 - accuracy: 1.0000 - val_loss: 0.0049 - val_accuracy: 0.9996 - lr: 3.1250e-05\n",
      "Epoch 394/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 8.2917e-05 - accuracy: 1.0000\n",
      "Epoch 394: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 54s 34ms/step - loss: 8.2914e-05 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 0.9995 - lr: 3.1250e-05\n",
      "Epoch 395/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 4.2438e-05 - accuracy: 1.0000\n",
      "Epoch 395: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 4.2410e-05 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 0.9995 - lr: 3.1250e-05\n",
      "Epoch 396/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 2.6105e-06 - accuracy: 1.0000\n",
      "Epoch 396: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 2.6087e-06 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 0.9995 - lr: 3.1250e-05\n",
      "Epoch 397/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 2.4448e-06 - accuracy: 1.0000\n",
      "Epoch 397: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 2.4433e-06 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 0.9995 - lr: 3.1250e-05\n",
      "Epoch 398/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 8.2728e-06 - accuracy: 1.0000\n",
      "Epoch 398: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 8.2673e-06 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 0.9995 - lr: 3.1250e-05\n",
      "Epoch 399/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 3.3332e-05 - accuracy: 1.0000\n",
      "Epoch 399: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 3.3310e-05 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 0.9996 - lr: 3.1250e-05\n",
      "Epoch 400/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 1.6689e-05 - accuracy: 1.0000\n",
      "Epoch 400: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 1.6688e-05 - accuracy: 1.0000 - val_loss: 0.0048 - val_accuracy: 0.9995 - lr: 3.1250e-05\n",
      "Epoch 401/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 5.0470e-06 - accuracy: 1.0000\n",
      "Epoch 401: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 5.0468e-06 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 0.9995 - lr: 3.1250e-05\n",
      "Epoch 402/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 2.7411e-06 - accuracy: 1.0000\n",
      "Epoch 402: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 2.7393e-06 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 0.9995 - lr: 3.1250e-05\n",
      "Epoch 403/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 4.8643e-06 - accuracy: 1.0000\n",
      "Epoch 403: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 4.8611e-06 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 0.9995 - lr: 3.1250e-05\n",
      "Epoch 404/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 3.5452e-06 - accuracy: 1.0000\n",
      "Epoch 404: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 3.5452e-06 - accuracy: 1.0000 - val_loss: 0.0049 - val_accuracy: 0.9995 - lr: 3.1250e-05\n",
      "Epoch 405/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 1.1809e-04 - accuracy: 1.0000\n",
      "Epoch 405: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 49s 30ms/step - loss: 1.1802e-04 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 0.9994 - lr: 3.1250e-05\n",
      "Epoch 406/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 6.2341e-07 - accuracy: 1.0000\n",
      "Epoch 406: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 6.2339e-07 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 0.9994 - lr: 3.1250e-05\n",
      "Epoch 407/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 1.7902e-06 - accuracy: 1.0000\n",
      "Epoch 407: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 1.7890e-06 - accuracy: 1.0000 - val_loss: 0.0049 - val_accuracy: 0.9996 - lr: 3.1250e-05\n",
      "Epoch 408/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 7.1895e-06 - accuracy: 1.0000\n",
      "Epoch 408: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 49s 30ms/step - loss: 7.1847e-06 - accuracy: 1.0000 - val_loss: 0.0049 - val_accuracy: 0.9995 - lr: 3.1250e-05\n",
      "Epoch 409/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 8.5874e-06 - accuracy: 1.0000\n",
      "Epoch 409: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 8.5817e-06 - accuracy: 1.0000 - val_loss: 0.0038 - val_accuracy: 0.9997 - lr: 3.1250e-05\n",
      "Epoch 410/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 1.3626e-05 - accuracy: 1.0000\n",
      "Epoch 410: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 1.3626e-05 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 0.9996 - lr: 3.1250e-05\n",
      "Epoch 411/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 9.3582e-06 - accuracy: 1.0000\n",
      "Epoch 411: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 9.3519e-06 - accuracy: 1.0000 - val_loss: 0.0048 - val_accuracy: 0.9995 - lr: 3.1250e-05\n",
      "Epoch 412/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 2.3023e-05 - accuracy: 1.0000\n",
      "Epoch 412: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 2.3022e-05 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 0.9997 - lr: 3.1250e-05\n",
      "Epoch 413/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 5.8920e-05 - accuracy: 1.0000\n",
      "Epoch 413: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 5.8880e-05 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 0.9996 - lr: 3.1250e-05\n",
      "Epoch 414/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 1.8488e-06 - accuracy: 1.0000\n",
      "Epoch 414: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 1.8479e-06 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 0.9996 - lr: 3.1250e-05\n",
      "Epoch 415/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 7.1284e-06 - accuracy: 1.0000\n",
      "Epoch 415: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 7.1236e-06 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 0.9996 - lr: 3.1250e-05\n",
      "Epoch 416/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 5.4084e-06 - accuracy: 1.0000\n",
      "Epoch 416: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 5.4082e-06 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 0.9997 - lr: 3.1250e-05\n",
      "Epoch 417/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 7.4163e-06 - accuracy: 1.0000\n",
      "Epoch 417: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 7.4114e-06 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 0.9995 - lr: 3.1250e-05\n",
      "Epoch 418/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 1.0292e-05 - accuracy: 1.0000\n",
      "Epoch 418: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 1.0292e-05 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 0.9995 - lr: 3.1250e-05\n",
      "Epoch 419/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 2.7990e-06 - accuracy: 1.0000\n",
      "Epoch 419: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 2.7989e-06 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 0.9996 - lr: 3.1250e-05\n",
      "Epoch 420/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 8.0333e-06 - accuracy: 1.0000\n",
      "Epoch 420: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 8.0330e-06 - accuracy: 1.0000 - val_loss: 0.0049 - val_accuracy: 0.9996 - lr: 3.1250e-05\n",
      "Epoch 421/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 5.4756e-06 - accuracy: 1.0000\n",
      "Epoch 421: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 50s 31ms/step - loss: 5.4721e-06 - accuracy: 1.0000 - val_loss: 0.0050 - val_accuracy: 0.9996 - lr: 3.1250e-05\n",
      "Epoch 422/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 1.9972e-06 - accuracy: 1.0000\n",
      "Epoch 422: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 1.9971e-06 - accuracy: 1.0000 - val_loss: 0.0049 - val_accuracy: 0.9996 - lr: 3.1250e-05\n",
      "Epoch 423/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 6.7436e-05 - accuracy: 1.0000\n",
      "Epoch 423: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 6.7433e-05 - accuracy: 1.0000 - val_loss: 0.0048 - val_accuracy: 0.9996 - lr: 3.1250e-05\n",
      "Epoch 424/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 9.0860e-05 - accuracy: 1.0000\n",
      "Epoch 424: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 9.0800e-05 - accuracy: 1.0000 - val_loss: 0.0049 - val_accuracy: 0.9996 - lr: 3.1250e-05\n",
      "Epoch 425/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 1.0515e-05 - accuracy: 1.0000\n",
      "Epoch 425: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 1.0514e-05 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 0.9996 - lr: 3.1250e-05\n",
      "Epoch 426/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 6.5645e-07 - accuracy: 1.0000\n",
      "Epoch 426: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 6.5615e-07 - accuracy: 1.0000 - val_loss: 0.0044 - val_accuracy: 0.9996 - lr: 3.1250e-05\n",
      "Epoch 427/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 2.2222e-06 - accuracy: 1.0000\n",
      "Epoch 427: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 2.2222e-06 - accuracy: 1.0000 - val_loss: 0.0038 - val_accuracy: 0.9997 - lr: 3.1250e-05\n",
      "Epoch 428/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 3.2309e-05 - accuracy: 1.0000\n",
      "Epoch 428: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 3.2309e-05 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 0.9996 - lr: 3.1250e-05\n",
      "Epoch 429/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 1.5385e-06 - accuracy: 1.0000\n",
      "Epoch 429: val_accuracy did not improve from 0.99992\n",
      "\n",
      "Epoch 429: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 1.5375e-06 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 0.9996 - lr: 3.1250e-05\n",
      "Epoch 430/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 9.2885e-07 - accuracy: 1.0000\n",
      "Epoch 430: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 9.2823e-07 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 0.9996 - lr: 1.5625e-05\n",
      "Epoch 431/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 3.8073e-06 - accuracy: 1.0000\n",
      "Epoch 431: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 3.8073e-06 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 432/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 8.9521e-07 - accuracy: 1.0000\n",
      "Epoch 432: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 8.9521e-07 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 433/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 2.9437e-06 - accuracy: 1.0000\n",
      "Epoch 433: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 2.9418e-06 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 434/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 6.0437e-05 - accuracy: 1.0000\n",
      "Epoch 434: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 6.0434e-05 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 435/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 4.6977e-06 - accuracy: 1.0000\n",
      "Epoch 435: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 4.6975e-06 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 436/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 3.2816e-06 - accuracy: 1.0000\n",
      "Epoch 436: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 3.2815e-06 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 437/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 1.3378e-06 - accuracy: 1.0000\n",
      "Epoch 437: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 1.3369e-06 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 438/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 1.8069e-06 - accuracy: 1.0000\n",
      "Epoch 438: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 1.8057e-06 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 439/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 1.1578e-05 - accuracy: 1.0000\n",
      "Epoch 439: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 45s 28ms/step - loss: 1.1577e-05 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 440/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 2.2641e-06 - accuracy: 1.0000\n",
      "Epoch 440: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 2.2640e-06 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 441/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 1.9121e-06 - accuracy: 1.0000\n",
      "Epoch 441: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 1.9120e-06 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 442/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 2.1244e-06 - accuracy: 1.0000\n",
      "Epoch 442: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 2.1230e-06 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 443/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 1.7139e-05 - accuracy: 1.0000\n",
      "Epoch 443: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 1.7138e-05 - accuracy: 1.0000 - val_loss: 0.0044 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 444/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 2.5969e-06 - accuracy: 1.0000\n",
      "Epoch 444: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 2.5951e-06 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 445/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 2.2373e-06 - accuracy: 1.0000\n",
      "Epoch 445: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 2.2358e-06 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 446/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 3.2131e-06 - accuracy: 1.0000\n",
      "Epoch 446: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 3.2129e-06 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 447/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 1.9771e-06 - accuracy: 1.0000\n",
      "Epoch 447: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 1.9759e-06 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 448/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 2.2852e-06 - accuracy: 1.0000\n",
      "Epoch 448: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 2.2851e-06 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 449/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 5.2514e-06 - accuracy: 1.0000\n",
      "Epoch 449: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 5.2512e-06 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 450/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 4.2701e-06 - accuracy: 1.0000\n",
      "Epoch 450: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 4.2673e-06 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 451/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 4.6546e-05 - accuracy: 1.0000\n",
      "Epoch 451: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 4.6546e-05 - accuracy: 1.0000 - val_loss: 0.0049 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 452/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 8.9663e-06 - accuracy: 1.0000\n",
      "Epoch 452: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 8.9663e-06 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 453/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 2.3781e-05 - accuracy: 1.0000\n",
      "Epoch 453: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 45s 28ms/step - loss: 2.3780e-05 - accuracy: 1.0000 - val_loss: 0.0049 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 454/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 8.3283e-06 - accuracy: 1.0000\n",
      "Epoch 454: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 49s 30ms/step - loss: 8.3280e-06 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 455/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 1.1830e-06 - accuracy: 1.0000\n",
      "Epoch 455: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 52s 32ms/step - loss: 1.1830e-06 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 456/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 9.7486e-07 - accuracy: 1.0000\n",
      "Epoch 456: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 9.7421e-07 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 457/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 3.6124e-05 - accuracy: 1.0000\n",
      "Epoch 457: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 3.6123e-05 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 0.9996 - lr: 1.5625e-05\n",
      "Epoch 458/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 2.1199e-06 - accuracy: 1.0000\n",
      "Epoch 458: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 2.1199e-06 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 0.9996 - lr: 1.5625e-05\n",
      "Epoch 459/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 3.6974e-07 - accuracy: 1.0000\n",
      "Epoch 459: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 2.5523e-06 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 0.9996 - lr: 1.5625e-05\n",
      "Epoch 460/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 3.0186e-06 - accuracy: 1.0000\n",
      "Epoch 460: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 3.0165e-06 - accuracy: 1.0000 - val_loss: 0.0048 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 461/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 4.8517e-05 - accuracy: 1.0000\n",
      "Epoch 461: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 4.8517e-05 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 462/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 1.3520e-06 - accuracy: 1.0000\n",
      "Epoch 462: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 1.3511e-06 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 463/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 7.6557e-07 - accuracy: 1.0000\n",
      "Epoch 463: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 7.6554e-07 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 464/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 6.0133e-06 - accuracy: 1.0000\n",
      "Epoch 464: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 6.0093e-06 - accuracy: 1.0000 - val_loss: 0.0044 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 465/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 1.6501e-06 - accuracy: 1.0000\n",
      "Epoch 465: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 1.6491e-06 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 0.9996 - lr: 1.5625e-05\n",
      "Epoch 466/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 4.0662e-07 - accuracy: 1.0000\n",
      "Epoch 466: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 4.0661e-07 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 467/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 1.8651e-06 - accuracy: 1.0000\n",
      "Epoch 467: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 59s 37ms/step - loss: 1.8650e-06 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 468/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 6.0870e-06 - accuracy: 1.0000\n",
      "Epoch 468: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 50s 31ms/step - loss: 6.0829e-06 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 469/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 4.0232e-06 - accuracy: 1.0000\n",
      "Epoch 469: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 4.0230e-06 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 470/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 3.6633e-06 - accuracy: 1.0000\n",
      "Epoch 470: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 51s 32ms/step - loss: 3.6633e-06 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 0.9996 - lr: 1.5625e-05\n",
      "Epoch 471/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 1.0077e-06 - accuracy: 1.0000\n",
      "Epoch 471: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 49s 30ms/step - loss: 1.0077e-06 - accuracy: 1.0000 - val_loss: 0.0044 - val_accuracy: 0.9996 - lr: 1.5625e-05\n",
      "Epoch 472/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 3.3256e-06 - accuracy: 1.0000\n",
      "Epoch 472: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 3.3234e-06 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 0.9996 - lr: 1.5625e-05\n",
      "Epoch 473/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 1.8505e-05 - accuracy: 1.0000\n",
      "Epoch 473: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 1.8504e-05 - accuracy: 1.0000 - val_loss: 0.0038 - val_accuracy: 0.9997 - lr: 1.5625e-05\n",
      "Epoch 474/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 2.6330e-05 - accuracy: 1.0000\n",
      "Epoch 474: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 2.6313e-05 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 0.9996 - lr: 1.5625e-05\n",
      "Epoch 475/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 6.3851e-07 - accuracy: 1.0000\n",
      "Epoch 475: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 6.3848e-07 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 0.9996 - lr: 1.5625e-05\n",
      "Epoch 476/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 1.3615e-04 - accuracy: 1.0000\n",
      "Epoch 476: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 30ms/step - loss: 1.3614e-04 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 0.9996 - lr: 1.5625e-05\n",
      "Epoch 477/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 1.4517e-06 - accuracy: 1.0000\n",
      "Epoch 477: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 1.4517e-06 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 0.9996 - lr: 1.5625e-05\n",
      "Epoch 478/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 2.8631e-06 - accuracy: 1.0000\n",
      "Epoch 478: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 47s 29ms/step - loss: 2.8631e-06 - accuracy: 1.0000 - val_loss: 0.0038 - val_accuracy: 0.9995 - lr: 1.5625e-05\n",
      "Epoch 479/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 3.1139e-05 - accuracy: 1.0000\n",
      "Epoch 479: val_accuracy did not improve from 0.99992\n",
      "\n",
      "Epoch 479: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 3.1138e-05 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 0.9996 - lr: 1.5625e-05\n",
      "Epoch 480/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 1.8744e-05 - accuracy: 1.0000\n",
      "Epoch 480: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 52s 32ms/step - loss: 1.8731e-05 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 0.9996 - lr: 7.8125e-06\n",
      "Epoch 481/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 6.2730e-06 - accuracy: 1.0000\n",
      "Epoch 481: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 45s 28ms/step - loss: 6.2730e-06 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 0.9996 - lr: 7.8125e-06\n",
      "Epoch 482/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 7.0974e-07 - accuracy: 1.0000\n",
      "Epoch 482: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 7.0971e-07 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 0.9996 - lr: 7.8125e-06\n",
      "Epoch 483/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 7.2052e-06 - accuracy: 1.0000\n",
      "Epoch 483: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 7.2050e-06 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 0.9996 - lr: 7.8125e-06\n",
      "Epoch 484/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 2.4290e-06 - accuracy: 1.0000\n",
      "Epoch 484: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 2.4289e-06 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 0.9996 - lr: 7.8125e-06\n",
      "Epoch 485/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 1.3618e-06 - accuracy: 1.0000\n",
      "Epoch 485: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 1.3618e-06 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 0.9996 - lr: 7.8125e-06\n",
      "Epoch 486/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 3.4382e-07 - accuracy: 1.0000\n",
      "Epoch 486: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 3.4359e-07 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 0.9996 - lr: 7.8125e-06\n",
      "Epoch 487/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 8.7922e-06 - accuracy: 1.0000\n",
      "Epoch 487: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 8.7864e-06 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 0.9996 - lr: 7.8125e-06\n",
      "Epoch 488/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 2.8255e-05 - accuracy: 1.0000\n",
      "Epoch 488: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 2.8254e-05 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 0.9996 - lr: 7.8125e-06\n",
      "Epoch 489/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 4.5464e-05 - accuracy: 1.0000\n",
      "Epoch 489: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 4.5434e-05 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 0.9996 - lr: 7.8125e-06\n",
      "Epoch 490/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 6.5839e-06 - accuracy: 1.0000\n",
      "Epoch 490: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 6.5795e-06 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 0.9996 - lr: 7.8125e-06\n",
      "Epoch 491/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 7.5934e-05 - accuracy: 1.0000\n",
      "Epoch 491: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 7.5931e-05 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 0.9996 - lr: 7.8125e-06\n",
      "Epoch 492/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 2.2418e-05 - accuracy: 1.0000\n",
      "Epoch 492: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 2.2403e-05 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 0.9996 - lr: 7.8125e-06\n",
      "Epoch 493/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 2.8071e-06 - accuracy: 1.0000\n",
      "Epoch 493: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 2.8071e-06 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 0.9996 - lr: 7.8125e-06\n",
      "Epoch 494/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 2.7181e-05 - accuracy: 1.0000\n",
      "Epoch 494: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 49s 31ms/step - loss: 2.7181e-05 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 0.9996 - lr: 7.8125e-06\n",
      "Epoch 495/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 1.2225e-05 - accuracy: 1.0000\n",
      "Epoch 495: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 1.2216e-05 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 0.9996 - lr: 7.8125e-06\n",
      "Epoch 496/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 1.7318e-06 - accuracy: 1.0000\n",
      "Epoch 496: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 1.7317e-06 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 0.9996 - lr: 7.8125e-06\n",
      "Epoch 497/500\n",
      "1597/1597 [==============================] - ETA: 0s - loss: 1.0429e-06 - accuracy: 1.0000\n",
      "Epoch 497: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 49s 30ms/step - loss: 1.0429e-06 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 0.9996 - lr: 7.8125e-06\n",
      "Epoch 498/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 4.5126e-06 - accuracy: 1.0000\n",
      "Epoch 498: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 4.5124e-06 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 0.9996 - lr: 7.8125e-06\n",
      "Epoch 499/500\n",
      "1595/1597 [============================>.] - ETA: 0s - loss: 5.9509e-06 - accuracy: 1.0000\n",
      "Epoch 499: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 46s 29ms/step - loss: 5.9469e-06 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 0.9996 - lr: 7.8125e-06\n",
      "Epoch 500/500\n",
      "1596/1597 [============================>.] - ETA: 0s - loss: 3.5058e-07 - accuracy: 1.0000\n",
      "Epoch 500: val_accuracy did not improve from 0.99992\n",
      "1597/1597 [==============================] - 48s 30ms/step - loss: 3.5057e-07 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 0.9996 - lr: 7.8125e-06\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# 모델 훈련\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=500,\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "        # save_best_only -> 모델 정확도가 이전보다 향상된 경우에만 모델 저장\n",
    "        ModelCheckpoint('models/model.keras', monitor='val_accuracy', verbose=1, save_best_only=True, mode='auto'),\n",
    "        # 정확도 개선이 없을시 학습률(factor) 0.5배로 감소, n 에포크 동안 개선 없을 경우 학습률 감소\n",
    "        ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=50, verbose=1, mode='auto'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 학습 데이터/검증 데이터 정확도/손실 그래프\n",
    "fig, loss_ax = plt.subplots(figsize=(16, 10))\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(history.history['accuracy'], 'b', label='train accuracy')\n",
    "acc_ax.plot(history.history['val_accuracy'], 'g', label='val accuracy')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
