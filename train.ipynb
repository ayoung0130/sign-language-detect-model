{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os   # 운영체제와 상호작용하기 위한 모듈\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경 변수 로드\n",
    "load_dotenv()\n",
    "base_dir = os.getenv('BASE_DIR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10142, 20, 235)\n"
     ]
    }
   ],
   "source": [
    "data = np.concatenate([\n",
    "    np.load(os.path.join(base_dir, 'seq_data/seq_npy_1729150467_20_10.npy')),\n",
    "    np.load(os.path.join(base_dir, 'seq_data/seq_npy_flip_1729150467_20_10.npy')),\n",
    "\n",
    "    np.load(os.path.join(base_dir, 'seq_data/seq_npy_1729349658_20_10.npy')),\n",
    "    np.load(os.path.join(base_dir, 'seq_data/seq_npy_flip_1729349658_20_10.npy')),\n",
    "], axis=0)\n",
    "\n",
    "print(data.shape)\n",
    "# (프레임 수, 시퀀스 길이, 한 프레임당 데이터 개수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\mshof\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "(10142, 20, 234) (10142,) (10142, 40)\n",
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
      " 36. 37. 38. 39.]\n"
     ]
    }
   ],
   "source": [
    "from setting import actions\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# 데이터 분리 및 전처리\n",
    "x_data = data[:, :, :-1]    # 시퀀스의 마지막 요소 제외한 모든 값 가져와 할당\n",
    "labels = data[:, 0, -1]     # 마지막 요소는 레이블 값\n",
    "\n",
    "# 원-핫 인코딩으로 변환\n",
    "y_data = to_categorical(labels, num_classes=len(actions))\n",
    "\n",
    "print(x_data.shape, labels.shape, y_data.shape)     # y_data 형태 -> [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...]\n",
    "print(np.unique(labels))    # 레이블 값 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7099, 20, 234) (7099, 40)\n",
      "(3043, 20, 234) (3043, 40)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터셋 분할 (학습 데이터와 검증 데이터만 사용)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.3, random_state=2, stratify=labels)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\mshof\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\rnn\\lstm.py:148: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\mshof\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 256)               502784    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 40)                10280     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 513064 (1.96 MB)\n",
      "Trainable params: 513064 (1.96 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# 모델 정의\n",
    "model = Sequential([\n",
    "    LSTM(256, activation='tanh', input_shape=x_data.shape[1:3]),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(actions), activation='softmax')\n",
    "])\n",
    "\n",
    "# 모델 컴파일 (최적화 알고리즘, 레이블 클래스 2개 이상일 때 사용하는 손실 함수, 모델평가지표)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델 요약\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "WARNING:tensorflow:From c:\\Users\\mshof\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\mshof\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "220/222 [============================>.] - ETA: 0s - loss: 2.6076 - accuracy: 0.2614\n",
      "Epoch 1: val_accuracy improved from -inf to 0.41768, saving model to models\\model.keras\n",
      "222/222 [==============================] - 8s 28ms/step - loss: 2.5995 - accuracy: 0.2640 - val_loss: 1.9381 - val_accuracy: 0.4177 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 1.7781 - accuracy: 0.4561\n",
      "Epoch 2: val_accuracy improved from 0.41768 to 0.58265, saving model to models\\model.keras\n",
      "222/222 [==============================] - 6s 29ms/step - loss: 1.7781 - accuracy: 0.4561 - val_loss: 1.4211 - val_accuracy: 0.5826 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 1.3284 - accuracy: 0.5897\n",
      "Epoch 3: val_accuracy improved from 0.58265 to 0.65823, saving model to models\\model.keras\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 1.3284 - accuracy: 0.5897 - val_loss: 1.1163 - val_accuracy: 0.6582 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 1.1173 - accuracy: 0.6435\n",
      "Epoch 4: val_accuracy improved from 0.65823 to 0.70194, saving model to models\\model.keras\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 1.1171 - accuracy: 0.6435 - val_loss: 0.9600 - val_accuracy: 0.7019 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 0.9139 - accuracy: 0.7116\n",
      "Epoch 5: val_accuracy improved from 0.70194 to 0.72626, saving model to models\\model.keras\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 0.9143 - accuracy: 0.7114 - val_loss: 0.8701 - val_accuracy: 0.7263 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.7945 - accuracy: 0.7469\n",
      "Epoch 6: val_accuracy improved from 0.72626 to 0.73546, saving model to models\\model.keras\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.7945 - accuracy: 0.7469 - val_loss: 0.8316 - val_accuracy: 0.7355 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.6974 - accuracy: 0.7748\n",
      "Epoch 7: val_accuracy improved from 0.73546 to 0.79494, saving model to models\\model.keras\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 0.6974 - accuracy: 0.7748 - val_loss: 0.6840 - val_accuracy: 0.7949 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.5739 - accuracy: 0.8176\n",
      "Epoch 8: val_accuracy did not improve from 0.79494\n",
      "222/222 [==============================] - 7s 31ms/step - loss: 0.5741 - accuracy: 0.8174 - val_loss: 0.6707 - val_accuracy: 0.7871 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.5531 - accuracy: 0.8179\n",
      "Epoch 9: val_accuracy improved from 0.79494 to 0.81959, saving model to models\\model.keras\n",
      "222/222 [==============================] - 7s 30ms/step - loss: 0.5535 - accuracy: 0.8179 - val_loss: 0.5817 - val_accuracy: 0.8196 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.4777 - accuracy: 0.8472\n",
      "Epoch 10: val_accuracy improved from 0.81959 to 0.82517, saving model to models\\model.keras\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.4777 - accuracy: 0.8472 - val_loss: 0.5459 - val_accuracy: 0.8252 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.4553 - accuracy: 0.8536\n",
      "Epoch 11: val_accuracy did not improve from 0.82517\n",
      "222/222 [==============================] - 7s 31ms/step - loss: 0.4559 - accuracy: 0.8535 - val_loss: 0.5527 - val_accuracy: 0.8212 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 0.3986 - accuracy: 0.8697\n",
      "Epoch 12: val_accuracy did not improve from 0.82517\n",
      "222/222 [==============================] - 7s 31ms/step - loss: 0.3999 - accuracy: 0.8691 - val_loss: 0.6777 - val_accuracy: 0.7841 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.3953 - accuracy: 0.8754\n",
      "Epoch 13: val_accuracy improved from 0.82517 to 0.86066, saving model to models\\model.keras\n",
      "222/222 [==============================] - 7s 30ms/step - loss: 0.3955 - accuracy: 0.8752 - val_loss: 0.4564 - val_accuracy: 0.8607 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.3364 - accuracy: 0.8928\n",
      "Epoch 14: val_accuracy did not improve from 0.86066\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.3364 - accuracy: 0.8928 - val_loss: 0.4896 - val_accuracy: 0.8423 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.3504 - accuracy: 0.8853\n",
      "Epoch 15: val_accuracy improved from 0.86066 to 0.86789, saving model to models\\model.keras\n",
      "222/222 [==============================] - 7s 31ms/step - loss: 0.3504 - accuracy: 0.8853 - val_loss: 0.4139 - val_accuracy: 0.8679 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 0.2645 - accuracy: 0.9159\n",
      "Epoch 16: val_accuracy did not improve from 0.86789\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 0.2659 - accuracy: 0.9153 - val_loss: 0.4543 - val_accuracy: 0.8574 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.2434 - accuracy: 0.9219\n",
      "Epoch 17: val_accuracy did not improve from 0.86789\n",
      "222/222 [==============================] - 7s 31ms/step - loss: 0.2440 - accuracy: 0.9218 - val_loss: 0.4833 - val_accuracy: 0.8501 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 0.2437 - accuracy: 0.9256\n",
      "Epoch 18: val_accuracy did not improve from 0.86789\n",
      "222/222 [==============================] - 7s 30ms/step - loss: 0.2452 - accuracy: 0.9251 - val_loss: 0.4510 - val_accuracy: 0.8515 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.2042 - accuracy: 0.9341\n",
      "Epoch 19: val_accuracy improved from 0.86789 to 0.88334, saving model to models\\model.keras\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 0.2037 - accuracy: 0.9342 - val_loss: 0.3885 - val_accuracy: 0.8833 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.2310 - accuracy: 0.9289\n",
      "Epoch 20: val_accuracy did not improve from 0.88334\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 0.2310 - accuracy: 0.9289 - val_loss: 0.3902 - val_accuracy: 0.8748 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.2066 - accuracy: 0.9369\n",
      "Epoch 21: val_accuracy did not improve from 0.88334\n",
      "222/222 [==============================] - 7s 31ms/step - loss: 0.2065 - accuracy: 0.9369 - val_loss: 0.4714 - val_accuracy: 0.8580 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.1891 - accuracy: 0.9393\n",
      "Epoch 22: val_accuracy did not improve from 0.88334\n",
      "222/222 [==============================] - 7s 31ms/step - loss: 0.1887 - accuracy: 0.9394 - val_loss: 0.5153 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.2206 - accuracy: 0.9335\n",
      "Epoch 23: val_accuracy did not improve from 0.88334\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 0.2209 - accuracy: 0.9334 - val_loss: 0.4223 - val_accuracy: 0.8689 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.1866 - accuracy: 0.9399\n",
      "Epoch 24: val_accuracy did not improve from 0.88334\n",
      "222/222 [==============================] - 7s 31ms/step - loss: 0.1865 - accuracy: 0.9399 - val_loss: 0.5636 - val_accuracy: 0.8400 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.1713 - accuracy: 0.9457\n",
      "Epoch 25: val_accuracy improved from 0.88334 to 0.89714, saving model to models\\model.keras\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 0.1710 - accuracy: 0.9458 - val_loss: 0.3380 - val_accuracy: 0.8971 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.1522 - accuracy: 0.9534\n",
      "Epoch 26: val_accuracy did not improve from 0.89714\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 0.1522 - accuracy: 0.9534 - val_loss: 0.3920 - val_accuracy: 0.8860 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.1530 - accuracy: 0.9505\n",
      "Epoch 27: val_accuracy did not improve from 0.89714\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 0.1525 - accuracy: 0.9507 - val_loss: 0.4001 - val_accuracy: 0.8820 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.1139 - accuracy: 0.9645\n",
      "Epoch 28: val_accuracy did not improve from 0.89714\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.1138 - accuracy: 0.9646 - val_loss: 0.4120 - val_accuracy: 0.8804 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.1516 - accuracy: 0.9484\n",
      "Epoch 29: val_accuracy did not improve from 0.89714\n",
      "222/222 [==============================] - 7s 31ms/step - loss: 0.1516 - accuracy: 0.9484 - val_loss: 0.4238 - val_accuracy: 0.8705 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.1017 - accuracy: 0.9707\n",
      "Epoch 30: val_accuracy improved from 0.89714 to 0.90569, saving model to models\\model.keras\n",
      "222/222 [==============================] - 7s 31ms/step - loss: 0.1017 - accuracy: 0.9707 - val_loss: 0.3123 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.1074 - accuracy: 0.9689\n",
      "Epoch 31: val_accuracy did not improve from 0.90569\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.1070 - accuracy: 0.9690 - val_loss: 0.3880 - val_accuracy: 0.8899 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.1201 - accuracy: 0.9635\n",
      "Epoch 32: val_accuracy did not improve from 0.90569\n",
      "222/222 [==============================] - 7s 31ms/step - loss: 0.1202 - accuracy: 0.9635 - val_loss: 0.3304 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.1177 - accuracy: 0.9613\n",
      "Epoch 33: val_accuracy did not improve from 0.90569\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 0.1177 - accuracy: 0.9613 - val_loss: 0.3711 - val_accuracy: 0.8942 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.1443 - accuracy: 0.9558\n",
      "Epoch 34: val_accuracy did not improve from 0.90569\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 0.1443 - accuracy: 0.9558 - val_loss: 0.3488 - val_accuracy: 0.8893 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0948 - accuracy: 0.9709\n",
      "Epoch 35: val_accuracy did not improve from 0.90569\n",
      "222/222 [==============================] - 8s 38ms/step - loss: 0.0956 - accuracy: 0.9706 - val_loss: 0.4213 - val_accuracy: 0.8837 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.1217 - accuracy: 0.9615\n",
      "Epoch 36: val_accuracy did not improve from 0.90569\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 0.1217 - accuracy: 0.9615 - val_loss: 0.4290 - val_accuracy: 0.8794 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.1183 - accuracy: 0.9642\n",
      "Epoch 37: val_accuracy improved from 0.90569 to 0.90799, saving model to models\\model.keras\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.1183 - accuracy: 0.9642 - val_loss: 0.3210 - val_accuracy: 0.9080 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9783\n",
      "Epoch 38: val_accuracy did not improve from 0.90799\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 0.0720 - accuracy: 0.9784 - val_loss: 0.3517 - val_accuracy: 0.9017 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.1057 - accuracy: 0.9682\n",
      "Epoch 39: val_accuracy did not improve from 0.90799\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 0.1057 - accuracy: 0.9682 - val_loss: 0.3135 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.1109 - accuracy: 0.9637\n",
      "Epoch 40: val_accuracy did not improve from 0.90799\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 0.1111 - accuracy: 0.9637 - val_loss: 0.3254 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9789\n",
      "Epoch 41: val_accuracy improved from 0.90799 to 0.91226, saving model to models\\model.keras\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 0.0692 - accuracy: 0.9789 - val_loss: 0.2977 - val_accuracy: 0.9123 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0672 - accuracy: 0.9796\n",
      "Epoch 42: val_accuracy did not improve from 0.91226\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 0.0692 - accuracy: 0.9793 - val_loss: 0.4757 - val_accuracy: 0.8672 - lr: 0.0010\n",
      "Epoch 43/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.1476 - accuracy: 0.9516\n",
      "Epoch 43: val_accuracy did not improve from 0.91226\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 0.1480 - accuracy: 0.9515 - val_loss: 0.3353 - val_accuracy: 0.9001 - lr: 0.0010\n",
      "Epoch 44/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.1251 - accuracy: 0.9590\n",
      "Epoch 44: val_accuracy did not improve from 0.91226\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 0.1253 - accuracy: 0.9589 - val_loss: 0.3231 - val_accuracy: 0.9024 - lr: 0.0010\n",
      "Epoch 45/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0934 - accuracy: 0.9728\n",
      "Epoch 45: val_accuracy did not improve from 0.91226\n",
      "222/222 [==============================] - 8s 38ms/step - loss: 0.0934 - accuracy: 0.9728 - val_loss: 0.3802 - val_accuracy: 0.8939 - lr: 0.0010\n",
      "Epoch 46/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0817 - accuracy: 0.9754\n",
      "Epoch 46: val_accuracy did not improve from 0.91226\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 0.0815 - accuracy: 0.9755 - val_loss: 0.4087 - val_accuracy: 0.8787 - lr: 0.0010\n",
      "Epoch 47/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0984 - accuracy: 0.9700\n",
      "Epoch 47: val_accuracy did not improve from 0.91226\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.0986 - accuracy: 0.9699 - val_loss: 0.3261 - val_accuracy: 0.9080 - lr: 0.0010\n",
      "Epoch 48/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0502 - accuracy: 0.9880\n",
      "Epoch 48: val_accuracy improved from 0.91226 to 0.91982, saving model to models\\model.keras\n",
      "222/222 [==============================] - 7s 31ms/step - loss: 0.0501 - accuracy: 0.9880 - val_loss: 0.2814 - val_accuracy: 0.9198 - lr: 0.0010\n",
      "Epoch 49/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9769\n",
      "Epoch 49: val_accuracy did not improve from 0.91982\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 0.0764 - accuracy: 0.9769 - val_loss: 0.4814 - val_accuracy: 0.8666 - lr: 0.0010\n",
      "Epoch 50/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0944 - accuracy: 0.9693\n",
      "Epoch 50: val_accuracy did not improve from 0.91982\n",
      "222/222 [==============================] - 7s 31ms/step - loss: 0.0951 - accuracy: 0.9692 - val_loss: 0.4463 - val_accuracy: 0.8787 - lr: 0.0010\n",
      "Epoch 51/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.1307 - accuracy: 0.9577\n",
      "Epoch 51: val_accuracy did not improve from 0.91982\n",
      "222/222 [==============================] - 7s 34ms/step - loss: 0.1303 - accuracy: 0.9579 - val_loss: 0.3594 - val_accuracy: 0.8952 - lr: 0.0010\n",
      "Epoch 52/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0857 - accuracy: 0.9743\n",
      "Epoch 52: val_accuracy did not improve from 0.91982\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.0862 - accuracy: 0.9741 - val_loss: 0.3289 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Epoch 53/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9758\n",
      "Epoch 53: val_accuracy did not improve from 0.91982\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 0.0750 - accuracy: 0.9758 - val_loss: 0.3194 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Epoch 54/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0758 - accuracy: 0.9775\n",
      "Epoch 54: val_accuracy did not improve from 0.91982\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 0.0758 - accuracy: 0.9775 - val_loss: 0.3348 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Epoch 55/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0469 - accuracy: 0.9866\n",
      "Epoch 55: val_accuracy did not improve from 0.91982\n",
      "222/222 [==============================] - 9s 38ms/step - loss: 0.0469 - accuracy: 0.9866 - val_loss: 0.3161 - val_accuracy: 0.9123 - lr: 0.0010\n",
      "Epoch 56/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0533 - accuracy: 0.9831\n",
      "Epoch 56: val_accuracy did not improve from 0.91982\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.0533 - accuracy: 0.9831 - val_loss: 0.3560 - val_accuracy: 0.9050 - lr: 0.0010\n",
      "Epoch 57/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0741 - accuracy: 0.9791\n",
      "Epoch 57: val_accuracy did not improve from 0.91982\n",
      "222/222 [==============================] - 7s 31ms/step - loss: 0.0745 - accuracy: 0.9790 - val_loss: 0.3197 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Epoch 58/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0387 - accuracy: 0.9891\n",
      "Epoch 58: val_accuracy did not improve from 0.91982\n",
      "222/222 [==============================] - 7s 31ms/step - loss: 0.0385 - accuracy: 0.9892 - val_loss: 0.3986 - val_accuracy: 0.8958 - lr: 0.0010\n",
      "Epoch 59/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0523 - accuracy: 0.9842\n",
      "Epoch 59: val_accuracy did not improve from 0.91982\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.0523 - accuracy: 0.9842 - val_loss: 0.3718 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Epoch 60/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 0.0708 - accuracy: 0.9807\n",
      "Epoch 60: val_accuracy did not improve from 0.91982\n",
      "222/222 [==============================] - 7s 31ms/step - loss: 0.0722 - accuracy: 0.9804 - val_loss: 0.4978 - val_accuracy: 0.8659 - lr: 0.0010\n",
      "Epoch 61/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.1143 - accuracy: 0.9645\n",
      "Epoch 61: val_accuracy did not improve from 0.91982\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.1142 - accuracy: 0.9645 - val_loss: 0.3605 - val_accuracy: 0.8958 - lr: 0.0010\n",
      "Epoch 62/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0481 - accuracy: 0.9874\n",
      "Epoch 62: val_accuracy did not improve from 0.91982\n",
      "222/222 [==============================] - 7s 34ms/step - loss: 0.0485 - accuracy: 0.9873 - val_loss: 0.3359 - val_accuracy: 0.9031 - lr: 0.0010\n",
      "Epoch 63/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 0.9915\n",
      "Epoch 63: val_accuracy did not improve from 0.91982\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 0.0287 - accuracy: 0.9915 - val_loss: 0.3415 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Epoch 64/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9776\n",
      "Epoch 64: val_accuracy did not improve from 0.91982\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 0.0788 - accuracy: 0.9776 - val_loss: 0.3015 - val_accuracy: 0.9126 - lr: 0.0010\n",
      "Epoch 65/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0632 - accuracy: 0.9814\n",
      "Epoch 65: val_accuracy did not improve from 0.91982\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 0.0632 - accuracy: 0.9814 - val_loss: 0.2941 - val_accuracy: 0.9169 - lr: 0.0010\n",
      "Epoch 66/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0306 - accuracy: 0.9918\n",
      "Epoch 66: val_accuracy did not improve from 0.91982\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.0307 - accuracy: 0.9917 - val_loss: 0.3026 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Epoch 67/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0895 - accuracy: 0.9734\n",
      "Epoch 67: val_accuracy did not improve from 0.91982\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.0899 - accuracy: 0.9732 - val_loss: 0.3746 - val_accuracy: 0.8909 - lr: 0.0010\n",
      "Epoch 68/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0447 - accuracy: 0.9861\n",
      "Epoch 68: val_accuracy did not improve from 0.91982\n",
      "222/222 [==============================] - 7s 31ms/step - loss: 0.0447 - accuracy: 0.9861 - val_loss: 0.3448 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Epoch 69/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.1220 - accuracy: 0.9615\n",
      "Epoch 69: val_accuracy did not improve from 0.91982\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 0.1220 - accuracy: 0.9615 - val_loss: 0.3187 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Epoch 70/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0594 - accuracy: 0.9821\n",
      "Epoch 70: val_accuracy did not improve from 0.91982\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.0594 - accuracy: 0.9821 - val_loss: 0.3261 - val_accuracy: 0.9129 - lr: 0.0010\n",
      "Epoch 71/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9928\n",
      "Epoch 71: val_accuracy did not improve from 0.91982\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 0.0240 - accuracy: 0.9928 - val_loss: 0.3203 - val_accuracy: 0.9175 - lr: 0.0010\n",
      "Epoch 72/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0815 - accuracy: 0.9752\n",
      "Epoch 72: val_accuracy did not improve from 0.91982\n",
      "222/222 [==============================] - 11s 48ms/step - loss: 0.0815 - accuracy: 0.9752 - val_loss: 0.3937 - val_accuracy: 0.8981 - lr: 0.0010\n",
      "Epoch 73/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0933 - accuracy: 0.9713\n",
      "Epoch 73: val_accuracy did not improve from 0.91982\n",
      "222/222 [==============================] - 11s 51ms/step - loss: 0.0941 - accuracy: 0.9711 - val_loss: 0.5256 - val_accuracy: 0.8561 - lr: 0.0010\n",
      "Epoch 74/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.1040 - accuracy: 0.9656\n",
      "Epoch 74: val_accuracy did not improve from 0.91982\n",
      "222/222 [==============================] - 9s 41ms/step - loss: 0.1040 - accuracy: 0.9656 - val_loss: 0.3670 - val_accuracy: 0.9050 - lr: 0.0010\n",
      "Epoch 75/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9911\n",
      "Epoch 75: val_accuracy improved from 0.91982 to 0.92475, saving model to models\\model.keras\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 0.0336 - accuracy: 0.9911 - val_loss: 0.2960 - val_accuracy: 0.9247 - lr: 0.0010\n",
      "Epoch 76/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 0.0319 - accuracy: 0.9906\n",
      "Epoch 76: val_accuracy did not improve from 0.92475\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.0321 - accuracy: 0.9906 - val_loss: 0.4056 - val_accuracy: 0.8952 - lr: 0.0010\n",
      "Epoch 77/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0161 - accuracy: 0.9962\n",
      "Epoch 77: val_accuracy improved from 0.92475 to 0.92507, saving model to models\\model.keras\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.0161 - accuracy: 0.9962 - val_loss: 0.2911 - val_accuracy: 0.9251 - lr: 0.0010\n",
      "Epoch 78/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0214 - accuracy: 0.9948\n",
      "Epoch 78: val_accuracy did not improve from 0.92507\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 0.0214 - accuracy: 0.9948 - val_loss: 0.3705 - val_accuracy: 0.9031 - lr: 0.0010\n",
      "Epoch 79/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0992 - accuracy: 0.9708\n",
      "Epoch 79: val_accuracy did not improve from 0.92507\n",
      "222/222 [==============================] - 7s 34ms/step - loss: 0.0992 - accuracy: 0.9708 - val_loss: 0.4369 - val_accuracy: 0.8807 - lr: 0.0010\n",
      "Epoch 80/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0770 - accuracy: 0.9748\n",
      "Epoch 80: val_accuracy did not improve from 0.92507\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 0.0769 - accuracy: 0.9748 - val_loss: 0.3721 - val_accuracy: 0.8942 - lr: 0.0010\n",
      "Epoch 81/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0218 - accuracy: 0.9935\n",
      "Epoch 81: val_accuracy did not improve from 0.92507\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 0.0223 - accuracy: 0.9934 - val_loss: 0.3047 - val_accuracy: 0.9208 - lr: 0.0010\n",
      "Epoch 82/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9815\n",
      "Epoch 82: val_accuracy did not improve from 0.92507\n",
      "222/222 [==============================] - 8s 38ms/step - loss: 0.0675 - accuracy: 0.9815 - val_loss: 0.4298 - val_accuracy: 0.8794 - lr: 0.0010\n",
      "Epoch 83/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0454 - accuracy: 0.9852\n",
      "Epoch 83: val_accuracy did not improve from 0.92507\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 0.0458 - accuracy: 0.9851 - val_loss: 0.3023 - val_accuracy: 0.9218 - lr: 0.0010\n",
      "Epoch 84/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0223 - accuracy: 0.9942\n",
      "Epoch 84: val_accuracy improved from 0.92507 to 0.92705, saving model to models\\model.keras\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 0.0223 - accuracy: 0.9942 - val_loss: 0.2891 - val_accuracy: 0.9270 - lr: 0.0010\n",
      "Epoch 85/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.9966\n",
      "Epoch 85: val_accuracy did not improve from 0.92705\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.0114 - accuracy: 0.9966 - val_loss: 0.3220 - val_accuracy: 0.9165 - lr: 0.0010\n",
      "Epoch 86/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0846 - accuracy: 0.9724\n",
      "Epoch 86: val_accuracy did not improve from 0.92705\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.0846 - accuracy: 0.9724 - val_loss: 0.5104 - val_accuracy: 0.8633 - lr: 0.0010\n",
      "Epoch 87/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.1315 - accuracy: 0.9603\n",
      "Epoch 87: val_accuracy did not improve from 0.92705\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 0.1313 - accuracy: 0.9604 - val_loss: 0.4079 - val_accuracy: 0.8909 - lr: 0.0010\n",
      "Epoch 88/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0480 - accuracy: 0.9868\n",
      "Epoch 88: val_accuracy did not improve from 0.92705\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.0480 - accuracy: 0.9868 - val_loss: 0.3725 - val_accuracy: 0.9070 - lr: 0.0010\n",
      "Epoch 89/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0332 - accuracy: 0.9887\n",
      "Epoch 89: val_accuracy did not improve from 0.92705\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 0.0331 - accuracy: 0.9887 - val_loss: 0.3028 - val_accuracy: 0.9182 - lr: 0.0010\n",
      "Epoch 90/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.1024 - accuracy: 0.9676\n",
      "Epoch 90: val_accuracy did not improve from 0.92705\n",
      "222/222 [==============================] - 8s 38ms/step - loss: 0.1024 - accuracy: 0.9676 - val_loss: 0.4158 - val_accuracy: 0.8860 - lr: 0.0010\n",
      "Epoch 91/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9854\n",
      "Epoch 91: val_accuracy did not improve from 0.92705\n",
      "222/222 [==============================] - 8s 38ms/step - loss: 0.0490 - accuracy: 0.9854 - val_loss: 0.3447 - val_accuracy: 0.9090 - lr: 0.0010\n",
      "Epoch 92/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0293 - accuracy: 0.9919\n",
      "Epoch 92: val_accuracy did not improve from 0.92705\n",
      "222/222 [==============================] - 8s 38ms/step - loss: 0.0292 - accuracy: 0.9920 - val_loss: 0.3370 - val_accuracy: 0.9126 - lr: 0.0010\n",
      "Epoch 93/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0476 - accuracy: 0.9861\n",
      "Epoch 93: val_accuracy did not improve from 0.92705\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 0.0476 - accuracy: 0.9861 - val_loss: 0.4116 - val_accuracy: 0.8925 - lr: 0.0010\n",
      "Epoch 94/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0958 - accuracy: 0.9713\n",
      "Epoch 94: val_accuracy did not improve from 0.92705\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 0.0958 - accuracy: 0.9713 - val_loss: 0.3536 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Epoch 95/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 0.0243 - accuracy: 0.9935\n",
      "Epoch 95: val_accuracy did not improve from 0.92705\n",
      "222/222 [==============================] - 7s 31ms/step - loss: 0.0244 - accuracy: 0.9934 - val_loss: 0.3291 - val_accuracy: 0.9149 - lr: 0.0010\n",
      "Epoch 96/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0460 - accuracy: 0.9871\n",
      "Epoch 96: val_accuracy did not improve from 0.92705\n",
      "222/222 [==============================] - 7s 31ms/step - loss: 0.0459 - accuracy: 0.9872 - val_loss: 0.3355 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Epoch 97/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0398 - accuracy: 0.9889\n",
      "Epoch 97: val_accuracy did not improve from 0.92705\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 0.0398 - accuracy: 0.9889 - val_loss: 0.4347 - val_accuracy: 0.8909 - lr: 0.0010\n",
      "Epoch 98/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0280 - accuracy: 0.9910\n",
      "Epoch 98: val_accuracy did not improve from 0.92705\n",
      "222/222 [==============================] - 7s 34ms/step - loss: 0.0284 - accuracy: 0.9908 - val_loss: 0.3559 - val_accuracy: 0.9119 - lr: 0.0010\n",
      "Epoch 99/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9792\n",
      "Epoch 99: val_accuracy did not improve from 0.92705\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 0.0666 - accuracy: 0.9792 - val_loss: 0.3617 - val_accuracy: 0.9054 - lr: 0.0010\n",
      "Epoch 100/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9927\n",
      "Epoch 100: val_accuracy improved from 0.92705 to 0.92869, saving model to models\\model.keras\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 0.0236 - accuracy: 0.9927 - val_loss: 0.2934 - val_accuracy: 0.9287 - lr: 0.0010\n",
      "Epoch 101/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9959\n",
      "Epoch 101: val_accuracy did not improve from 0.92869\n",
      "222/222 [==============================] - 9s 40ms/step - loss: 0.0125 - accuracy: 0.9959 - val_loss: 0.3416 - val_accuracy: 0.9201 - lr: 0.0010\n",
      "Epoch 102/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9897\n",
      "Epoch 102: val_accuracy did not improve from 0.92869\n",
      "222/222 [==============================] - 8s 38ms/step - loss: 0.0431 - accuracy: 0.9897 - val_loss: 0.3891 - val_accuracy: 0.9119 - lr: 0.0010\n",
      "Epoch 103/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0646 - accuracy: 0.9796\n",
      "Epoch 103: val_accuracy did not improve from 0.92869\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 0.0652 - accuracy: 0.9793 - val_loss: 0.5789 - val_accuracy: 0.8574 - lr: 0.0010\n",
      "Epoch 104/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.1123 - accuracy: 0.9644\n",
      "Epoch 104: val_accuracy did not improve from 0.92869\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 0.1123 - accuracy: 0.9644 - val_loss: 0.3848 - val_accuracy: 0.9014 - lr: 0.0010\n",
      "Epoch 105/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0716 - accuracy: 0.9778\n",
      "Epoch 105: val_accuracy did not improve from 0.92869\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.0722 - accuracy: 0.9777 - val_loss: 0.3879 - val_accuracy: 0.9008 - lr: 0.0010\n",
      "Epoch 106/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 0.0238 - accuracy: 0.9929\n",
      "Epoch 106: val_accuracy did not improve from 0.92869\n",
      "222/222 [==============================] - 7s 31ms/step - loss: 0.0237 - accuracy: 0.9930 - val_loss: 0.3009 - val_accuracy: 0.9257 - lr: 0.0010\n",
      "Epoch 107/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9968\n",
      "Epoch 107: val_accuracy improved from 0.92869 to 0.93329, saving model to models\\model.keras\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.0143 - accuracy: 0.9968 - val_loss: 0.2861 - val_accuracy: 0.9333 - lr: 0.0010\n",
      "Epoch 108/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9948\n",
      "Epoch 108: val_accuracy did not improve from 0.93329\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 0.0166 - accuracy: 0.9948 - val_loss: 0.3557 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Epoch 109/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0728 - accuracy: 0.9785\n",
      "Epoch 109: val_accuracy did not improve from 0.93329\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 0.0729 - accuracy: 0.9784 - val_loss: 0.3353 - val_accuracy: 0.9165 - lr: 0.0010\n",
      "Epoch 110/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0355 - accuracy: 0.9905\n",
      "Epoch 110: val_accuracy did not improve from 0.93329\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 0.0354 - accuracy: 0.9906 - val_loss: 0.3283 - val_accuracy: 0.9215 - lr: 0.0010\n",
      "Epoch 111/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0531 - accuracy: 0.9842\n",
      "Epoch 111: val_accuracy did not improve from 0.93329\n",
      "222/222 [==============================] - 9s 38ms/step - loss: 0.0530 - accuracy: 0.9842 - val_loss: 0.3996 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Epoch 112/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0883 - accuracy: 0.9750\n",
      "Epoch 112: val_accuracy did not improve from 0.93329\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 0.0885 - accuracy: 0.9749 - val_loss: 0.3279 - val_accuracy: 0.9185 - lr: 0.0010\n",
      "Epoch 113/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0384 - accuracy: 0.9890\n",
      "Epoch 113: val_accuracy did not improve from 0.93329\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 0.0384 - accuracy: 0.9889 - val_loss: 0.3283 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Epoch 114/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0609 - accuracy: 0.9818\n",
      "Epoch 114: val_accuracy did not improve from 0.93329\n",
      "222/222 [==============================] - 7s 31ms/step - loss: 0.0609 - accuracy: 0.9818 - val_loss: 0.3690 - val_accuracy: 0.9070 - lr: 0.0010\n",
      "Epoch 115/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0278 - accuracy: 0.9919\n",
      "Epoch 115: val_accuracy did not improve from 0.93329\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.0277 - accuracy: 0.9920 - val_loss: 0.3089 - val_accuracy: 0.9231 - lr: 0.0010\n",
      "Epoch 116/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0395 - accuracy: 0.9888\n",
      "Epoch 116: val_accuracy did not improve from 0.93329\n",
      "222/222 [==============================] - 7s 31ms/step - loss: 0.0393 - accuracy: 0.9889 - val_loss: 0.3433 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Epoch 117/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0104 - accuracy: 0.9973\n",
      "Epoch 117: val_accuracy did not improve from 0.93329\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 0.0104 - accuracy: 0.9973 - val_loss: 0.3230 - val_accuracy: 0.9238 - lr: 0.0010\n",
      "Epoch 118/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0344 - accuracy: 0.9891\n",
      "Epoch 118: val_accuracy did not improve from 0.93329\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 0.0344 - accuracy: 0.9892 - val_loss: 0.4440 - val_accuracy: 0.8870 - lr: 0.0010\n",
      "Epoch 119/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0972 - accuracy: 0.9720\n",
      "Epoch 119: val_accuracy did not improve from 0.93329\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 0.0972 - accuracy: 0.9720 - val_loss: 0.3757 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Epoch 120/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0277 - accuracy: 0.9914\n",
      "Epoch 120: val_accuracy did not improve from 0.93329\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 0.0276 - accuracy: 0.9914 - val_loss: 0.3428 - val_accuracy: 0.9149 - lr: 0.0010\n",
      "Epoch 121/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9907\n",
      "Epoch 121: val_accuracy did not improve from 0.93329\n",
      "222/222 [==============================] - 9s 43ms/step - loss: 0.0336 - accuracy: 0.9907 - val_loss: 0.3952 - val_accuracy: 0.9050 - lr: 0.0010\n",
      "Epoch 122/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0368 - accuracy: 0.9891\n",
      "Epoch 122: val_accuracy did not improve from 0.93329\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 0.0368 - accuracy: 0.9890 - val_loss: 0.3757 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Epoch 123/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 0.9911\n",
      "Epoch 123: val_accuracy did not improve from 0.93329\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.0286 - accuracy: 0.9911 - val_loss: 0.3435 - val_accuracy: 0.9201 - lr: 0.0010\n",
      "Epoch 124/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 0.9904\n",
      "Epoch 124: val_accuracy did not improve from 0.93329\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 0.0292 - accuracy: 0.9904 - val_loss: 0.3687 - val_accuracy: 0.9123 - lr: 0.0010\n",
      "Epoch 125/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0248 - accuracy: 0.9935\n",
      "Epoch 125: val_accuracy did not improve from 0.93329\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.0250 - accuracy: 0.9934 - val_loss: 0.6280 - val_accuracy: 0.8597 - lr: 0.0010\n",
      "Epoch 126/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0871 - accuracy: 0.9751\n",
      "Epoch 126: val_accuracy did not improve from 0.93329\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 0.0871 - accuracy: 0.9751 - val_loss: 0.4631 - val_accuracy: 0.8902 - lr: 0.0010\n",
      "Epoch 127/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 0.9882\n",
      "Epoch 127: val_accuracy did not improve from 0.93329\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.0379 - accuracy: 0.9882 - val_loss: 0.3340 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Epoch 128/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0327 - accuracy: 0.9915\n",
      "Epoch 128: val_accuracy did not improve from 0.93329\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 0.0328 - accuracy: 0.9915 - val_loss: 0.3919 - val_accuracy: 0.9024 - lr: 0.0010\n",
      "Epoch 129/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0357 - accuracy: 0.9887\n",
      "Epoch 129: val_accuracy did not improve from 0.93329\n",
      "222/222 [==============================] - 8s 38ms/step - loss: 0.0357 - accuracy: 0.9887 - val_loss: 0.3228 - val_accuracy: 0.9261 - lr: 0.0010\n",
      "Epoch 130/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.9979\n",
      "Epoch 130: val_accuracy did not improve from 0.93329\n",
      "222/222 [==============================] - 8s 38ms/step - loss: 0.0081 - accuracy: 0.9979 - val_loss: 0.2933 - val_accuracy: 0.9277 - lr: 0.0010\n",
      "Epoch 131/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9993\n",
      "Epoch 131: val_accuracy did not improve from 0.93329\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 0.0040 - accuracy: 0.9993 - val_loss: 0.2953 - val_accuracy: 0.9313 - lr: 0.0010\n",
      "Epoch 132/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 0.9997\n",
      "Epoch 132: val_accuracy did not improve from 0.93329\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 0.0027 - accuracy: 0.9997 - val_loss: 0.3029 - val_accuracy: 0.9323 - lr: 0.0010\n",
      "Epoch 133/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9991\n",
      "Epoch 133: val_accuracy did not improve from 0.93329\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 0.0045 - accuracy: 0.9992 - val_loss: 0.2963 - val_accuracy: 0.9326 - lr: 0.0010\n",
      "Epoch 134/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 0.0752 - accuracy: 0.9770\n",
      "Epoch 134: val_accuracy did not improve from 0.93329\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 0.0748 - accuracy: 0.9769 - val_loss: 0.4578 - val_accuracy: 0.8794 - lr: 0.0010\n",
      "Epoch 135/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.1557 - accuracy: 0.9534\n",
      "Epoch 135: val_accuracy did not improve from 0.93329\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 0.1557 - accuracy: 0.9534 - val_loss: 0.4349 - val_accuracy: 0.8906 - lr: 0.0010\n",
      "Epoch 136/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0582 - accuracy: 0.9804\n",
      "Epoch 136: val_accuracy did not improve from 0.93329\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.0582 - accuracy: 0.9804 - val_loss: 0.3777 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Epoch 137/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 0.9907\n",
      "Epoch 137: val_accuracy did not improve from 0.93329\n",
      "\n",
      "Epoch 137: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 0.0301 - accuracy: 0.9907 - val_loss: 0.2960 - val_accuracy: 0.9280 - lr: 0.0010\n",
      "Epoch 138/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 0.9977\n",
      "Epoch 138: val_accuracy did not improve from 0.93329\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 0.0092 - accuracy: 0.9977 - val_loss: 0.2838 - val_accuracy: 0.9333 - lr: 5.0000e-04\n",
      "Epoch 139/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0039 - accuracy: 0.9993\n",
      "Epoch 139: val_accuracy improved from 0.93329 to 0.93592, saving model to models\\model.keras\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 0.0039 - accuracy: 0.9993 - val_loss: 0.2805 - val_accuracy: 0.9359 - lr: 5.0000e-04\n",
      "Epoch 140/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 0.9999\n",
      "Epoch 140: val_accuracy improved from 0.93592 to 0.93920, saving model to models\\model.keras\n",
      "222/222 [==============================] - 9s 41ms/step - loss: 0.0021 - accuracy: 0.9999 - val_loss: 0.2714 - val_accuracy: 0.9392 - lr: 5.0000e-04\n",
      "Epoch 141/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 141: val_accuracy did not improve from 0.93920\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.2704 - val_accuracy: 0.9389 - lr: 5.0000e-04\n",
      "Epoch 142/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 142: val_accuracy improved from 0.93920 to 0.93953, saving model to models\\model.keras\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2678 - val_accuracy: 0.9395 - lr: 5.0000e-04\n",
      "Epoch 143/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 143: val_accuracy improved from 0.93953 to 0.94052, saving model to models\\model.keras\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.2715 - val_accuracy: 0.9405 - lr: 5.0000e-04\n",
      "Epoch 144/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 144: val_accuracy did not improve from 0.94052\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2634 - val_accuracy: 0.9405 - lr: 5.0000e-04\n",
      "Epoch 145/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 8.9519e-04 - accuracy: 1.0000\n",
      "Epoch 145: val_accuracy did not improve from 0.94052\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 8.9243e-04 - accuracy: 1.0000 - val_loss: 0.2699 - val_accuracy: 0.9399 - lr: 5.0000e-04\n",
      "Epoch 146/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 8.0154e-04 - accuracy: 1.0000\n",
      "Epoch 146: val_accuracy improved from 0.94052 to 0.94085, saving model to models\\model.keras\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 8.0154e-04 - accuracy: 1.0000 - val_loss: 0.2722 - val_accuracy: 0.9408 - lr: 5.0000e-04\n",
      "Epoch 147/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 8.5345e-04 - accuracy: 1.0000\n",
      "Epoch 147: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 8.5140e-04 - accuracy: 1.0000 - val_loss: 0.2772 - val_accuracy: 0.9389 - lr: 5.0000e-04\n",
      "Epoch 148/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 6.4037e-04 - accuracy: 1.0000\n",
      "Epoch 148: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 9s 39ms/step - loss: 6.4037e-04 - accuracy: 1.0000 - val_loss: 0.2860 - val_accuracy: 0.9399 - lr: 5.0000e-04\n",
      "Epoch 149/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 7.8497e-04 - accuracy: 1.0000\n",
      "Epoch 149: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 8s 38ms/step - loss: 7.8497e-04 - accuracy: 1.0000 - val_loss: 0.2890 - val_accuracy: 0.9382 - lr: 5.0000e-04\n",
      "Epoch 150/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 6.9104e-04 - accuracy: 1.0000\n",
      "Epoch 150: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 6.8921e-04 - accuracy: 1.0000 - val_loss: 0.2964 - val_accuracy: 0.9382 - lr: 5.0000e-04\n",
      "Epoch 151/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 6.4081e-04 - accuracy: 1.0000\n",
      "Epoch 151: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 6.4081e-04 - accuracy: 1.0000 - val_loss: 0.3020 - val_accuracy: 0.9362 - lr: 5.0000e-04\n",
      "Epoch 152/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 7.2697e-04 - accuracy: 1.0000\n",
      "Epoch 152: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 7.2479e-04 - accuracy: 1.0000 - val_loss: 0.3056 - val_accuracy: 0.9376 - lr: 5.0000e-04\n",
      "Epoch 153/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 8.0700e-04 - accuracy: 1.0000\n",
      "Epoch 153: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 8.0443e-04 - accuracy: 1.0000 - val_loss: 0.3208 - val_accuracy: 0.9362 - lr: 5.0000e-04\n",
      "Epoch 154/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 9.2410e-04 - accuracy: 1.0000\n",
      "Epoch 154: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 9.2099e-04 - accuracy: 1.0000 - val_loss: 0.3248 - val_accuracy: 0.9359 - lr: 5.0000e-04\n",
      "Epoch 155/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0455 - accuracy: 0.9880\n",
      "Epoch 155: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.0462 - accuracy: 0.9879 - val_loss: 0.3924 - val_accuracy: 0.9077 - lr: 5.0000e-04\n",
      "Epoch 156/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.9932\n",
      "Epoch 156: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 0.0282 - accuracy: 0.9932 - val_loss: 0.3493 - val_accuracy: 0.9201 - lr: 5.0000e-04\n",
      "Epoch 157/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0188 - accuracy: 0.9945\n",
      "Epoch 157: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 0.0188 - accuracy: 0.9945 - val_loss: 0.3107 - val_accuracy: 0.9300 - lr: 5.0000e-04\n",
      "Epoch 158/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0040 - accuracy: 0.9992\n",
      "Epoch 158: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 8s 38ms/step - loss: 0.0040 - accuracy: 0.9992 - val_loss: 0.2966 - val_accuracy: 0.9366 - lr: 5.0000e-04\n",
      "Epoch 159/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9999\n",
      "Epoch 159: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 9s 39ms/step - loss: 0.0020 - accuracy: 0.9999 - val_loss: 0.3004 - val_accuracy: 0.9349 - lr: 5.0000e-04\n",
      "Epoch 160/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9997\n",
      "Epoch 160: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 8s 38ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.2915 - val_accuracy: 0.9399 - lr: 5.0000e-04\n",
      "Epoch 161/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 161: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.3132 - val_accuracy: 0.9339 - lr: 5.0000e-04\n",
      "Epoch 162/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0074 - accuracy: 0.9982\n",
      "Epoch 162: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 0.0074 - accuracy: 0.9982 - val_loss: 0.3577 - val_accuracy: 0.9277 - lr: 5.0000e-04\n",
      "Epoch 163/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9932\n",
      "Epoch 163: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 0.0226 - accuracy: 0.9932 - val_loss: 0.3484 - val_accuracy: 0.9195 - lr: 5.0000e-04\n",
      "Epoch 164/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9972\n",
      "Epoch 164: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 0.0126 - accuracy: 0.9972 - val_loss: 0.3281 - val_accuracy: 0.9323 - lr: 5.0000e-04\n",
      "Epoch 165/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0052 - accuracy: 0.9987\n",
      "Epoch 165: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.0052 - accuracy: 0.9987 - val_loss: 0.3118 - val_accuracy: 0.9343 - lr: 5.0000e-04\n",
      "Epoch 166/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9923\n",
      "Epoch 166: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 0.0246 - accuracy: 0.9923 - val_loss: 0.4039 - val_accuracy: 0.9103 - lr: 5.0000e-04\n",
      "Epoch 167/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0104 - accuracy: 0.9977\n",
      "Epoch 167: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 0.0104 - accuracy: 0.9977 - val_loss: 0.3080 - val_accuracy: 0.9353 - lr: 5.0000e-04\n",
      "Epoch 168/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 0.9999\n",
      "Epoch 168: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 9s 41ms/step - loss: 0.0022 - accuracy: 0.9999 - val_loss: 0.3054 - val_accuracy: 0.9366 - lr: 5.0000e-04\n",
      "Epoch 169/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 7.9063e-04 - accuracy: 1.0000\n",
      "Epoch 169: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 8s 38ms/step - loss: 7.9063e-04 - accuracy: 1.0000 - val_loss: 0.3066 - val_accuracy: 0.9379 - lr: 5.0000e-04\n",
      "Epoch 170/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 8.7277e-04 - accuracy: 1.0000\n",
      "Epoch 170: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 8.7277e-04 - accuracy: 1.0000 - val_loss: 0.3053 - val_accuracy: 0.9362 - lr: 5.0000e-04\n",
      "Epoch 171/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 6.4886e-04 - accuracy: 1.0000\n",
      "Epoch 171: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 6.4886e-04 - accuracy: 1.0000 - val_loss: 0.3114 - val_accuracy: 0.9362 - lr: 5.0000e-04\n",
      "Epoch 172/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 9.7940e-04 - accuracy: 0.9999\n",
      "Epoch 172: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 7s 31ms/step - loss: 9.7940e-04 - accuracy: 0.9999 - val_loss: 0.3069 - val_accuracy: 0.9402 - lr: 5.0000e-04\n",
      "Epoch 173/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9994\n",
      "Epoch 173: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 7s 31ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.3140 - val_accuracy: 0.9330 - lr: 5.0000e-04\n",
      "Epoch 174/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0306 - accuracy: 0.9923\n",
      "Epoch 174: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.0306 - accuracy: 0.9923 - val_loss: 0.3886 - val_accuracy: 0.9096 - lr: 5.0000e-04\n",
      "Epoch 175/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9942\n",
      "Epoch 175: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 0.0199 - accuracy: 0.9942 - val_loss: 0.2929 - val_accuracy: 0.9313 - lr: 5.0000e-04\n",
      "Epoch 176/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0047 - accuracy: 0.9989\n",
      "Epoch 176: val_accuracy did not improve from 0.94085\n",
      "\n",
      "Epoch 176: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 0.0047 - accuracy: 0.9989 - val_loss: 0.2863 - val_accuracy: 0.9372 - lr: 5.0000e-04\n",
      "Epoch 177/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9999\n",
      "Epoch 177: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 0.0016 - accuracy: 0.9999 - val_loss: 0.2765 - val_accuracy: 0.9389 - lr: 2.5000e-04\n",
      "Epoch 178/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0010 - accuracy: 0.9999\n",
      "Epoch 178: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 8s 38ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.2685 - val_accuracy: 0.9392 - lr: 2.5000e-04\n",
      "Epoch 179/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 7.2378e-04 - accuracy: 1.0000\n",
      "Epoch 179: val_accuracy did not improve from 0.94085\n",
      "222/222 [==============================] - 9s 39ms/step - loss: 7.2374e-04 - accuracy: 1.0000 - val_loss: 0.2701 - val_accuracy: 0.9402 - lr: 2.5000e-04\n",
      "Epoch 180/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 7.6611e-04 - accuracy: 1.0000\n",
      "Epoch 180: val_accuracy improved from 0.94085 to 0.94183, saving model to models\\model.keras\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 7.6611e-04 - accuracy: 1.0000 - val_loss: 0.2701 - val_accuracy: 0.9418 - lr: 2.5000e-04\n",
      "Epoch 181/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 5.7565e-04 - accuracy: 1.0000\n",
      "Epoch 181: val_accuracy did not improve from 0.94183\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 5.7476e-04 - accuracy: 1.0000 - val_loss: 0.2718 - val_accuracy: 0.9412 - lr: 2.5000e-04\n",
      "Epoch 182/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 5.0290e-04 - accuracy: 1.0000\n",
      "Epoch 182: val_accuracy improved from 0.94183 to 0.94216, saving model to models\\model.keras\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 5.0003e-04 - accuracy: 1.0000 - val_loss: 0.2687 - val_accuracy: 0.9422 - lr: 2.5000e-04\n",
      "Epoch 183/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 4.5335e-04 - accuracy: 1.0000\n",
      "Epoch 183: val_accuracy did not improve from 0.94216\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 4.5226e-04 - accuracy: 1.0000 - val_loss: 0.2686 - val_accuracy: 0.9408 - lr: 2.5000e-04\n",
      "Epoch 184/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.7199e-04 - accuracy: 1.0000\n",
      "Epoch 184: val_accuracy did not improve from 0.94216\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 3.7199e-04 - accuracy: 1.0000 - val_loss: 0.2683 - val_accuracy: 0.9418 - lr: 2.5000e-04\n",
      "Epoch 185/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.2541e-04 - accuracy: 1.0000\n",
      "Epoch 185: val_accuracy improved from 0.94216 to 0.94249, saving model to models\\model.keras\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 3.2541e-04 - accuracy: 1.0000 - val_loss: 0.2667 - val_accuracy: 0.9425 - lr: 2.5000e-04\n",
      "Epoch 186/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.4340e-04 - accuracy: 1.0000\n",
      "Epoch 186: val_accuracy improved from 0.94249 to 0.94282, saving model to models\\model.keras\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 3.4305e-04 - accuracy: 1.0000 - val_loss: 0.2675 - val_accuracy: 0.9428 - lr: 2.5000e-04\n",
      "Epoch 187/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 4.8477e-04 - accuracy: 0.9999\n",
      "Epoch 187: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 40ms/step - loss: 4.8477e-04 - accuracy: 0.9999 - val_loss: 0.2782 - val_accuracy: 0.9408 - lr: 2.5000e-04\n",
      "Epoch 188/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.4893e-04 - accuracy: 1.0000\n",
      "Epoch 188: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 38ms/step - loss: 3.4830e-04 - accuracy: 1.0000 - val_loss: 0.2825 - val_accuracy: 0.9399 - lr: 2.5000e-04\n",
      "Epoch 189/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.1548e-04 - accuracy: 1.0000\n",
      "Epoch 189: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 3.1497e-04 - accuracy: 1.0000 - val_loss: 0.2787 - val_accuracy: 0.9402 - lr: 2.5000e-04\n",
      "Epoch 190/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.6767e-04 - accuracy: 1.0000\n",
      "Epoch 190: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 2.6773e-04 - accuracy: 1.0000 - val_loss: 0.2782 - val_accuracy: 0.9415 - lr: 2.5000e-04\n",
      "Epoch 191/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.0295e-04 - accuracy: 1.0000\n",
      "Epoch 191: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 2.0250e-04 - accuracy: 1.0000 - val_loss: 0.2776 - val_accuracy: 0.9402 - lr: 2.5000e-04\n",
      "Epoch 192/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.0836e-04 - accuracy: 1.0000\n",
      "Epoch 192: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 2.0929e-04 - accuracy: 1.0000 - val_loss: 0.2797 - val_accuracy: 0.9415 - lr: 2.5000e-04\n",
      "Epoch 193/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.0263e-04 - accuracy: 1.0000\n",
      "Epoch 193: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 2.0263e-04 - accuracy: 1.0000 - val_loss: 0.2793 - val_accuracy: 0.9422 - lr: 2.5000e-04\n",
      "Epoch 194/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 1.8082e-04 - accuracy: 1.0000\n",
      "Epoch 194: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 1.8076e-04 - accuracy: 1.0000 - val_loss: 0.2851 - val_accuracy: 0.9418 - lr: 2.5000e-04\n",
      "Epoch 195/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 1.9581e-04 - accuracy: 1.0000\n",
      "Epoch 195: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 1.9581e-04 - accuracy: 1.0000 - val_loss: 0.2937 - val_accuracy: 0.9395 - lr: 2.5000e-04\n",
      "Epoch 196/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 1.3721e-04 - accuracy: 1.0000\n",
      "Epoch 196: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 38ms/step - loss: 1.3687e-04 - accuracy: 1.0000 - val_loss: 0.2977 - val_accuracy: 0.9399 - lr: 2.5000e-04\n",
      "Epoch 197/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 1.4656e-04 - accuracy: 1.0000\n",
      "Epoch 197: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 38ms/step - loss: 1.4656e-04 - accuracy: 1.0000 - val_loss: 0.3000 - val_accuracy: 0.9389 - lr: 2.5000e-04\n",
      "Epoch 198/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 1.4923e-04 - accuracy: 1.0000\n",
      "Epoch 198: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 1.4923e-04 - accuracy: 1.0000 - val_loss: 0.3036 - val_accuracy: 0.9366 - lr: 2.5000e-04\n",
      "Epoch 199/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0173 - accuracy: 0.9959\n",
      "Epoch 199: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.0172 - accuracy: 0.9959 - val_loss: 0.3948 - val_accuracy: 0.9238 - lr: 2.5000e-04\n",
      "Epoch 200/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 0.9969\n",
      "Epoch 200: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 0.0103 - accuracy: 0.9969 - val_loss: 0.3046 - val_accuracy: 0.9349 - lr: 2.5000e-04\n",
      "Epoch 201/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 0.0058 - accuracy: 0.9987\n",
      "Epoch 201: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 0.0058 - accuracy: 0.9987 - val_loss: 0.3223 - val_accuracy: 0.9303 - lr: 2.5000e-04\n",
      "Epoch 202/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9993\n",
      "Epoch 202: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.3072 - val_accuracy: 0.9376 - lr: 2.5000e-04\n",
      "Epoch 203/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 6.9468e-04 - accuracy: 1.0000\n",
      "Epoch 203: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 6.9468e-04 - accuracy: 1.0000 - val_loss: 0.3054 - val_accuracy: 0.9366 - lr: 2.5000e-04\n",
      "Epoch 204/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 4.3595e-04 - accuracy: 1.0000\n",
      "Epoch 204: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 4.3683e-04 - accuracy: 1.0000 - val_loss: 0.3084 - val_accuracy: 0.9376 - lr: 2.5000e-04\n",
      "Epoch 205/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 4.3546e-04 - accuracy: 1.0000\n",
      "Epoch 205: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 4.3408e-04 - accuracy: 1.0000 - val_loss: 0.3109 - val_accuracy: 0.9382 - lr: 2.5000e-04\n",
      "Epoch 206/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.4870e-04 - accuracy: 1.0000\n",
      "Epoch 206: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 38ms/step - loss: 3.4870e-04 - accuracy: 1.0000 - val_loss: 0.3119 - val_accuracy: 0.9362 - lr: 2.5000e-04\n",
      "Epoch 207/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.4687e-04 - accuracy: 1.0000\n",
      "Epoch 207: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 39ms/step - loss: 3.4702e-04 - accuracy: 1.0000 - val_loss: 0.3146 - val_accuracy: 0.9366 - lr: 2.5000e-04\n",
      "Epoch 208/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.8054e-04 - accuracy: 1.0000\n",
      "Epoch 208: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 2.7970e-04 - accuracy: 1.0000 - val_loss: 0.3174 - val_accuracy: 0.9379 - lr: 2.5000e-04\n",
      "Epoch 209/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.3986e-04 - accuracy: 1.0000\n",
      "Epoch 209: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 2.3908e-04 - accuracy: 1.0000 - val_loss: 0.3173 - val_accuracy: 0.9389 - lr: 2.5000e-04\n",
      "Epoch 210/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.0215e-04 - accuracy: 1.0000\n",
      "Epoch 210: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 2.0215e-04 - accuracy: 1.0000 - val_loss: 0.3185 - val_accuracy: 0.9395 - lr: 2.5000e-04\n",
      "Epoch 211/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 1.8372e-04 - accuracy: 1.0000\n",
      "Epoch 211: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 1.8316e-04 - accuracy: 1.0000 - val_loss: 0.3206 - val_accuracy: 0.9382 - lr: 2.5000e-04\n",
      "Epoch 212/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 1.8905e-04 - accuracy: 1.0000\n",
      "Epoch 212: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 1.8863e-04 - accuracy: 1.0000 - val_loss: 0.3198 - val_accuracy: 0.9385 - lr: 2.5000e-04\n",
      "Epoch 213/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 1.6054e-04 - accuracy: 1.0000\n",
      "Epoch 213: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 1.6054e-04 - accuracy: 1.0000 - val_loss: 0.3187 - val_accuracy: 0.9379 - lr: 2.5000e-04\n",
      "Epoch 214/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 1.5293e-04 - accuracy: 1.0000\n",
      "Epoch 214: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 1.5284e-04 - accuracy: 1.0000 - val_loss: 0.3186 - val_accuracy: 0.9402 - lr: 2.5000e-04\n",
      "Epoch 215/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 1.5011e-04 - accuracy: 1.0000\n",
      "Epoch 215: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 1.4960e-04 - accuracy: 1.0000 - val_loss: 0.3218 - val_accuracy: 0.9402 - lr: 2.5000e-04\n",
      "Epoch 216/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 1.1386e-04 - accuracy: 1.0000\n",
      "Epoch 216: val_accuracy did not improve from 0.94282\n",
      "\n",
      "Epoch 216: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "222/222 [==============================] - 10s 44ms/step - loss: 1.1386e-04 - accuracy: 1.0000 - val_loss: 0.3216 - val_accuracy: 0.9402 - lr: 2.5000e-04\n",
      "Epoch 217/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 1.1808e-04 - accuracy: 1.0000\n",
      "Epoch 217: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 1.1808e-04 - accuracy: 1.0000 - val_loss: 0.3209 - val_accuracy: 0.9395 - lr: 1.2500e-04\n",
      "Epoch 218/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 9.9398e-05 - accuracy: 1.0000\n",
      "Epoch 218: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 9.9132e-05 - accuracy: 1.0000 - val_loss: 0.3213 - val_accuracy: 0.9412 - lr: 1.2500e-04\n",
      "Epoch 219/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 1.2574e-04 - accuracy: 1.0000\n",
      "Epoch 219: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 1.2538e-04 - accuracy: 1.0000 - val_loss: 0.3236 - val_accuracy: 0.9399 - lr: 1.2500e-04\n",
      "Epoch 220/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 1.0437e-04 - accuracy: 1.0000\n",
      "Epoch 220: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 1.0437e-04 - accuracy: 1.0000 - val_loss: 0.3248 - val_accuracy: 0.9392 - lr: 1.2500e-04\n",
      "Epoch 221/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 1.0231e-04 - accuracy: 1.0000\n",
      "Epoch 221: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 1.0231e-04 - accuracy: 1.0000 - val_loss: 0.3250 - val_accuracy: 0.9402 - lr: 1.2500e-04\n",
      "Epoch 222/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 1.1293e-04 - accuracy: 1.0000\n",
      "Epoch 222: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 1.1293e-04 - accuracy: 1.0000 - val_loss: 0.3252 - val_accuracy: 0.9395 - lr: 1.2500e-04\n",
      "Epoch 223/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 1.0008e-04 - accuracy: 1.0000\n",
      "Epoch 223: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 1.0008e-04 - accuracy: 1.0000 - val_loss: 0.3289 - val_accuracy: 0.9392 - lr: 1.2500e-04\n",
      "Epoch 224/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 8.6876e-05 - accuracy: 1.0000\n",
      "Epoch 224: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 8.6876e-05 - accuracy: 1.0000 - val_loss: 0.3282 - val_accuracy: 0.9392 - lr: 1.2500e-04\n",
      "Epoch 225/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.1868e-04 - accuracy: 1.0000\n",
      "Epoch 225: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 41ms/step - loss: 2.1868e-04 - accuracy: 1.0000 - val_loss: 0.3211 - val_accuracy: 0.9385 - lr: 1.2500e-04\n",
      "Epoch 226/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 7.6535e-04 - accuracy: 0.9999\n",
      "Epoch 226: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 7.6535e-04 - accuracy: 0.9999 - val_loss: 0.3271 - val_accuracy: 0.9389 - lr: 1.2500e-04\n",
      "Epoch 227/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 1.1940e-04 - accuracy: 1.0000\n",
      "Epoch 227: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 1.1940e-04 - accuracy: 1.0000 - val_loss: 0.3240 - val_accuracy: 0.9385 - lr: 1.2500e-04\n",
      "Epoch 228/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 1.0677e-04 - accuracy: 1.0000\n",
      "Epoch 228: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 1.0646e-04 - accuracy: 1.0000 - val_loss: 0.3196 - val_accuracy: 0.9395 - lr: 1.2500e-04\n",
      "Epoch 229/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 8.8782e-05 - accuracy: 1.0000\n",
      "Epoch 229: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 34ms/step - loss: 8.8782e-05 - accuracy: 1.0000 - val_loss: 0.3202 - val_accuracy: 0.9372 - lr: 1.2500e-04\n",
      "Epoch 230/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 7.2962e-05 - accuracy: 1.0000\n",
      "Epoch 230: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 7.2962e-05 - accuracy: 1.0000 - val_loss: 0.3217 - val_accuracy: 0.9379 - lr: 1.2500e-04\n",
      "Epoch 231/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 7.3990e-05 - accuracy: 1.0000\n",
      "Epoch 231: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 34ms/step - loss: 7.3990e-05 - accuracy: 1.0000 - val_loss: 0.3239 - val_accuracy: 0.9389 - lr: 1.2500e-04\n",
      "Epoch 232/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 6.2306e-05 - accuracy: 1.0000\n",
      "Epoch 232: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 6.2306e-05 - accuracy: 1.0000 - val_loss: 0.3260 - val_accuracy: 0.9389 - lr: 1.2500e-04\n",
      "Epoch 233/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 8.2425e-05 - accuracy: 1.0000\n",
      "Epoch 233: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 38ms/step - loss: 8.2425e-05 - accuracy: 1.0000 - val_loss: 0.3262 - val_accuracy: 0.9382 - lr: 1.2500e-04\n",
      "Epoch 234/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 7.8190e-05 - accuracy: 1.0000\n",
      "Epoch 234: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 39ms/step - loss: 7.8993e-05 - accuracy: 1.0000 - val_loss: 0.3221 - val_accuracy: 0.9385 - lr: 1.2500e-04\n",
      "Epoch 235/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 4.0993e-05 - accuracy: 1.0000\n",
      "Epoch 235: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 4.2279e-05 - accuracy: 1.0000 - val_loss: 0.3239 - val_accuracy: 0.9389 - lr: 1.2500e-04\n",
      "Epoch 236/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.9768e-05 - accuracy: 1.0000\n",
      "Epoch 236: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 3.9768e-05 - accuracy: 1.0000 - val_loss: 0.3249 - val_accuracy: 0.9385 - lr: 1.2500e-04\n",
      "Epoch 237/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 5.1251e-05 - accuracy: 1.0000\n",
      "Epoch 237: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 5.1251e-05 - accuracy: 1.0000 - val_loss: 0.3322 - val_accuracy: 0.9382 - lr: 1.2500e-04\n",
      "Epoch 238/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 5.6044e-05 - accuracy: 1.0000\n",
      "Epoch 238: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 5.6044e-05 - accuracy: 1.0000 - val_loss: 0.3304 - val_accuracy: 0.9408 - lr: 1.2500e-04\n",
      "Epoch 239/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 4.1893e-05 - accuracy: 1.0000\n",
      "Epoch 239: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 4.1767e-05 - accuracy: 1.0000 - val_loss: 0.3354 - val_accuracy: 0.9395 - lr: 1.2500e-04\n",
      "Epoch 240/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.4585e-05 - accuracy: 1.0000\n",
      "Epoch 240: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 3.4459e-05 - accuracy: 1.0000 - val_loss: 0.3355 - val_accuracy: 0.9382 - lr: 1.2500e-04\n",
      "Epoch 241/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.6854e-05 - accuracy: 1.0000\n",
      "Epoch 241: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 3.6747e-05 - accuracy: 1.0000 - val_loss: 0.3381 - val_accuracy: 0.9399 - lr: 1.2500e-04\n",
      "Epoch 242/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 6.7458e-05 - accuracy: 1.0000\n",
      "Epoch 242: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 6.7212e-05 - accuracy: 1.0000 - val_loss: 0.3454 - val_accuracy: 0.9376 - lr: 1.2500e-04\n",
      "Epoch 243/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 9.5617e-05 - accuracy: 1.0000\n",
      "Epoch 243: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 11s 48ms/step - loss: 9.5617e-05 - accuracy: 1.0000 - val_loss: 0.3506 - val_accuracy: 0.9372 - lr: 1.2500e-04\n",
      "Epoch 244/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 5.3002e-05 - accuracy: 1.0000\n",
      "Epoch 244: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 42ms/step - loss: 5.3002e-05 - accuracy: 1.0000 - val_loss: 0.3556 - val_accuracy: 0.9339 - lr: 1.2500e-04\n",
      "Epoch 245/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.8086e-05 - accuracy: 1.0000\n",
      "Epoch 245: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 3.8086e-05 - accuracy: 1.0000 - val_loss: 0.3557 - val_accuracy: 0.9359 - lr: 1.2500e-04\n",
      "Epoch 246/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.4921e-05 - accuracy: 1.0000\n",
      "Epoch 246: val_accuracy did not improve from 0.94282\n",
      "\n",
      "Epoch 246: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 2.4842e-05 - accuracy: 1.0000 - val_loss: 0.3556 - val_accuracy: 0.9372 - lr: 1.2500e-04\n",
      "Epoch 247/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 2.9240e-05 - accuracy: 1.0000\n",
      "Epoch 247: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 2.9142e-05 - accuracy: 1.0000 - val_loss: 0.3565 - val_accuracy: 0.9372 - lr: 6.2500e-05\n",
      "Epoch 248/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.5834e-05 - accuracy: 1.0000\n",
      "Epoch 248: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 3.5736e-05 - accuracy: 1.0000 - val_loss: 0.3573 - val_accuracy: 0.9376 - lr: 6.2500e-05\n",
      "Epoch 249/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.7642e-05 - accuracy: 1.0000\n",
      "Epoch 249: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 2.7642e-05 - accuracy: 1.0000 - val_loss: 0.3574 - val_accuracy: 0.9379 - lr: 6.2500e-05\n",
      "Epoch 250/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.8226e-05 - accuracy: 1.0000\n",
      "Epoch 250: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 2.8226e-05 - accuracy: 1.0000 - val_loss: 0.3601 - val_accuracy: 0.9372 - lr: 6.2500e-05\n",
      "Epoch 251/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.1152e-05 - accuracy: 1.0000\n",
      "Epoch 251: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 2.1152e-05 - accuracy: 1.0000 - val_loss: 0.3597 - val_accuracy: 0.9362 - lr: 6.2500e-05\n",
      "Epoch 252/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.2003e-05 - accuracy: 1.0000\n",
      "Epoch 252: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 40ms/step - loss: 2.2003e-05 - accuracy: 1.0000 - val_loss: 0.3598 - val_accuracy: 0.9359 - lr: 6.2500e-05\n",
      "Epoch 253/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.5820e-05 - accuracy: 1.0000\n",
      "Epoch 253: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 39ms/step - loss: 2.5820e-05 - accuracy: 1.0000 - val_loss: 0.3599 - val_accuracy: 0.9359 - lr: 6.2500e-05\n",
      "Epoch 254/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.2476e-05 - accuracy: 1.0000\n",
      "Epoch 254: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 2.2406e-05 - accuracy: 1.0000 - val_loss: 0.3583 - val_accuracy: 0.9376 - lr: 6.2500e-05\n",
      "Epoch 255/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 1.9468e-05 - accuracy: 1.0000\n",
      "Epoch 255: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 1.9498e-05 - accuracy: 1.0000 - val_loss: 0.3573 - val_accuracy: 0.9379 - lr: 6.2500e-05\n",
      "Epoch 256/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 1.8203e-05 - accuracy: 1.0000\n",
      "Epoch 256: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 1.8994e-05 - accuracy: 1.0000 - val_loss: 0.3592 - val_accuracy: 0.9376 - lr: 6.2500e-05\n",
      "Epoch 257/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 5.5054e-05 - accuracy: 1.0000\n",
      "Epoch 257: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 5.5054e-05 - accuracy: 1.0000 - val_loss: 0.3704 - val_accuracy: 0.9379 - lr: 6.2500e-05\n",
      "Epoch 258/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.7494e-05 - accuracy: 1.0000\n",
      "Epoch 258: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 2.7432e-05 - accuracy: 1.0000 - val_loss: 0.3644 - val_accuracy: 0.9385 - lr: 6.2500e-05\n",
      "Epoch 259/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.4939e-05 - accuracy: 1.0000\n",
      "Epoch 259: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 2.4875e-05 - accuracy: 1.0000 - val_loss: 0.3577 - val_accuracy: 0.9392 - lr: 6.2500e-05\n",
      "Epoch 260/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 5.1209e-05 - accuracy: 1.0000\n",
      "Epoch 260: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 5.1209e-05 - accuracy: 1.0000 - val_loss: 0.3646 - val_accuracy: 0.9369 - lr: 6.2500e-05\n",
      "Epoch 261/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 1.9996e-05 - accuracy: 1.0000\n",
      "Epoch 261: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 1.9948e-05 - accuracy: 1.0000 - val_loss: 0.3664 - val_accuracy: 0.9392 - lr: 6.2500e-05\n",
      "Epoch 262/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.0819e-05 - accuracy: 1.0000\n",
      "Epoch 262: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 42ms/step - loss: 2.0819e-05 - accuracy: 1.0000 - val_loss: 0.3694 - val_accuracy: 0.9395 - lr: 6.2500e-05\n",
      "Epoch 263/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.3123e-05 - accuracy: 1.0000\n",
      "Epoch 263: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 38ms/step - loss: 2.3044e-05 - accuracy: 1.0000 - val_loss: 0.3687 - val_accuracy: 0.9405 - lr: 6.2500e-05\n",
      "Epoch 264/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 1.5234e-05 - accuracy: 1.0000\n",
      "Epoch 264: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 1.5184e-05 - accuracy: 1.0000 - val_loss: 0.3714 - val_accuracy: 0.9382 - lr: 6.2500e-05\n",
      "Epoch 265/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 1.8614e-05 - accuracy: 1.0000\n",
      "Epoch 265: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 1.8532e-05 - accuracy: 1.0000 - val_loss: 0.3713 - val_accuracy: 0.9385 - lr: 6.2500e-05\n",
      "Epoch 266/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 1.3984e-05 - accuracy: 1.0000\n",
      "Epoch 266: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 34ms/step - loss: 1.3984e-05 - accuracy: 1.0000 - val_loss: 0.3656 - val_accuracy: 0.9408 - lr: 6.2500e-05\n",
      "Epoch 267/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 1.6310e-05 - accuracy: 1.0000\n",
      "Epoch 267: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 1.6591e-05 - accuracy: 1.0000 - val_loss: 0.3649 - val_accuracy: 0.9382 - lr: 6.2500e-05\n",
      "Epoch 268/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.0517e-05 - accuracy: 1.0000\n",
      "Epoch 268: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 34ms/step - loss: 2.0469e-05 - accuracy: 1.0000 - val_loss: 0.3694 - val_accuracy: 0.9389 - lr: 6.2500e-05\n",
      "Epoch 269/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 1.5310e-05 - accuracy: 1.0000\n",
      "Epoch 269: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 1.5278e-05 - accuracy: 1.0000 - val_loss: 0.3720 - val_accuracy: 0.9392 - lr: 6.2500e-05\n",
      "Epoch 270/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 1.6102e-05 - accuracy: 1.0000\n",
      "Epoch 270: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 1.6332e-05 - accuracy: 1.0000 - val_loss: 0.3741 - val_accuracy: 0.9402 - lr: 6.2500e-05\n",
      "Epoch 271/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 1.1191e-05 - accuracy: 1.0000\n",
      "Epoch 271: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 41ms/step - loss: 1.1215e-05 - accuracy: 1.0000 - val_loss: 0.3759 - val_accuracy: 0.9402 - lr: 6.2500e-05\n",
      "Epoch 272/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 1.3947e-05 - accuracy: 1.0000\n",
      "Epoch 272: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 39ms/step - loss: 1.3947e-05 - accuracy: 1.0000 - val_loss: 0.3798 - val_accuracy: 0.9389 - lr: 6.2500e-05\n",
      "Epoch 273/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 1.1332e-05 - accuracy: 1.0000\n",
      "Epoch 273: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 1.1332e-05 - accuracy: 1.0000 - val_loss: 0.3824 - val_accuracy: 0.9405 - lr: 6.2500e-05\n",
      "Epoch 274/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 1.1723e-05 - accuracy: 1.0000\n",
      "Epoch 274: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 1.1688e-05 - accuracy: 1.0000 - val_loss: 0.3873 - val_accuracy: 0.9379 - lr: 6.2500e-05\n",
      "Epoch 275/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 1.9182e-05 - accuracy: 1.0000\n",
      "Epoch 275: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 1.9115e-05 - accuracy: 1.0000 - val_loss: 0.3804 - val_accuracy: 0.9379 - lr: 6.2500e-05\n",
      "Epoch 276/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 1.3030e-05 - accuracy: 1.0000\n",
      "Epoch 276: val_accuracy did not improve from 0.94282\n",
      "\n",
      "Epoch 276: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 1.2995e-05 - accuracy: 1.0000 - val_loss: 0.3846 - val_accuracy: 0.9382 - lr: 6.2500e-05\n",
      "Epoch 277/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 1.2194e-05 - accuracy: 1.0000\n",
      "Epoch 277: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 1.2194e-05 - accuracy: 1.0000 - val_loss: 0.3904 - val_accuracy: 0.9366 - lr: 3.1250e-05\n",
      "Epoch 278/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 9.1005e-06 - accuracy: 1.0000\n",
      "Epoch 278: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 9.1005e-06 - accuracy: 1.0000 - val_loss: 0.3877 - val_accuracy: 0.9376 - lr: 3.1250e-05\n",
      "Epoch 279/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 1.7407e-05 - accuracy: 1.0000\n",
      "Epoch 279: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 1.7407e-05 - accuracy: 1.0000 - val_loss: 0.3892 - val_accuracy: 0.9369 - lr: 3.1250e-05\n",
      "Epoch 280/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.3738e-05 - accuracy: 1.0000\n",
      "Epoch 280: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 38ms/step - loss: 2.3738e-05 - accuracy: 1.0000 - val_loss: 0.3820 - val_accuracy: 0.9395 - lr: 3.1250e-05\n",
      "Epoch 281/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 1.1990e-05 - accuracy: 1.0000\n",
      "Epoch 281: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 38ms/step - loss: 1.1990e-05 - accuracy: 1.0000 - val_loss: 0.3867 - val_accuracy: 0.9379 - lr: 3.1250e-05\n",
      "Epoch 282/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 1.2674e-05 - accuracy: 1.0000\n",
      "Epoch 282: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 1.2629e-05 - accuracy: 1.0000 - val_loss: 0.3871 - val_accuracy: 0.9379 - lr: 3.1250e-05\n",
      "Epoch 283/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 8.5460e-06 - accuracy: 1.0000\n",
      "Epoch 283: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 8.5460e-06 - accuracy: 1.0000 - val_loss: 0.3872 - val_accuracy: 0.9385 - lr: 3.1250e-05\n",
      "Epoch 284/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 9.1936e-06 - accuracy: 1.0000\n",
      "Epoch 284: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 9.1521e-06 - accuracy: 1.0000 - val_loss: 0.3895 - val_accuracy: 0.9382 - lr: 3.1250e-05\n",
      "Epoch 285/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 8.3827e-06 - accuracy: 1.0000\n",
      "Epoch 285: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 8.3827e-06 - accuracy: 1.0000 - val_loss: 0.3912 - val_accuracy: 0.9382 - lr: 3.1250e-05\n",
      "Epoch 286/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 7.7923e-06 - accuracy: 1.0000\n",
      "Epoch 286: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 7.7760e-06 - accuracy: 1.0000 - val_loss: 0.3931 - val_accuracy: 0.9379 - lr: 3.1250e-05\n",
      "Epoch 287/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 7.8552e-06 - accuracy: 1.0000\n",
      "Epoch 287: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 7.8552e-06 - accuracy: 1.0000 - val_loss: 0.3907 - val_accuracy: 0.9382 - lr: 3.1250e-05\n",
      "Epoch 288/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.7758e-05 - accuracy: 1.0000\n",
      "Epoch 288: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 3.7758e-05 - accuracy: 1.0000 - val_loss: 0.3974 - val_accuracy: 0.9356 - lr: 3.1250e-05\n",
      "Epoch 289/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 8.4593e-06 - accuracy: 1.0000\n",
      "Epoch 289: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 8.4593e-06 - accuracy: 1.0000 - val_loss: 0.3942 - val_accuracy: 0.9382 - lr: 3.1250e-05\n",
      "Epoch 290/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 1.1014e-05 - accuracy: 1.0000\n",
      "Epoch 290: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 41ms/step - loss: 1.1043e-05 - accuracy: 1.0000 - val_loss: 0.3946 - val_accuracy: 0.9389 - lr: 3.1250e-05\n",
      "Epoch 291/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 1.0015e-05 - accuracy: 1.0000\n",
      "Epoch 291: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 1.0015e-05 - accuracy: 1.0000 - val_loss: 0.3915 - val_accuracy: 0.9395 - lr: 3.1250e-05\n",
      "Epoch 292/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 6.0519e-06 - accuracy: 1.0000\n",
      "Epoch 292: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 34ms/step - loss: 6.0519e-06 - accuracy: 1.0000 - val_loss: 0.3902 - val_accuracy: 0.9392 - lr: 3.1250e-05\n",
      "Epoch 293/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 9.1913e-06 - accuracy: 1.0000\n",
      "Epoch 293: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 9.1913e-06 - accuracy: 1.0000 - val_loss: 0.3907 - val_accuracy: 0.9402 - lr: 3.1250e-05\n",
      "Epoch 294/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 1.1523e-05 - accuracy: 1.0000\n",
      "Epoch 294: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 1.1523e-05 - accuracy: 1.0000 - val_loss: 0.3910 - val_accuracy: 0.9399 - lr: 3.1250e-05\n",
      "Epoch 295/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 6.3522e-06 - accuracy: 1.0000\n",
      "Epoch 295: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 6.3522e-06 - accuracy: 1.0000 - val_loss: 0.3937 - val_accuracy: 0.9389 - lr: 3.1250e-05\n",
      "Epoch 296/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 8.8896e-06 - accuracy: 1.0000\n",
      "Epoch 296: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 8.8571e-06 - accuracy: 1.0000 - val_loss: 0.3958 - val_accuracy: 0.9389 - lr: 3.1250e-05\n",
      "Epoch 297/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 8.1390e-06 - accuracy: 1.0000\n",
      "Epoch 297: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 8.1390e-06 - accuracy: 1.0000 - val_loss: 0.3975 - val_accuracy: 0.9379 - lr: 3.1250e-05\n",
      "Epoch 298/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 1.3549e-05 - accuracy: 1.0000\n",
      "Epoch 298: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 40ms/step - loss: 1.3506e-05 - accuracy: 1.0000 - val_loss: 0.3984 - val_accuracy: 0.9372 - lr: 3.1250e-05\n",
      "Epoch 299/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 8.1539e-06 - accuracy: 1.0000\n",
      "Epoch 299: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 38ms/step - loss: 8.1539e-06 - accuracy: 1.0000 - val_loss: 0.4021 - val_accuracy: 0.9362 - lr: 3.1250e-05\n",
      "Epoch 300/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 4.7712e-06 - accuracy: 1.0000\n",
      "Epoch 300: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 4.8114e-06 - accuracy: 1.0000 - val_loss: 0.4018 - val_accuracy: 0.9349 - lr: 3.1250e-05\n",
      "Epoch 301/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 5.1108e-06 - accuracy: 1.0000\n",
      "Epoch 301: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 34ms/step - loss: 5.1108e-06 - accuracy: 1.0000 - val_loss: 0.4013 - val_accuracy: 0.9379 - lr: 3.1250e-05\n",
      "Epoch 302/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 7.4652e-06 - accuracy: 1.0000\n",
      "Epoch 302: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 7.4652e-06 - accuracy: 1.0000 - val_loss: 0.4002 - val_accuracy: 0.9385 - lr: 3.1250e-05\n",
      "Epoch 303/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 9.8047e-06 - accuracy: 1.0000\n",
      "Epoch 303: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 9.7704e-06 - accuracy: 1.0000 - val_loss: 0.4078 - val_accuracy: 0.9369 - lr: 3.1250e-05\n",
      "Epoch 304/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 8.8291e-06 - accuracy: 1.0000\n",
      "Epoch 304: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 8.8101e-06 - accuracy: 1.0000 - val_loss: 0.4089 - val_accuracy: 0.9362 - lr: 3.1250e-05\n",
      "Epoch 305/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 4.8761e-06 - accuracy: 1.0000\n",
      "Epoch 305: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 4.8761e-06 - accuracy: 1.0000 - val_loss: 0.4089 - val_accuracy: 0.9372 - lr: 3.1250e-05\n",
      "Epoch 306/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 7.4867e-06 - accuracy: 1.0000\n",
      "Epoch 306: val_accuracy did not improve from 0.94282\n",
      "\n",
      "Epoch 306: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 7.4867e-06 - accuracy: 1.0000 - val_loss: 0.4091 - val_accuracy: 0.9369 - lr: 3.1250e-05\n",
      "Epoch 307/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 5.3179e-06 - accuracy: 1.0000\n",
      "Epoch 307: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 5.3179e-06 - accuracy: 1.0000 - val_loss: 0.4078 - val_accuracy: 0.9362 - lr: 1.5625e-05\n",
      "Epoch 308/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 8.3710e-06 - accuracy: 1.0000\n",
      "Epoch 308: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 39ms/step - loss: 8.3710e-06 - accuracy: 1.0000 - val_loss: 0.4069 - val_accuracy: 0.9372 - lr: 1.5625e-05\n",
      "Epoch 309/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 5.2645e-06 - accuracy: 1.0000\n",
      "Epoch 309: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 10s 43ms/step - loss: 5.2645e-06 - accuracy: 1.0000 - val_loss: 0.4062 - val_accuracy: 0.9359 - lr: 1.5625e-05\n",
      "Epoch 310/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 6.4784e-06 - accuracy: 1.0000\n",
      "Epoch 310: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 6.4553e-06 - accuracy: 1.0000 - val_loss: 0.4034 - val_accuracy: 0.9372 - lr: 1.5625e-05\n",
      "Epoch 311/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 6.0391e-06 - accuracy: 1.0000\n",
      "Epoch 311: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 6.0011e-06 - accuracy: 1.0000 - val_loss: 0.4042 - val_accuracy: 0.9369 - lr: 1.5625e-05\n",
      "Epoch 312/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 8.3574e-06 - accuracy: 1.0000\n",
      "Epoch 312: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 8.3574e-06 - accuracy: 1.0000 - val_loss: 0.4053 - val_accuracy: 0.9362 - lr: 1.5625e-05\n",
      "Epoch 313/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 6.2051e-06 - accuracy: 1.0000\n",
      "Epoch 313: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 34ms/step - loss: 6.1872e-06 - accuracy: 1.0000 - val_loss: 0.4051 - val_accuracy: 0.9362 - lr: 1.5625e-05\n",
      "Epoch 314/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 5.5128e-06 - accuracy: 1.0000\n",
      "Epoch 314: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 5.5128e-06 - accuracy: 1.0000 - val_loss: 0.4054 - val_accuracy: 0.9366 - lr: 1.5625e-05\n",
      "Epoch 315/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 5.5154e-06 - accuracy: 1.0000\n",
      "Epoch 315: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 5.5154e-06 - accuracy: 1.0000 - val_loss: 0.4058 - val_accuracy: 0.9369 - lr: 1.5625e-05\n",
      "Epoch 316/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 5.7923e-06 - accuracy: 1.0000\n",
      "Epoch 316: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 5.7923e-06 - accuracy: 1.0000 - val_loss: 0.4077 - val_accuracy: 0.9372 - lr: 1.5625e-05\n",
      "Epoch 317/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 7.8674e-06 - accuracy: 1.0000\n",
      "Epoch 317: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 39ms/step - loss: 7.8674e-06 - accuracy: 1.0000 - val_loss: 0.4093 - val_accuracy: 0.9366 - lr: 1.5625e-05\n",
      "Epoch 318/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 4.8813e-06 - accuracy: 1.0000\n",
      "Epoch 318: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 39ms/step - loss: 4.8665e-06 - accuracy: 1.0000 - val_loss: 0.4099 - val_accuracy: 0.9372 - lr: 1.5625e-05\n",
      "Epoch 319/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 5.5690e-06 - accuracy: 1.0000\n",
      "Epoch 319: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 5.5690e-06 - accuracy: 1.0000 - val_loss: 0.4074 - val_accuracy: 0.9376 - lr: 1.5625e-05\n",
      "Epoch 320/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 6.1071e-06 - accuracy: 1.0000\n",
      "Epoch 320: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 34ms/step - loss: 6.0861e-06 - accuracy: 1.0000 - val_loss: 0.4067 - val_accuracy: 0.9372 - lr: 1.5625e-05\n",
      "Epoch 321/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 5.2338e-06 - accuracy: 1.0000\n",
      "Epoch 321: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 5.1991e-06 - accuracy: 1.0000 - val_loss: 0.4069 - val_accuracy: 0.9369 - lr: 1.5625e-05\n",
      "Epoch 322/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 4.4844e-06 - accuracy: 1.0000\n",
      "Epoch 322: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 4.4844e-06 - accuracy: 1.0000 - val_loss: 0.4074 - val_accuracy: 0.9366 - lr: 1.5625e-05\n",
      "Epoch 323/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 5.3104e-06 - accuracy: 1.0000\n",
      "Epoch 323: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 5.3031e-06 - accuracy: 1.0000 - val_loss: 0.4106 - val_accuracy: 0.9366 - lr: 1.5625e-05\n",
      "Epoch 324/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 8.7671e-06 - accuracy: 1.0000\n",
      "Epoch 324: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 40ms/step - loss: 8.7671e-06 - accuracy: 1.0000 - val_loss: 0.4110 - val_accuracy: 0.9385 - lr: 1.5625e-05\n",
      "Epoch 325/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 4.8633e-06 - accuracy: 1.0000\n",
      "Epoch 325: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 10s 45ms/step - loss: 4.8458e-06 - accuracy: 1.0000 - val_loss: 0.4097 - val_accuracy: 0.9379 - lr: 1.5625e-05\n",
      "Epoch 326/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 4.6348e-06 - accuracy: 1.0000\n",
      "Epoch 326: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 11s 52ms/step - loss: 4.6206e-06 - accuracy: 1.0000 - val_loss: 0.4084 - val_accuracy: 0.9379 - lr: 1.5625e-05\n",
      "Epoch 327/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 5.0330e-06 - accuracy: 1.0000\n",
      "Epoch 327: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 11s 51ms/step - loss: 5.0330e-06 - accuracy: 1.0000 - val_loss: 0.4061 - val_accuracy: 0.9379 - lr: 1.5625e-05\n",
      "Epoch 328/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 4.6667e-06 - accuracy: 1.0000\n",
      "Epoch 328: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 4.6667e-06 - accuracy: 1.0000 - val_loss: 0.4062 - val_accuracy: 0.9379 - lr: 1.5625e-05\n",
      "Epoch 329/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 4.5854e-06 - accuracy: 1.0000\n",
      "Epoch 329: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 4.5742e-06 - accuracy: 1.0000 - val_loss: 0.4073 - val_accuracy: 0.9376 - lr: 1.5625e-05\n",
      "Epoch 330/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.7006e-06 - accuracy: 1.0000\n",
      "Epoch 330: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 3.7006e-06 - accuracy: 1.0000 - val_loss: 0.4073 - val_accuracy: 0.9385 - lr: 1.5625e-05\n",
      "Epoch 331/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 4.8999e-06 - accuracy: 1.0000\n",
      "Epoch 331: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 38ms/step - loss: 4.9587e-06 - accuracy: 1.0000 - val_loss: 0.4090 - val_accuracy: 0.9382 - lr: 1.5625e-05\n",
      "Epoch 332/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 4.4623e-06 - accuracy: 1.0000\n",
      "Epoch 332: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 4.4623e-06 - accuracy: 1.0000 - val_loss: 0.4113 - val_accuracy: 0.9376 - lr: 1.5625e-05\n",
      "Epoch 333/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 5.5215e-06 - accuracy: 1.0000\n",
      "Epoch 333: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 5.5050e-06 - accuracy: 1.0000 - val_loss: 0.4088 - val_accuracy: 0.9392 - lr: 1.5625e-05\n",
      "Epoch 334/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 5.2133e-06 - accuracy: 1.0000\n",
      "Epoch 334: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 5.2133e-06 - accuracy: 1.0000 - val_loss: 0.4103 - val_accuracy: 0.9376 - lr: 1.5625e-05\n",
      "Epoch 335/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 6.0813e-06 - accuracy: 1.0000\n",
      "Epoch 335: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 39ms/step - loss: 6.0784e-06 - accuracy: 1.0000 - val_loss: 0.4155 - val_accuracy: 0.9389 - lr: 1.5625e-05\n",
      "Epoch 336/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 7.8682e-06 - accuracy: 1.0000\n",
      "Epoch 336: val_accuracy did not improve from 0.94282\n",
      "\n",
      "Epoch 336: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "222/222 [==============================] - 9s 40ms/step - loss: 7.8682e-06 - accuracy: 1.0000 - val_loss: 0.4170 - val_accuracy: 0.9385 - lr: 1.5625e-05\n",
      "Epoch 337/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.7438e-06 - accuracy: 1.0000\n",
      "Epoch 337: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 39ms/step - loss: 3.7366e-06 - accuracy: 1.0000 - val_loss: 0.4172 - val_accuracy: 0.9392 - lr: 7.8125e-06\n",
      "Epoch 338/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 5.6503e-06 - accuracy: 1.0000\n",
      "Epoch 338: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 5.6332e-06 - accuracy: 1.0000 - val_loss: 0.4152 - val_accuracy: 0.9392 - lr: 7.8125e-06\n",
      "Epoch 339/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 3.7690e-06 - accuracy: 1.0000\n",
      "Epoch 339: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 3.7423e-06 - accuracy: 1.0000 - val_loss: 0.4150 - val_accuracy: 0.9385 - lr: 7.8125e-06\n",
      "Epoch 340/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 4.0652e-06 - accuracy: 1.0000\n",
      "Epoch 340: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 4.0509e-06 - accuracy: 1.0000 - val_loss: 0.4157 - val_accuracy: 0.9382 - lr: 7.8125e-06\n",
      "Epoch 341/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 7.8764e-06 - accuracy: 1.0000\n",
      "Epoch 341: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 7.8477e-06 - accuracy: 1.0000 - val_loss: 0.4154 - val_accuracy: 0.9382 - lr: 7.8125e-06\n",
      "Epoch 342/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 4.1197e-06 - accuracy: 1.0000\n",
      "Epoch 342: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 4.1221e-06 - accuracy: 1.0000 - val_loss: 0.4150 - val_accuracy: 0.9382 - lr: 7.8125e-06\n",
      "Epoch 343/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 4.9344e-06 - accuracy: 1.0000\n",
      "Epoch 343: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 4.9181e-06 - accuracy: 1.0000 - val_loss: 0.4150 - val_accuracy: 0.9382 - lr: 7.8125e-06\n",
      "Epoch 344/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 4.3340e-06 - accuracy: 1.0000\n",
      "Epoch 344: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 4.3221e-06 - accuracy: 1.0000 - val_loss: 0.4156 - val_accuracy: 0.9382 - lr: 7.8125e-06\n",
      "Epoch 345/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 4.0846e-06 - accuracy: 1.0000\n",
      "Epoch 345: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 39ms/step - loss: 4.0846e-06 - accuracy: 1.0000 - val_loss: 0.4151 - val_accuracy: 0.9389 - lr: 7.8125e-06\n",
      "Epoch 346/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.6795e-06 - accuracy: 1.0000\n",
      "Epoch 346: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 39ms/step - loss: 3.6795e-06 - accuracy: 1.0000 - val_loss: 0.4155 - val_accuracy: 0.9389 - lr: 7.8125e-06\n",
      "Epoch 347/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.9239e-06 - accuracy: 1.0000\n",
      "Epoch 347: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 34ms/step - loss: 3.9239e-06 - accuracy: 1.0000 - val_loss: 0.4169 - val_accuracy: 0.9395 - lr: 7.8125e-06\n",
      "Epoch 348/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 4.2234e-06 - accuracy: 1.0000\n",
      "Epoch 348: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 4.2234e-06 - accuracy: 1.0000 - val_loss: 0.4160 - val_accuracy: 0.9395 - lr: 7.8125e-06\n",
      "Epoch 349/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.2743e-06 - accuracy: 1.0000\n",
      "Epoch 349: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 3.2634e-06 - accuracy: 1.0000 - val_loss: 0.4161 - val_accuracy: 0.9389 - lr: 7.8125e-06\n",
      "Epoch 350/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 6.9172e-06 - accuracy: 1.0000\n",
      "Epoch 350: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 6.8927e-06 - accuracy: 1.0000 - val_loss: 0.4151 - val_accuracy: 0.9382 - lr: 7.8125e-06\n",
      "Epoch 351/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.9239e-06 - accuracy: 1.0000\n",
      "Epoch 351: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 2.9239e-06 - accuracy: 1.0000 - val_loss: 0.4156 - val_accuracy: 0.9385 - lr: 7.8125e-06\n",
      "Epoch 352/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 4.1644e-06 - accuracy: 1.0000\n",
      "Epoch 352: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 4.1644e-06 - accuracy: 1.0000 - val_loss: 0.4157 - val_accuracy: 0.9392 - lr: 7.8125e-06\n",
      "Epoch 353/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.0808e-06 - accuracy: 1.0000\n",
      "Epoch 353: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 39ms/step - loss: 3.0756e-06 - accuracy: 1.0000 - val_loss: 0.4165 - val_accuracy: 0.9385 - lr: 7.8125e-06\n",
      "Epoch 354/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.4569e-06 - accuracy: 1.0000\n",
      "Epoch 354: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 10s 44ms/step - loss: 3.4569e-06 - accuracy: 1.0000 - val_loss: 0.4182 - val_accuracy: 0.9382 - lr: 7.8125e-06\n",
      "Epoch 355/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 4.0942e-06 - accuracy: 1.0000\n",
      "Epoch 355: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 4.0802e-06 - accuracy: 1.0000 - val_loss: 0.4184 - val_accuracy: 0.9382 - lr: 7.8125e-06\n",
      "Epoch 356/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 4.0170e-06 - accuracy: 1.0000\n",
      "Epoch 356: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 4.0170e-06 - accuracy: 1.0000 - val_loss: 0.4178 - val_accuracy: 0.9389 - lr: 7.8125e-06\n",
      "Epoch 357/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 3.0993e-06 - accuracy: 1.0000\n",
      "Epoch 357: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 3.0824e-06 - accuracy: 1.0000 - val_loss: 0.4171 - val_accuracy: 0.9395 - lr: 7.8125e-06\n",
      "Epoch 358/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 8.1256e-06 - accuracy: 1.0000\n",
      "Epoch 358: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 8.0992e-06 - accuracy: 1.0000 - val_loss: 0.4171 - val_accuracy: 0.9395 - lr: 7.8125e-06\n",
      "Epoch 359/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 4.8021e-06 - accuracy: 1.0000\n",
      "Epoch 359: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 34ms/step - loss: 4.8021e-06 - accuracy: 1.0000 - val_loss: 0.4152 - val_accuracy: 0.9399 - lr: 7.8125e-06\n",
      "Epoch 360/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.8280e-06 - accuracy: 1.0000\n",
      "Epoch 360: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 3.8280e-06 - accuracy: 1.0000 - val_loss: 0.4146 - val_accuracy: 0.9389 - lr: 7.8125e-06\n",
      "Epoch 361/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.6449e-06 - accuracy: 1.0000\n",
      "Epoch 361: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 38ms/step - loss: 3.6323e-06 - accuracy: 1.0000 - val_loss: 0.4148 - val_accuracy: 0.9389 - lr: 7.8125e-06\n",
      "Epoch 362/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 8.6125e-06 - accuracy: 1.0000\n",
      "Epoch 362: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 39ms/step - loss: 8.6125e-06 - accuracy: 1.0000 - val_loss: 0.4178 - val_accuracy: 0.9385 - lr: 7.8125e-06\n",
      "Epoch 363/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 6.9533e-06 - accuracy: 1.0000\n",
      "Epoch 363: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 39ms/step - loss: 6.9533e-06 - accuracy: 1.0000 - val_loss: 0.4164 - val_accuracy: 0.9392 - lr: 7.8125e-06\n",
      "Epoch 364/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.1635e-06 - accuracy: 1.0000\n",
      "Epoch 364: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 3.1562e-06 - accuracy: 1.0000 - val_loss: 0.4160 - val_accuracy: 0.9392 - lr: 7.8125e-06\n",
      "Epoch 365/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 5.2933e-06 - accuracy: 1.0000\n",
      "Epoch 365: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 5.2740e-06 - accuracy: 1.0000 - val_loss: 0.4168 - val_accuracy: 0.9385 - lr: 7.8125e-06\n",
      "Epoch 366/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 5.4574e-06 - accuracy: 1.0000\n",
      "Epoch 366: val_accuracy did not improve from 0.94282\n",
      "\n",
      "Epoch 366: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 5.4433e-06 - accuracy: 1.0000 - val_loss: 0.4152 - val_accuracy: 0.9392 - lr: 7.8125e-06\n",
      "Epoch 367/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.5320e-06 - accuracy: 1.0000\n",
      "Epoch 367: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 3.5336e-06 - accuracy: 1.0000 - val_loss: 0.4155 - val_accuracy: 0.9389 - lr: 3.9063e-06\n",
      "Epoch 368/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.5511e-06 - accuracy: 1.0000\n",
      "Epoch 368: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 3.5444e-06 - accuracy: 1.0000 - val_loss: 0.4152 - val_accuracy: 0.9389 - lr: 3.9063e-06\n",
      "Epoch 369/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 4.9798e-06 - accuracy: 1.0000\n",
      "Epoch 369: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 4.9772e-06 - accuracy: 1.0000 - val_loss: 0.4139 - val_accuracy: 0.9399 - lr: 3.9063e-06\n",
      "Epoch 370/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.5235e-06 - accuracy: 1.0000\n",
      "Epoch 370: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 3.5214e-06 - accuracy: 1.0000 - val_loss: 0.4141 - val_accuracy: 0.9399 - lr: 3.9063e-06\n",
      "Epoch 371/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 4.2064e-06 - accuracy: 1.0000\n",
      "Epoch 371: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 39ms/step - loss: 4.2205e-06 - accuracy: 1.0000 - val_loss: 0.4138 - val_accuracy: 0.9395 - lr: 3.9063e-06\n",
      "Epoch 372/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.6288e-06 - accuracy: 1.0000\n",
      "Epoch 372: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 40ms/step - loss: 3.6288e-06 - accuracy: 1.0000 - val_loss: 0.4142 - val_accuracy: 0.9395 - lr: 3.9063e-06\n",
      "Epoch 373/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 5.1755e-06 - accuracy: 1.0000\n",
      "Epoch 373: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 5.1593e-06 - accuracy: 1.0000 - val_loss: 0.4150 - val_accuracy: 0.9395 - lr: 3.9063e-06\n",
      "Epoch 374/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 3.9334e-06 - accuracy: 1.0000\n",
      "Epoch 374: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 3.9232e-06 - accuracy: 1.0000 - val_loss: 0.4150 - val_accuracy: 0.9395 - lr: 3.9063e-06\n",
      "Epoch 375/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.3498e-06 - accuracy: 1.0000\n",
      "Epoch 375: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 3.3498e-06 - accuracy: 1.0000 - val_loss: 0.4151 - val_accuracy: 0.9395 - lr: 3.9063e-06\n",
      "Epoch 376/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.8520e-06 - accuracy: 1.0000\n",
      "Epoch 376: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 2.8498e-06 - accuracy: 1.0000 - val_loss: 0.4151 - val_accuracy: 0.9399 - lr: 3.9063e-06\n",
      "Epoch 377/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 5.0257e-06 - accuracy: 1.0000\n",
      "Epoch 377: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 5.0257e-06 - accuracy: 1.0000 - val_loss: 0.4147 - val_accuracy: 0.9399 - lr: 3.9063e-06\n",
      "Epoch 378/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.6737e-06 - accuracy: 1.0000\n",
      "Epoch 378: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 3.6737e-06 - accuracy: 1.0000 - val_loss: 0.4148 - val_accuracy: 0.9399 - lr: 3.9063e-06\n",
      "Epoch 379/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 4.5776e-06 - accuracy: 1.0000\n",
      "Epoch 379: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 4.5776e-06 - accuracy: 1.0000 - val_loss: 0.4153 - val_accuracy: 0.9395 - lr: 3.9063e-06\n",
      "Epoch 380/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.1474e-06 - accuracy: 1.0000\n",
      "Epoch 380: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 40ms/step - loss: 3.1368e-06 - accuracy: 1.0000 - val_loss: 0.4156 - val_accuracy: 0.9395 - lr: 3.9063e-06\n",
      "Epoch 381/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 5.2921e-06 - accuracy: 1.0000\n",
      "Epoch 381: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 40ms/step - loss: 5.2921e-06 - accuracy: 1.0000 - val_loss: 0.4162 - val_accuracy: 0.9399 - lr: 3.9063e-06\n",
      "Epoch 382/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.6182e-06 - accuracy: 1.0000\n",
      "Epoch 382: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 3.6182e-06 - accuracy: 1.0000 - val_loss: 0.4163 - val_accuracy: 0.9395 - lr: 3.9063e-06\n",
      "Epoch 383/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 4.0533e-06 - accuracy: 1.0000\n",
      "Epoch 383: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 4.2666e-06 - accuracy: 1.0000 - val_loss: 0.4164 - val_accuracy: 0.9392 - lr: 3.9063e-06\n",
      "Epoch 384/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 5.1351e-06 - accuracy: 1.0000\n",
      "Epoch 384: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 5.0952e-06 - accuracy: 1.0000 - val_loss: 0.4164 - val_accuracy: 0.9392 - lr: 3.9063e-06\n",
      "Epoch 385/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.6164e-06 - accuracy: 1.0000\n",
      "Epoch 385: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 3.6060e-06 - accuracy: 1.0000 - val_loss: 0.4162 - val_accuracy: 0.9392 - lr: 3.9063e-06\n",
      "Epoch 386/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.2777e-06 - accuracy: 1.0000\n",
      "Epoch 386: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 3.2777e-06 - accuracy: 1.0000 - val_loss: 0.4162 - val_accuracy: 0.9389 - lr: 3.9063e-06\n",
      "Epoch 387/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.9429e-06 - accuracy: 1.0000\n",
      "Epoch 387: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 2.9382e-06 - accuracy: 1.0000 - val_loss: 0.4168 - val_accuracy: 0.9385 - lr: 3.9063e-06\n",
      "Epoch 388/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.8228e-06 - accuracy: 1.0000\n",
      "Epoch 388: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 3.8228e-06 - accuracy: 1.0000 - val_loss: 0.4172 - val_accuracy: 0.9392 - lr: 3.9063e-06\n",
      "Epoch 389/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.2023e-06 - accuracy: 1.0000\n",
      "Epoch 389: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 3.2155e-06 - accuracy: 1.0000 - val_loss: 0.4176 - val_accuracy: 0.9392 - lr: 3.9063e-06\n",
      "Epoch 390/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.0591e-06 - accuracy: 1.0000\n",
      "Epoch 390: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 40ms/step - loss: 3.0591e-06 - accuracy: 1.0000 - val_loss: 0.4180 - val_accuracy: 0.9392 - lr: 3.9063e-06\n",
      "Epoch 391/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 5.2387e-06 - accuracy: 1.0000\n",
      "Epoch 391: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 5.2387e-06 - accuracy: 1.0000 - val_loss: 0.4193 - val_accuracy: 0.9389 - lr: 3.9063e-06\n",
      "Epoch 392/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.0255e-06 - accuracy: 1.0000\n",
      "Epoch 392: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 3.0310e-06 - accuracy: 1.0000 - val_loss: 0.4192 - val_accuracy: 0.9389 - lr: 3.9063e-06\n",
      "Epoch 393/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 3.3517e-06 - accuracy: 1.0000\n",
      "Epoch 393: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 32ms/step - loss: 3.3367e-06 - accuracy: 1.0000 - val_loss: 0.4195 - val_accuracy: 0.9389 - lr: 3.9063e-06\n",
      "Epoch 394/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.0793e-06 - accuracy: 1.0000\n",
      "Epoch 394: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 3.0777e-06 - accuracy: 1.0000 - val_loss: 0.4186 - val_accuracy: 0.9392 - lr: 3.9063e-06\n",
      "Epoch 395/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.9180e-06 - accuracy: 1.0000\n",
      "Epoch 395: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 3.9180e-06 - accuracy: 1.0000 - val_loss: 0.4194 - val_accuracy: 0.9385 - lr: 3.9063e-06\n",
      "Epoch 396/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.3473e-06 - accuracy: 1.0000\n",
      "Epoch 396: val_accuracy did not improve from 0.94282\n",
      "\n",
      "Epoch 396: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 3.3473e-06 - accuracy: 1.0000 - val_loss: 0.4188 - val_accuracy: 0.9382 - lr: 3.9063e-06\n",
      "Epoch 397/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.7679e-06 - accuracy: 1.0000\n",
      "Epoch 397: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 3.7798e-06 - accuracy: 1.0000 - val_loss: 0.4190 - val_accuracy: 0.9382 - lr: 1.9531e-06\n",
      "Epoch 398/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.9382e-06 - accuracy: 1.0000\n",
      "Epoch 398: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 39ms/step - loss: 3.9384e-06 - accuracy: 1.0000 - val_loss: 0.4191 - val_accuracy: 0.9385 - lr: 1.9531e-06\n",
      "Epoch 399/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.3432e-06 - accuracy: 1.0000\n",
      "Epoch 399: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 40ms/step - loss: 3.3432e-06 - accuracy: 1.0000 - val_loss: 0.4191 - val_accuracy: 0.9382 - lr: 1.9531e-06\n",
      "Epoch 400/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 4.2125e-06 - accuracy: 1.0000\n",
      "Epoch 400: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 4.2125e-06 - accuracy: 1.0000 - val_loss: 0.4192 - val_accuracy: 0.9382 - lr: 1.9531e-06\n",
      "Epoch 401/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.8730e-06 - accuracy: 1.0000\n",
      "Epoch 401: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 3.8730e-06 - accuracy: 1.0000 - val_loss: 0.4190 - val_accuracy: 0.9385 - lr: 1.9531e-06\n",
      "Epoch 402/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.9610e-06 - accuracy: 1.0000\n",
      "Epoch 402: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 3.9507e-06 - accuracy: 1.0000 - val_loss: 0.4195 - val_accuracy: 0.9379 - lr: 1.9531e-06\n",
      "Epoch 403/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 2.2322e-06 - accuracy: 1.0000\n",
      "Epoch 403: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 2.2254e-06 - accuracy: 1.0000 - val_loss: 0.4198 - val_accuracy: 0.9379 - lr: 1.9531e-06\n",
      "Epoch 404/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.6603e-06 - accuracy: 1.0000\n",
      "Epoch 404: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 3.6516e-06 - accuracy: 1.0000 - val_loss: 0.4198 - val_accuracy: 0.9379 - lr: 1.9531e-06\n",
      "Epoch 405/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.8503e-06 - accuracy: 1.0000\n",
      "Epoch 405: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 2.8398e-06 - accuracy: 1.0000 - val_loss: 0.4197 - val_accuracy: 0.9379 - lr: 1.9531e-06\n",
      "Epoch 406/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.3942e-06 - accuracy: 1.0000\n",
      "Epoch 406: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 3.3875e-06 - accuracy: 1.0000 - val_loss: 0.4194 - val_accuracy: 0.9382 - lr: 1.9531e-06\n",
      "Epoch 407/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.3805e-06 - accuracy: 1.0000\n",
      "Epoch 407: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 40ms/step - loss: 3.3805e-06 - accuracy: 1.0000 - val_loss: 0.4197 - val_accuracy: 0.9382 - lr: 1.9531e-06\n",
      "Epoch 408/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.8970e-06 - accuracy: 1.0000\n",
      "Epoch 408: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 39ms/step - loss: 2.8970e-06 - accuracy: 1.0000 - val_loss: 0.4194 - val_accuracy: 0.9382 - lr: 1.9531e-06\n",
      "Epoch 409/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 5.3414e-06 - accuracy: 1.0000\n",
      "Epoch 409: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 5.3414e-06 - accuracy: 1.0000 - val_loss: 0.4199 - val_accuracy: 0.9385 - lr: 1.9531e-06\n",
      "Epoch 410/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.9112e-06 - accuracy: 1.0000\n",
      "Epoch 410: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 2.9112e-06 - accuracy: 1.0000 - val_loss: 0.4200 - val_accuracy: 0.9385 - lr: 1.9531e-06\n",
      "Epoch 411/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.4709e-06 - accuracy: 1.0000\n",
      "Epoch 411: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 2.4787e-06 - accuracy: 1.0000 - val_loss: 0.4200 - val_accuracy: 0.9385 - lr: 1.9531e-06\n",
      "Epoch 412/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 4.1277e-06 - accuracy: 1.0000\n",
      "Epoch 412: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 4.1142e-06 - accuracy: 1.0000 - val_loss: 0.4198 - val_accuracy: 0.9385 - lr: 1.9531e-06\n",
      "Epoch 413/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.8881e-06 - accuracy: 1.0000\n",
      "Epoch 413: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 2.8881e-06 - accuracy: 1.0000 - val_loss: 0.4197 - val_accuracy: 0.9385 - lr: 1.9531e-06\n",
      "Epoch 414/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 4.6545e-06 - accuracy: 1.0000\n",
      "Epoch 414: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 4.6545e-06 - accuracy: 1.0000 - val_loss: 0.4196 - val_accuracy: 0.9389 - lr: 1.9531e-06\n",
      "Epoch 415/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.8048e-06 - accuracy: 1.0000\n",
      "Epoch 415: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 10s 43ms/step - loss: 2.7978e-06 - accuracy: 1.0000 - val_loss: 0.4194 - val_accuracy: 0.9389 - lr: 1.9531e-06\n",
      "Epoch 416/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 4.8573e-06 - accuracy: 1.0000\n",
      "Epoch 416: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 39ms/step - loss: 4.8456e-06 - accuracy: 1.0000 - val_loss: 0.4195 - val_accuracy: 0.9389 - lr: 1.9531e-06\n",
      "Epoch 417/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.2600e-06 - accuracy: 1.0000\n",
      "Epoch 417: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 40ms/step - loss: 2.2600e-06 - accuracy: 1.0000 - val_loss: 0.4197 - val_accuracy: 0.9389 - lr: 1.9531e-06\n",
      "Epoch 418/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 3.1248e-06 - accuracy: 1.0000\n",
      "Epoch 418: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 3.1540e-06 - accuracy: 1.0000 - val_loss: 0.4196 - val_accuracy: 0.9389 - lr: 1.9531e-06\n",
      "Epoch 419/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.8590e-06 - accuracy: 1.0000\n",
      "Epoch 419: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 2.8557e-06 - accuracy: 1.0000 - val_loss: 0.4198 - val_accuracy: 0.9389 - lr: 1.9531e-06\n",
      "Epoch 420/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.7337e-06 - accuracy: 1.0000\n",
      "Epoch 420: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 2.7337e-06 - accuracy: 1.0000 - val_loss: 0.4201 - val_accuracy: 0.9389 - lr: 1.9531e-06\n",
      "Epoch 421/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 4.2561e-06 - accuracy: 1.0000\n",
      "Epoch 421: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 4.2470e-06 - accuracy: 1.0000 - val_loss: 0.4202 - val_accuracy: 0.9389 - lr: 1.9531e-06\n",
      "Epoch 422/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.9799e-06 - accuracy: 1.0000\n",
      "Epoch 422: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 2.9783e-06 - accuracy: 1.0000 - val_loss: 0.4202 - val_accuracy: 0.9385 - lr: 1.9531e-06\n",
      "Epoch 423/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.9813e-06 - accuracy: 1.0000\n",
      "Epoch 423: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 38ms/step - loss: 2.9745e-06 - accuracy: 1.0000 - val_loss: 0.4204 - val_accuracy: 0.9385 - lr: 1.9531e-06\n",
      "Epoch 424/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.7830e-06 - accuracy: 1.0000\n",
      "Epoch 424: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 39ms/step - loss: 2.7767e-06 - accuracy: 1.0000 - val_loss: 0.4207 - val_accuracy: 0.9385 - lr: 1.9531e-06\n",
      "Epoch 425/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.9002e-06 - accuracy: 1.0000\n",
      "Epoch 425: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 41ms/step - loss: 3.9002e-06 - accuracy: 1.0000 - val_loss: 0.4205 - val_accuracy: 0.9389 - lr: 1.9531e-06\n",
      "Epoch 426/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.1912e-06 - accuracy: 1.0000\n",
      "Epoch 426: val_accuracy did not improve from 0.94282\n",
      "\n",
      "Epoch 426: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "222/222 [==============================] - 9s 40ms/step - loss: 3.1912e-06 - accuracy: 1.0000 - val_loss: 0.4205 - val_accuracy: 0.9392 - lr: 1.9531e-06\n",
      "Epoch 427/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.1567e-06 - accuracy: 1.0000\n",
      "Epoch 427: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 38ms/step - loss: 3.1564e-06 - accuracy: 1.0000 - val_loss: 0.4205 - val_accuracy: 0.9392 - lr: 9.7656e-07\n",
      "Epoch 428/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.0721e-06 - accuracy: 1.0000\n",
      "Epoch 428: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 3.0916e-06 - accuracy: 1.0000 - val_loss: 0.4205 - val_accuracy: 0.9389 - lr: 9.7656e-07\n",
      "Epoch 429/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.8539e-06 - accuracy: 1.0000\n",
      "Epoch 429: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 34ms/step - loss: 3.8539e-06 - accuracy: 1.0000 - val_loss: 0.4206 - val_accuracy: 0.9389 - lr: 9.7656e-07\n",
      "Epoch 430/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.2423e-06 - accuracy: 1.0000\n",
      "Epoch 430: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 3.2325e-06 - accuracy: 1.0000 - val_loss: 0.4207 - val_accuracy: 0.9389 - lr: 9.7656e-07\n",
      "Epoch 431/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.1910e-06 - accuracy: 1.0000\n",
      "Epoch 431: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 3.1910e-06 - accuracy: 1.0000 - val_loss: 0.4207 - val_accuracy: 0.9389 - lr: 9.7656e-07\n",
      "Epoch 432/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.3497e-06 - accuracy: 1.0000\n",
      "Epoch 432: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 2.3420e-06 - accuracy: 1.0000 - val_loss: 0.4206 - val_accuracy: 0.9389 - lr: 9.7656e-07\n",
      "Epoch 433/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.1192e-06 - accuracy: 1.0000\n",
      "Epoch 433: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 38ms/step - loss: 3.1090e-06 - accuracy: 1.0000 - val_loss: 0.4208 - val_accuracy: 0.9389 - lr: 9.7656e-07\n",
      "Epoch 434/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.7078e-06 - accuracy: 1.0000\n",
      "Epoch 434: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 39ms/step - loss: 2.7078e-06 - accuracy: 1.0000 - val_loss: 0.4210 - val_accuracy: 0.9389 - lr: 9.7656e-07\n",
      "Epoch 435/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.7577e-06 - accuracy: 1.0000\n",
      "Epoch 435: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 41ms/step - loss: 2.7577e-06 - accuracy: 1.0000 - val_loss: 0.4210 - val_accuracy: 0.9389 - lr: 9.7656e-07\n",
      "Epoch 436/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.7443e-06 - accuracy: 1.0000\n",
      "Epoch 436: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 38ms/step - loss: 2.7443e-06 - accuracy: 1.0000 - val_loss: 0.4212 - val_accuracy: 0.9389 - lr: 9.7656e-07\n",
      "Epoch 437/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.2010e-06 - accuracy: 1.0000\n",
      "Epoch 437: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 3.2010e-06 - accuracy: 1.0000 - val_loss: 0.4212 - val_accuracy: 0.9389 - lr: 9.7656e-07\n",
      "Epoch 438/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.7059e-06 - accuracy: 1.0000\n",
      "Epoch 438: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 3.7059e-06 - accuracy: 1.0000 - val_loss: 0.4214 - val_accuracy: 0.9389 - lr: 9.7656e-07\n",
      "Epoch 439/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.8914e-06 - accuracy: 1.0000\n",
      "Epoch 439: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 34ms/step - loss: 2.8914e-06 - accuracy: 1.0000 - val_loss: 0.4215 - val_accuracy: 0.9389 - lr: 9.7656e-07\n",
      "Epoch 440/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.6178e-06 - accuracy: 1.0000\n",
      "Epoch 440: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 3.6141e-06 - accuracy: 1.0000 - val_loss: 0.4212 - val_accuracy: 0.9389 - lr: 9.7656e-07\n",
      "Epoch 441/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.3169e-06 - accuracy: 1.0000\n",
      "Epoch 441: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 3.3051e-06 - accuracy: 1.0000 - val_loss: 0.4212 - val_accuracy: 0.9389 - lr: 9.7656e-07\n",
      "Epoch 442/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.7931e-06 - accuracy: 1.0000\n",
      "Epoch 442: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 2.7931e-06 - accuracy: 1.0000 - val_loss: 0.4212 - val_accuracy: 0.9389 - lr: 9.7656e-07\n",
      "Epoch 443/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.2631e-06 - accuracy: 1.0000\n",
      "Epoch 443: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 38ms/step - loss: 3.2631e-06 - accuracy: 1.0000 - val_loss: 0.4211 - val_accuracy: 0.9389 - lr: 9.7656e-07\n",
      "Epoch 444/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.9598e-06 - accuracy: 1.0000\n",
      "Epoch 444: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 42ms/step - loss: 2.9598e-06 - accuracy: 1.0000 - val_loss: 0.4214 - val_accuracy: 0.9389 - lr: 9.7656e-07\n",
      "Epoch 445/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.6457e-06 - accuracy: 1.0000\n",
      "Epoch 445: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 38ms/step - loss: 3.6457e-06 - accuracy: 1.0000 - val_loss: 0.4212 - val_accuracy: 0.9389 - lr: 9.7656e-07\n",
      "Epoch 446/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.3926e-06 - accuracy: 1.0000\n",
      "Epoch 446: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 2.4197e-06 - accuracy: 1.0000 - val_loss: 0.4213 - val_accuracy: 0.9389 - lr: 9.7656e-07\n",
      "Epoch 447/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.7936e-06 - accuracy: 1.0000\n",
      "Epoch 447: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 34ms/step - loss: 2.7968e-06 - accuracy: 1.0000 - val_loss: 0.4214 - val_accuracy: 0.9389 - lr: 9.7656e-07\n",
      "Epoch 448/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.5156e-06 - accuracy: 1.0000\n",
      "Epoch 448: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 3.5081e-06 - accuracy: 1.0000 - val_loss: 0.4216 - val_accuracy: 0.9389 - lr: 9.7656e-07\n",
      "Epoch 449/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.3266e-06 - accuracy: 1.0000\n",
      "Epoch 449: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 3.3266e-06 - accuracy: 1.0000 - val_loss: 0.4216 - val_accuracy: 0.9389 - lr: 9.7656e-07\n",
      "Epoch 450/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.9211e-06 - accuracy: 1.0000\n",
      "Epoch 450: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 2.9211e-06 - accuracy: 1.0000 - val_loss: 0.4215 - val_accuracy: 0.9389 - lr: 9.7656e-07\n",
      "Epoch 451/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.6777e-06 - accuracy: 1.0000\n",
      "Epoch 451: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 2.6718e-06 - accuracy: 1.0000 - val_loss: 0.4216 - val_accuracy: 0.9389 - lr: 9.7656e-07\n",
      "Epoch 452/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.6405e-06 - accuracy: 1.0000\n",
      "Epoch 452: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 3.6347e-06 - accuracy: 1.0000 - val_loss: 0.4213 - val_accuracy: 0.9389 - lr: 9.7656e-07\n",
      "Epoch 453/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.2716e-06 - accuracy: 1.0000\n",
      "Epoch 453: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 38ms/step - loss: 3.2752e-06 - accuracy: 1.0000 - val_loss: 0.4214 - val_accuracy: 0.9389 - lr: 9.7656e-07\n",
      "Epoch 454/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.4311e-06 - accuracy: 1.0000\n",
      "Epoch 454: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 41ms/step - loss: 3.4242e-06 - accuracy: 1.0000 - val_loss: 0.4215 - val_accuracy: 0.9389 - lr: 9.7656e-07\n",
      "Epoch 455/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.9160e-06 - accuracy: 1.0000\n",
      "Epoch 455: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 40ms/step - loss: 2.9160e-06 - accuracy: 1.0000 - val_loss: 0.4216 - val_accuracy: 0.9389 - lr: 9.7656e-07\n",
      "Epoch 456/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.1405e-06 - accuracy: 1.0000\n",
      "Epoch 456: val_accuracy did not improve from 0.94282\n",
      "\n",
      "Epoch 456: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 3.1297e-06 - accuracy: 1.0000 - val_loss: 0.4215 - val_accuracy: 0.9389 - lr: 9.7656e-07\n",
      "Epoch 457/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.6906e-06 - accuracy: 1.0000\n",
      "Epoch 457: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 2.6826e-06 - accuracy: 1.0000 - val_loss: 0.4215 - val_accuracy: 0.9389 - lr: 4.8828e-07\n",
      "Epoch 458/500\n",
      "220/222 [============================>.] - ETA: 0s - loss: 6.6095e-06 - accuracy: 1.0000\n",
      "Epoch 458: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 6.5584e-06 - accuracy: 1.0000 - val_loss: 0.4215 - val_accuracy: 0.9389 - lr: 4.8828e-07\n",
      "Epoch 459/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 4.4968e-06 - accuracy: 1.0000\n",
      "Epoch 459: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 42ms/step - loss: 4.4833e-06 - accuracy: 1.0000 - val_loss: 0.4215 - val_accuracy: 0.9389 - lr: 4.8828e-07\n",
      "Epoch 460/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.0414e-06 - accuracy: 1.0000\n",
      "Epoch 460: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 10s 43ms/step - loss: 3.0414e-06 - accuracy: 1.0000 - val_loss: 0.4215 - val_accuracy: 0.9389 - lr: 4.8828e-07\n",
      "Epoch 461/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.2607e-06 - accuracy: 1.0000\n",
      "Epoch 461: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 40ms/step - loss: 3.2607e-06 - accuracy: 1.0000 - val_loss: 0.4216 - val_accuracy: 0.9389 - lr: 4.8828e-07\n",
      "Epoch 462/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.9048e-06 - accuracy: 1.0000\n",
      "Epoch 462: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 42ms/step - loss: 2.9492e-06 - accuracy: 1.0000 - val_loss: 0.4216 - val_accuracy: 0.9389 - lr: 4.8828e-07\n",
      "Epoch 463/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 4.4444e-06 - accuracy: 1.0000\n",
      "Epoch 463: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 40ms/step - loss: 4.4444e-06 - accuracy: 1.0000 - val_loss: 0.4216 - val_accuracy: 0.9389 - lr: 4.8828e-07\n",
      "Epoch 464/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.6147e-06 - accuracy: 1.0000\n",
      "Epoch 464: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 2.6123e-06 - accuracy: 1.0000 - val_loss: 0.4216 - val_accuracy: 0.9389 - lr: 4.8828e-07\n",
      "Epoch 465/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.4783e-06 - accuracy: 1.0000\n",
      "Epoch 465: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 2.4704e-06 - accuracy: 1.0000 - val_loss: 0.4217 - val_accuracy: 0.9389 - lr: 4.8828e-07\n",
      "Epoch 466/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.9591e-06 - accuracy: 1.0000\n",
      "Epoch 466: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 34ms/step - loss: 3.9591e-06 - accuracy: 1.0000 - val_loss: 0.4217 - val_accuracy: 0.9389 - lr: 4.8828e-07\n",
      "Epoch 467/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.4356e-06 - accuracy: 1.0000\n",
      "Epoch 467: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 3.4248e-06 - accuracy: 1.0000 - val_loss: 0.4217 - val_accuracy: 0.9389 - lr: 4.8828e-07\n",
      "Epoch 468/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 4.4616e-06 - accuracy: 1.0000\n",
      "Epoch 468: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 4.4616e-06 - accuracy: 1.0000 - val_loss: 0.4217 - val_accuracy: 0.9389 - lr: 4.8828e-07\n",
      "Epoch 469/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.6829e-06 - accuracy: 1.0000\n",
      "Epoch 469: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 2.6740e-06 - accuracy: 1.0000 - val_loss: 0.4216 - val_accuracy: 0.9389 - lr: 4.8828e-07\n",
      "Epoch 470/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.0023e-06 - accuracy: 1.0000\n",
      "Epoch 470: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 3.0486e-06 - accuracy: 1.0000 - val_loss: 0.4215 - val_accuracy: 0.9389 - lr: 4.8828e-07\n",
      "Epoch 471/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.2696e-06 - accuracy: 1.0000\n",
      "Epoch 471: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 38ms/step - loss: 2.2627e-06 - accuracy: 1.0000 - val_loss: 0.4215 - val_accuracy: 0.9389 - lr: 4.8828e-07\n",
      "Epoch 472/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.5476e-06 - accuracy: 1.0000\n",
      "Epoch 472: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 40ms/step - loss: 3.5353e-06 - accuracy: 1.0000 - val_loss: 0.4216 - val_accuracy: 0.9385 - lr: 4.8828e-07\n",
      "Epoch 473/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.6584e-06 - accuracy: 1.0000\n",
      "Epoch 473: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 41ms/step - loss: 3.6653e-06 - accuracy: 1.0000 - val_loss: 0.4216 - val_accuracy: 0.9385 - lr: 4.8828e-07\n",
      "Epoch 474/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.4618e-06 - accuracy: 1.0000\n",
      "Epoch 474: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 2.4577e-06 - accuracy: 1.0000 - val_loss: 0.4217 - val_accuracy: 0.9385 - lr: 4.8828e-07\n",
      "Epoch 475/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 6.2674e-06 - accuracy: 1.0000\n",
      "Epoch 475: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 6.2674e-06 - accuracy: 1.0000 - val_loss: 0.4217 - val_accuracy: 0.9385 - lr: 4.8828e-07\n",
      "Epoch 476/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.4349e-06 - accuracy: 1.0000\n",
      "Epoch 476: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 3.4349e-06 - accuracy: 1.0000 - val_loss: 0.4216 - val_accuracy: 0.9385 - lr: 4.8828e-07\n",
      "Epoch 477/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.3086e-06 - accuracy: 1.0000\n",
      "Epoch 477: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 3.3086e-06 - accuracy: 1.0000 - val_loss: 0.4217 - val_accuracy: 0.9385 - lr: 4.8828e-07\n",
      "Epoch 478/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.5993e-06 - accuracy: 1.0000\n",
      "Epoch 478: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 2.5993e-06 - accuracy: 1.0000 - val_loss: 0.4217 - val_accuracy: 0.9385 - lr: 4.8828e-07\n",
      "Epoch 479/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.3082e-06 - accuracy: 1.0000\n",
      "Epoch 479: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 3.3082e-06 - accuracy: 1.0000 - val_loss: 0.4217 - val_accuracy: 0.9385 - lr: 4.8828e-07\n",
      "Epoch 480/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.9565e-06 - accuracy: 1.0000\n",
      "Epoch 480: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 38ms/step - loss: 2.9565e-06 - accuracy: 1.0000 - val_loss: 0.4217 - val_accuracy: 0.9385 - lr: 4.8828e-07\n",
      "Epoch 481/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.7313e-06 - accuracy: 1.0000\n",
      "Epoch 481: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 41ms/step - loss: 2.7410e-06 - accuracy: 1.0000 - val_loss: 0.4218 - val_accuracy: 0.9385 - lr: 4.8828e-07\n",
      "Epoch 482/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.2119e-06 - accuracy: 1.0000\n",
      "Epoch 482: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 38ms/step - loss: 3.2119e-06 - accuracy: 1.0000 - val_loss: 0.4218 - val_accuracy: 0.9385 - lr: 4.8828e-07\n",
      "Epoch 483/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.2301e-06 - accuracy: 1.0000\n",
      "Epoch 483: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 3.2301e-06 - accuracy: 1.0000 - val_loss: 0.4218 - val_accuracy: 0.9385 - lr: 4.8828e-07\n",
      "Epoch 484/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.1668e-06 - accuracy: 1.0000\n",
      "Epoch 484: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 3.1550e-06 - accuracy: 1.0000 - val_loss: 0.4219 - val_accuracy: 0.9385 - lr: 4.8828e-07\n",
      "Epoch 485/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.7015e-06 - accuracy: 1.0000\n",
      "Epoch 485: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 2.7015e-06 - accuracy: 1.0000 - val_loss: 0.4219 - val_accuracy: 0.9385 - lr: 4.8828e-07\n",
      "Epoch 486/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 4.1278e-06 - accuracy: 1.0000\n",
      "Epoch 486: val_accuracy did not improve from 0.94282\n",
      "\n",
      "Epoch 486: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 4.1172e-06 - accuracy: 1.0000 - val_loss: 0.4219 - val_accuracy: 0.9385 - lr: 4.8828e-07\n",
      "Epoch 487/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.7205e-06 - accuracy: 1.0000\n",
      "Epoch 487: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 35ms/step - loss: 2.7205e-06 - accuracy: 1.0000 - val_loss: 0.4219 - val_accuracy: 0.9385 - lr: 2.4414e-07\n",
      "Epoch 488/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.9070e-06 - accuracy: 1.0000\n",
      "Epoch 488: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 3.8931e-06 - accuracy: 1.0000 - val_loss: 0.4219 - val_accuracy: 0.9385 - lr: 2.4414e-07\n",
      "Epoch 489/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 4.7589e-06 - accuracy: 1.0000\n",
      "Epoch 489: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 39ms/step - loss: 4.7589e-06 - accuracy: 1.0000 - val_loss: 0.4219 - val_accuracy: 0.9385 - lr: 2.4414e-07\n",
      "Epoch 490/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.5870e-06 - accuracy: 1.0000\n",
      "Epoch 490: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 42ms/step - loss: 2.5870e-06 - accuracy: 1.0000 - val_loss: 0.4219 - val_accuracy: 0.9385 - lr: 2.4414e-07\n",
      "Epoch 491/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.1505e-06 - accuracy: 1.0000\n",
      "Epoch 491: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 10s 47ms/step - loss: 2.1433e-06 - accuracy: 1.0000 - val_loss: 0.4219 - val_accuracy: 0.9385 - lr: 2.4414e-07\n",
      "Epoch 492/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 2.7921e-06 - accuracy: 1.0000\n",
      "Epoch 492: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 34ms/step - loss: 2.7917e-06 - accuracy: 1.0000 - val_loss: 0.4219 - val_accuracy: 0.9385 - lr: 2.4414e-07\n",
      "Epoch 493/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.3297e-06 - accuracy: 1.0000\n",
      "Epoch 493: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 3.3297e-06 - accuracy: 1.0000 - val_loss: 0.4219 - val_accuracy: 0.9385 - lr: 2.4414e-07\n",
      "Epoch 494/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.3890e-06 - accuracy: 1.0000\n",
      "Epoch 494: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 7s 34ms/step - loss: 2.3890e-06 - accuracy: 1.0000 - val_loss: 0.4219 - val_accuracy: 0.9385 - lr: 2.4414e-07\n",
      "Epoch 495/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.8426e-06 - accuracy: 1.0000\n",
      "Epoch 495: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 3.8426e-06 - accuracy: 1.0000 - val_loss: 0.4219 - val_accuracy: 0.9385 - lr: 2.4414e-07\n",
      "Epoch 496/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.6543e-06 - accuracy: 1.0000\n",
      "Epoch 496: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 37ms/step - loss: 3.6543e-06 - accuracy: 1.0000 - val_loss: 0.4219 - val_accuracy: 0.9385 - lr: 2.4414e-07\n",
      "Epoch 497/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.8852e-06 - accuracy: 1.0000\n",
      "Epoch 497: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 38ms/step - loss: 2.8852e-06 - accuracy: 1.0000 - val_loss: 0.4219 - val_accuracy: 0.9385 - lr: 2.4414e-07\n",
      "Epoch 498/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 3.7563e-06 - accuracy: 1.0000\n",
      "Epoch 498: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 39ms/step - loss: 3.7563e-06 - accuracy: 1.0000 - val_loss: 0.4219 - val_accuracy: 0.9385 - lr: 2.4414e-07\n",
      "Epoch 499/500\n",
      "222/222 [==============================] - ETA: 0s - loss: 2.8720e-06 - accuracy: 1.0000\n",
      "Epoch 499: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 9s 41ms/step - loss: 2.8720e-06 - accuracy: 1.0000 - val_loss: 0.4219 - val_accuracy: 0.9385 - lr: 2.4414e-07\n",
      "Epoch 500/500\n",
      "221/222 [============================>.] - ETA: 0s - loss: 3.7891e-06 - accuracy: 1.0000\n",
      "Epoch 500: val_accuracy did not improve from 0.94282\n",
      "222/222 [==============================] - 8s 36ms/step - loss: 3.7896e-06 - accuracy: 1.0000 - val_loss: 0.4219 - val_accuracy: 0.9385 - lr: 2.4414e-07\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# 모델 훈련\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=500,\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "        # save_best_only -> 모델 정확도가 이전보다 향상된 경우에만 모델 저장\n",
    "        ModelCheckpoint('models/model.keras', monitor='val_accuracy', verbose=1, save_best_only=True, mode='auto'),\n",
    "        # 정확도 개선이 없을시 학습률(factor) 0.5배로 감소, n 에포크 동안 개선 없을 경우 학습률 감소\n",
    "        ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=30, verbose=1, mode='auto'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 학습 데이터/검증 데이터 정확도/손실 그래프\n",
    "fig, loss_ax = plt.subplots(figsize=(16, 10))\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(history.history['accuracy'], 'b', label='train accuracy')\n",
    "acc_ax.plot(history.history['val_accuracy'], 'g', label='val accuracy')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
